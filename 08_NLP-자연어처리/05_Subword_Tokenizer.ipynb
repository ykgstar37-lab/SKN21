{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 형태소 분석 기반 토큰화의 문제\n",
    "- 형태소 분석기는 작성된 알고리즘 또는 학습된 내용을 바탕으로 토큰화를 하기 때문에 오탈자나 띄어쓰기 실수, 신조어, 외래어, 고유어 등이 사용된 경우 제대로 토큰화 하지 못한다.\n",
    "- 그래서 발생 할 수있는 잠재적 문제점\n",
    "    - 어휘사전을 크게 만든다.\n",
    "        - 같은 의미의 단어가 형태소 분석이 안되어 여러개 등록될 수있다.\n",
    "        - ex) 신조어 `돈쭐` 이라는 단어를 인식 못할 경우 `\"돈쭐내러\", \"돈쭐나\", \"돈쭐냄\"` 등이 다 등록 될 수 있다.\n",
    "    - OOV(Out Of Vocab)에 대응하기 어렵게 만든다.\n",
    "        - 같은 어근의 단어가 있지만 조사등이 바뀐 신조어등을 OOV로 인식할 수있다.\n",
    "\n",
    "\n",
    "## 어휘 사전(Vocabulary)과 Out Of Vocabulary (OOV)\n",
    "\n",
    "- 어휘사전(Vocab)은 토크나이저(Tokenizer)가 사용하는 모든 토큰의 집합이며,**각 토큰**을 고유한 **정수 ID**에 매핑한 사전이다. 토크나이저가 텍스트를 토큰 ID 시퀀스로 변환할 때 기준으로 사용된다. \n",
    "\n",
    "   - 매핑된 정수는 모델에 입력되는 텍스트 데이터를 숫자 형식으로 변환해 모델이 처리할 수 있도록 돕는다.\n",
    "   - 예\n",
    "        ```bash\n",
    "        {\n",
    "            \"자연어\": 0,\n",
    "            \"처리\": 1,\n",
    "            \"는\": 2,\n",
    "            \"재미있다\": 3,\n",
    "            \"공부\": 4,\n",
    "        }\n",
    "        ```\n",
    "- **Out Of Vocabulary (OOV)**\n",
    "   - 어휘 사전(Vocab): 코퍼스를 구성하는 모든 토큰의 집합.\n",
    "   - **OOV**란 어휘 사전에 포함되지 않은 토큰을 의미하며, 모델이 해당 토큰을 처리할 수 없기 때문에 일반적으로 특별한 토큰(예: `[UNK]`)으로 대체되거나 다른 방식으로 처리된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization(하위 단어 토큰화)\n",
    "\n",
    "## 정의\n",
    "\n",
    "- Subword Tokenization은 단어를 더 작은 단위(subword)로 나누어 텍스트를 토큰화하는 방식이다.  \n",
    "    - subword는 하나의 단어를 구성하는 단어들을 말한다.(coworker: co, work, er)\n",
    "- 주로 자주 등장하는 단어의 일부를 공통된 토큰으로 만들고, 희귀하거나 복합적인 단어는 작은 조각(subword)으로 나누어 처리한다.\n",
    "- 단어 자체를 그대로 사용하기보다는 단어의 일부를 나누어 처리함으로써 새로운 단어나 미등록 단어(Out-of-Vocabulary) 문제를 줄일 수 있다.\n",
    "\n",
    "## 장점\n",
    "\n",
    "1. **미등록 단어 처리 가능**  \n",
    "   -  새로운 단어(신조어, 속어, 고유어등)가 등장해도 미리 정의된 subword를 조합해서 표현할 수 있어 OOV 문제를 줄일 수 있다.  \n",
    "\n",
    "2. **어휘 크기 축소**  \n",
    "   - 같은 subword를 여러 단어에서 공유함으로써, 완전한 단어를 사용하는 경우보다 어휘집의 크기를 작게 유지할 수 있다.\n",
    "\n",
    "\n",
    "## 종류\n",
    "\n",
    "1. **Byte-Pair Encoding (BPE)**  \n",
    "   - 자주 등장하는 문자 쌍을 반복적으로 병합해 서브워드를 생성하는 방식.\n",
    "   - OpenAI의 GPT 모델에 사용된 토크나이저이다.\n",
    "\n",
    "2. **Unigram**  \n",
    "   - 빈도기반 확률모델에 따라 subword 단위를 선택하는 방식이다.  \n",
    "   - BPE보다 유연하여 더 다양한 분할 결과를 얻을 수 있다.\n",
    "\n",
    "3. **WordPiece**  \n",
    "   - BPE와 유사하지만, 빈도수가 아니라, 가능성이 높은 조합(합쳐질 가능성이 높은 subword)에 기반해 subword들을 찾는다.\n",
    "   - Google의 BERT 모델에 사용된 토크나이저이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding 방식\n",
    "\n",
    "- 원래 Text data 압축을 위해 만들어진 방법으로 text 에서 자주 같이 나오는 두 글짜 쌍을 합쳐서 하나의 부호(기호,글자)로 만들어 나가면서 글자 수를 줄이는 알고리즘이다. \n",
    "- 연속된 글자 쌍이 더 나타나지 않거나 정해진 어휘사전 크기에 도달 할 때 까지 조합을 찾아 부호화 하는 작업을 반복한다.\n",
    "\n",
    "## text 압축 방식의 예\n",
    "- 원문: abracadabra\n",
    "1. AracadAra: ab -> A :=> 원문에서 가장 빈도수 많은 ab를 A(부호로 아무 글자나 사용할 수 있다.)로 치환\n",
    "2. ABcadAB: ra -> B :=> 1에서 가장 빈도수가 많은 ra를 B로 치환\n",
    "3. CcadC: AB -> C :=> 2에서 가장 빈도수 맣은 AB를 C로 치환한다.(치환된 글자 쌍도 변환대상에 포함된다.)\n",
    "\n",
    "## BPE Tokenizer 방식\n",
    "BPE 토크나이저는 자주 등장하는 글자 쌍을 찾아 치환하는 대신 **단어 사전**에 추가한다.\n",
    "\n",
    "### 예)\n",
    "1. 말뭉치의 토큰들의 빈도수, 어휘사전은 아래와 같을 경우\n",
    "    - 빈도사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)\n",
    "    - 어휘사전: ['low', 'lower', 'newest', 'widest']\n",
    "2. 빈도 사전내의 모든 단어들을 글자 단위로 나눈다. (Pre Tokenization)\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
    "3. 빈도 사전을 기준으로 가장 자주 등장하는 글자 쌍(byte pair)를 찾는다.  위에서는 **'e'와 's'가 총 9번으로 가장 많이 등장함**. 'e'와 's'를 'es'로 합치고 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'es'**, 't', 6), ('w', 'i', 'd', **'es'**, 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**]\n",
    "4. 3 번의 과정을 계속 반복한다. 빈도수가 가장 많은 'es'와 't' 쌍을 'est'로 병합하고 'est'를 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**]\n",
    "5. 만약 10번 반복했다고 하면 다음과 같은 빈도 사전과 어휘 사전이 생성된다.\n",
    "    - 빈도 사전: (**'low'**, 5), (**'low'**, 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**, **'lo'**,**'low'**, **'ne'**, **'new'**, **'newest'**, **'wi'**, **'wid'**, **'widest'**]\n",
    "\n",
    "- 위와 같이 어휘 사전이 만들어 지면 원래 어휘서전에 없던 것들에 대한 처리를 할 수있다.\n",
    "    - ex)\n",
    "        - 'newer' :=> 'new', 'e', 'r', \n",
    "        - 'lowest' :=> 'low', 'est'\n",
    "        - 'wider' :=> 'wid', 'e', 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "- Byte Pair Encoding 이 빈도 기반이라면 wordpiece tokenizer는 확률 기반으로 글자 쌍을 병합한다.\n",
    "- 두개 글자 쌍의 빈도수를 각 개별 글자 빈도수의 곱으로 나눈 점수가 가장 높은 순서대로 글자쌍을 묶어 나간다.\n",
    "\n",
    "$$\n",
    "score = \\cfrac{f(x, y)}{f(x)\\cdot f(y)} \n",
    "$$\n",
    "\n",
    "함수 f는 빈도를 나타내며 x, y는 병합하려는 하위 단어이다.\n",
    "\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w')\n",
    "- 가장 빈도수가 높은 쌍은 'e','s'로 9번 등장한다. 이때 각 글자는 전체에서 각각 'e'는 17번, 's'는 9번 등장한다. 위 공식에 대입하면 score는 $\\frac{9}{17 \\times 9} \\approx 0.06$ 이다.\n",
    "- 'i'와 'd' 쌍은 3번만 등장하지만 전체에서 각각 'i' 3번, 'd' 3번 등장한다. 그래서 score는 $\\frac{3}{3 \\times 3} \\approx 0.33$ 이다.\n",
    "- 나타난 빈도수는 'es' 가 많치만 더 높은 score를 가지는 'id' 쌍을 병합한다.\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', **'id'**, 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'id'**)\n",
    "위의 작업을 반복해 연속된 글자 쌍이 더이상 나타나지 않거나 어휘 사전 max 크기에 도달할 때 까지 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram 방식\n",
    "- 빈도 기반 확률 모델을 사용하여 효율적으로 서브워드를 선택하고, 불필요한 서브워드를 제거해 최적의 어휘 크기를 찾는 알고리즘\n",
    "\n",
    "\n",
    "- **초기 어휘 집합 구성**\n",
    "    - 대상 text에 모든 단어와 그 서브스트링을 포함한 어휘 집합을 생성한다. 이 어휘 집합은 나올 수있는 모든 subword들을 다 모아놓은 것이다. \n",
    "    - 예를 들어 \"hug\" 단어의  [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"hug\"]  substring을 만든다. 이들이 subword 후보가 된다.\n",
    "- **각 Subword의 빈도수 기반 확률 계산**\n",
    "    -  $\\cfrac{subword가\\;나타난\\;횟수}{전체\\;빈도수}$ 로 각 subword들의 나타난 확률을 계산한다.\n",
    "- **가능한 분할에 대한 확률 계산**\n",
    "    - 단어를 여러 서브워드로 분할할 수 있는 경우, 각 분할에 대한 전체 확률을 계산한다.\n",
    "    - 확률 계산은 $ P(subword1)\\;\\times \\; P(subword2)\\;\\times\\; ..$ 으로 계산한다.\n",
    "    - 예를 들어 \"hug\" 를 분할 한다고 했을 때\n",
    "        1. \\[\"h\", \"u\", \"g\"\\]: $ P(h) \\times P(u) \\times P(g) $\n",
    "        2. \\[\"hu\", \"g\"\\]: $ P(hu) \\times P(g) $\n",
    "\n",
    "   - 각각의 확률을 계산한 후, **가장 높은 확률**을 가진 분할을 선택한다.\n",
    "     - 위 예에서 만약 1의 확률이 0.01 이고 2의 확률이 0.00001 이라면 첫번째 분할이 선택된다.\n",
    "\n",
    "- **서브워드 제거**\n",
    "    - 위의 훈련 과정에서 불필요한 서브워드를 제거하면서 최적의 어휘 집합을 찾아간다. \n",
    "    - 제거 대상은 빈도수가 낮거나 조합에 크게 영향을 주지 않은 subword들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### korpora 말뭉치\n",
    "> - 다양한 한글 데이터셋을 제공하는 패키지\n",
    "> - `pip install korpora`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m11 packages\u001b[0m \u001b[2min 512ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 203ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses\u001b[0m\u001b[2m==0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkorpora\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxlrd\u001b[0m\u001b[2m==2.0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-03\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-04\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-05\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-06\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-07\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2019-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2019-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2019-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Korpora.korpus_korean_petitions.KoreanPetitionsKorpus at 0x1ed21087dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 433631)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions = corpus.get_all_texts()\n",
    "type(petitions), len(petitions) # 청와대청원 데이터셋. list[str] str: 개별청원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용, 기간제 교사, 영전강, 스강의 무기계약직화가 그들의 임용 절벽과는 전혀 무관한 일이라고 주장하고 있지만 조금만 생각해보면 전혀 설득력 없는 말이라고 생각합니다. 학교 수가 같고, 학생 수가 동일한데 영양교사와 기간제 교사, 영전강 스강이 학교에 늘어나게 되면 당연히 정규 교원의 수는 줄어들게 되지 않겠습니까? 기간제 교사, 영전강, 스강의 무기계약직화, 정규직화 꼭 전면 백지화해주십시오. 백년대계인 국가의 교육에 달린 문제입니다. 단순히 대통령님의 일자리 공약, 81만개 일자리 창출 공약을 지키시고자 돌이킬 수 없는 실수는 하지 않으시길 바랍니다. 세계 어느 나라와 비교해도, 한국 교원의 수준과 질은 최고 수준입니다. 고등교육을 받고 어려운 국가 고시를 통과해야만 대한민국 공립 학교의 교단에 설 수 있고, 이러한 과정이 힘들기는 하지만 교원들이 교육자로서의 사명감과 자부심을 갖고 교육하게 되는 원동력이기도 합니다. 자격도 없는 비정규 인력들을 일자리 늘리기 명목 하에 학교로 들이게 되면, 그들이 무슨 낯으로 대한민국이 '공정한 사회' 라고 아이들에게 가르칠 수 있겠습니까? 그들이 가르치는 것을 학부모와 학생들이 납득할 수 있겠으며, 학생들은 공부를 열심히 해야하는 이유를 찾을 수나 있겠습니까? 열심히 안 해도 떼 쓰면 되는 세상이라고 생각하지 않겠습니까? 영양사의 영양교사화도 재고해주십시오. 영양사분들 정말 너무나 고마운 분들입니다. 학생들의 건강과 영양? 당연히 성장기에 있는 아이들에게 필수적이고 중요한 문제입니다. 하지만 이들이 왜 교사입니까. 유래를 찾아 볼 수 없는 영양사의 '교사'화. 정말 대통령님이 생각하신 아이디어라고 믿기 싫을 정도로 납득하기 어렵습니다. 중등은 실과교과 교사가 존재하지요? 초등 역시 임용 시험에 실과가 포함돼 있으며 학교 현장에서도 정규 교원이 직접 실과 과목을 학생들에게 가르칩니다. 영양'교사', 아니 영양사가 학생들에게 실과를 가르치지 않습니다. 아니 그 어떤 것도 가르치지 않습니다. 올해 대통령님 취임 후에 초등, 중등 임용 티오가 초전박살 나는 동안 영양'교사' 티오는 폭발적으로 확대된 줄로 압니다. 학생들의 교육을 위해 정말 교원의 수를 줄이고, 영양 교사의 수를 늘리는 것이 올바른 해답인지 묻고 싶습니다. 마지막으로 교원 당 학생 수. 이 통계도 제대로 내주시기 바랍니다. 다른 나라들은 '정규 교원', 즉 담임이나 교과 교사들로만 통계를 내는데(너무나 당연한 것이지요) 왜 한국은 보건, 영양, 기간제, 영전강, 스강 까지 다 포함해서 교원 수 통계를 내는건가요? 이런 통계의 장난을 통해 OECD 평균 교원 당 학생 수와 거의 비슷한 수준에 이르렀다고 주장하시는건가요? 학교는 교육의 장이고 학생들의 공간이지, 인력 센터가 아닙니다. 부탁드립니다. 부디 넓은 안목으로 멀리 내다봐주시길 간곡히 부탁드립니다.\n"
     ]
    }
   ],
   "source": [
    "print(petitions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일저장\n",
    "import os\n",
    "os.makedirs('data/petitions', exist_ok=True)\n",
    "with open(\"data/petitions/petition_corpus.txt\", \"wt\", encoding=\"utf-8\") as fw:\n",
    "    for p in petitions:\n",
    "        fw.write(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로드 : 문자열로 load\n",
    "with open(\"data/petitions/petition_corpus.txt\", \"rt\", encoding='utf-8') as fr:\n",
    "    petitions_txt = fr.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용,\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions_txt[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face tokenizers 패키지 사용해 토큰화 수행\n",
    "\n",
    "## 주요 라이브러리 \n",
    "- [tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "    - huggingface에서 개발한 tokenizer 라이브러리로 BPE, WordPiece, Unigram 알고리즘을 지원한다. \n",
    "    - 설치: `pip install tokenizers`\n",
    "- [Sentencepiece](https://github.com/google/sentencepiece)\n",
    "    - 구글에서 개발한 subword tokenizer 라이브러리로 BPE, Unigram 방식 지원.\n",
    "    - 설치: `pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m20 packages\u001b[0m \u001b[2min 489ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 105ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 495ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.20.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face tokenizers 패키지이용\n",
    "- Hugging face는 \n",
    "  - Subword방식의 토크나이저를 생성할 수 있는 알고리즘을 제공한다. `tokenizers` library를 이용해 사용할 수 있다.\n",
    "  - GPT, BERT등 다양한 LLM 모델에서 사용된 Pretrained Tokenizer들을 제공한다. `transformers` Library를 이용해 사용할 수 있다.\n",
    "- 설치: `pip install tokenizers`\n",
    "- Tokenizer 생성\n",
    "    - 토큰화 알고리즘을 지정해 instance 생성.\n",
    "- Trainer 생성\n",
    "    - 학습 파라미터를 설정해서 instance 생성\n",
    "- Tokenizer 학습\n",
    "    - train() 메소드: 학습 text 파일 경로를 지정해서 학습\n",
    "    - train_from_iterator() 메소드: 학습할 string들을 iterator를 통해 제공.\n",
    "- https://github.com/huggingface/tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Tokenizer (어휘사전을 바탕으로 토큰화를 처리하는 객체.)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "# Pre Tokenizer 로 Subword 방식 토큰화 전에 미리 나누는 방식을 지정.\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "# Trainer (토크나이저 모델 학습기) - 알고리즘에 맞춰 trainer를 제공.\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 객체 생성\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\")  # Unknown Token(모르는 단어를 표현할 토큰)을 설정. \n",
    ")\n",
    "\n",
    "# 토크나이저에 Pre tokenizer 설정\n",
    "## BPE 알고리즘으로 토큰화 하기 전에 적용할 나누는 방법-Whitespace(): 공백기준으로 미리 나눈다.\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 생성 - 학습 하이퍼 파라미터들을 설정\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000,  # 어휘사전의 최대크기.\n",
    "    min_frequency=10,  # 최소 출연 횟수. 지정한 횟수(10) 이하로 나온 쌍(pair)는 단어사전에서 제외.\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\"], # 어휘사전에 추가할 특수목적 토큰. unk_token은 반드시 추가한다.\n",
    "    # continuing_subword_prefix=\"##\" \n",
    "        # # 단어의 시작이 아니라 연결 subword의 앞에 붙이는 prefix\n",
    "        # BpeTrainer default: 안붙인다. (\"\")\n",
    "        # WordPieceTrainer: `##` 이 default\n",
    "        # UnigramTrainer: `\"\"` 이 default. 안붙인다.\n",
    ")\n",
    "\n",
    "# Special Token \n",
    "#    - 특수목적 토큰. \n",
    "#    - 토크나이저나 모델에서 사용할 특정 목적의 토큰으로 우리가 직접 어휘사전에 추가해야 한다.\n",
    "# 대표적인 special token들. 보통 [] 나 <> 로 묶어준다.\n",
    "# - OOV 표시 토큰: <unk>, [unk] 로 표현\n",
    "# - Padding 토큰 : <pad>로 표현. 글자(토큰)수를 맞춰주기 위해 글자수가 적은 문장에 추가하는 토큰\n",
    "# - 문장의 시작: <sos> 로 표현\n",
    "# - 문장의 끝: <eos>\n",
    "# - <cls> : 문서의 시작 + 전체 문서의 의미를 표현 (BERT  모델이 사용하는 특수토큰.)\n",
    "# - 문서 내 일부 토큰을 가리기: <mask>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 걸린시간: 21.873293161392212 초\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "s = time.time()\n",
    "\n",
    "tokenizer.train([\"data/petitions/petition_corpus.txt\"], trainer=trainer)\n",
    "\n",
    "print(\"학습에 걸린시간:\", time.time() - s, \"초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 파일로 저장 -> 어휘사전 + 설정들이 저장. (json)\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "save_path = \"saved_models/petitions_bpe.json\"\n",
    "tokenizer.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 토크나이저 불러오기\n",
    "from tokenizers import Tokenizer\n",
    "load_tokenizer = Tokenizer.from_file(save_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size (토큰(어휘) 개수)\n",
    "tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 조회\n",
    "# tokenizer.get_vocab() # 딕셔너리로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문장\n",
    "sports_txt = \"프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다.\"\n",
    "petition_txt = \"이 글을 쓴 이유는 다름아닌 '전안법'시행 반대를 주장하기 위해서입니다. 먼저, '전안법'은 전기용품 및 생활용품을 판매하는 업체에서 KC인증마크를 의무적으로 받는 것입니다.\"\n",
    "comment_txt = \"멋진 식사를 즐기기에 좋은 장소 - 채식 메뉴가 정말 훌륭했습니다. 당근 케이크는 아마도 내가 먹어본 디저트 중 최고였을 거예요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=34, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화: text(str) -> token들\n",
    "token_output = tokenizer.encode(sports_txt)\n",
    "token_output # Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '리',\n",
       " '미',\n",
       " '어',\n",
       " '리',\n",
       " '그',\n",
       " '역',\n",
       " '대',\n",
       " '개인',\n",
       " '최',\n",
       " '다',\n",
       " '골',\n",
       " '기록',\n",
       " '을',\n",
       " '보유',\n",
       " '하고',\n",
       " '있는',\n",
       " '시',\n",
       " '어',\n",
       " '러',\n",
       " '가',\n",
       " '손',\n",
       " '흥',\n",
       " '민',\n",
       " '의',\n",
       " '골',\n",
       " '결정',\n",
       " '력을',\n",
       " '재',\n",
       " '차',\n",
       " '극',\n",
       " '찬',\n",
       " '했다',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding (토큰화 결과) 에서 각 토큰들을 문자열로 조회\n",
    "token_output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7390,\n",
       " 5123,\n",
       " 5330,\n",
       " 6140,\n",
       " 5123,\n",
       " 4128,\n",
       " 6180,\n",
       " 4589,\n",
       " 8414,\n",
       " 6891,\n",
       " 4563,\n",
       " 4027,\n",
       " 9449,\n",
       " 6364,\n",
       " 9442,\n",
       " 8209,\n",
       " 8219,\n",
       " 5908,\n",
       " 6140,\n",
       " 4980,\n",
       " 3902,\n",
       " 5802,\n",
       " 7638,\n",
       " 5332,\n",
       " 6384,\n",
       " 4027,\n",
       " 8940,\n",
       " 8644,\n",
       " 6449,\n",
       " 6793,\n",
       " 4129,\n",
       " 6795,\n",
       " 8565,\n",
       " 15]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 토큰들의 id(정수)로 반환.\n",
    "token_output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=52, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['이', '글을', '쓴', '이유는', '다', '름', '아닌', \"'\", '전', '안', '법', \"'\", '시행', '반대', '를', '주장', '하기', '위해서', '입니다', '.', '먼저', ',', \"'\", '전', '안', '법', \"'\", '은', '전기', '용', '품', '및', '생활', '용', '품', '을', '판매', '하는', '업체', '에서', 'K', 'C', '인', '증', '마', '크', '를', '의무', '적으로', '받는', '것입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "token_output2 = tokenizer.encode(petition_txt)\n",
    "print(token_output2)\n",
    "print(token_output2.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6396,\n",
       " 8577,\n",
       " 6044,\n",
       " 9246,\n",
       " 4563,\n",
       " 5107,\n",
       " 8323,\n",
       " 8,\n",
       " 6476,\n",
       " 6073,\n",
       " 5412,\n",
       " 8,\n",
       " 8656,\n",
       " 8658,\n",
       " 5101,\n",
       " 8953,\n",
       " 8301,\n",
       " 8580,\n",
       " 8212,\n",
       " 15,\n",
       " 8855,\n",
       " 13,\n",
       " 8,\n",
       " 6476,\n",
       " 6073,\n",
       " 5412,\n",
       " 8,\n",
       " 6360,\n",
       " 9353,\n",
       " 6278,\n",
       " 7375,\n",
       " 5345,\n",
       " 8437,\n",
       " 6278,\n",
       " 7375,\n",
       " 6364,\n",
       " 9118,\n",
       " 8208,\n",
       " 8692,\n",
       " 8210,\n",
       " 44,\n",
       " 36,\n",
       " 6400,\n",
       " 6625,\n",
       " 5140,\n",
       " 7091,\n",
       " 5101,\n",
       " 8521,\n",
       " 8253,\n",
       " 8546,\n",
       " 8299,\n",
       " 15]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_output2.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9246"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 토큰 문자열 -> 토큰 ID \n",
    "tokenizer.token_to_id(\"이유는\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이유는'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 ID -> 토큰 문자열\n",
    "tokenizer.id_to_token(9246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글을 쓴 이유는 다름아닌 '전안법'시행 반대를 주장하기 위해서입니다. 먼저, '전안법'은 전기용품 및 생활용품을 판매하는 업체에서 KC인증마크를 의무적으로 받는 것입니다.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petition_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글을 쓴 이유는 다 름 아닌 ' 전 안 법 ' 시행 반대 를 주장 하기 위해서 입니다 . 먼저 , ' 전 안 법 ' 은 전기 용 품 및 생활 용 품 을 판매 하는 업체 에서 K C 인 증 마 크 를 의무 적으로 받는 것입니다 .\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 id 리스트 -> 문장(str) \n",
    "decode_output = tokenizer.decode(token_output2.ids)\n",
    "decode_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글을 쓴 이유는 다 름 아닌 ' 전 안 법 ' 시행 반대 를 주장 하기 위해서 입니다 . 먼저 , ' 전 안 법 ' 은 전기 용 품 및 생활 용 품 을 판매 하는 업체 에서 K C 인 증 마 크 를 의무 적으로 받는 것입니다 .\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(token_output2.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Wordpiece 방식으로 학습\n",
    "###############################\n",
    "from tokenizers.models import WordPiece  #, BPE\n",
    "from tokenizers.trainers import WordPieceTrainer  #BpeTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "wp_tokenizer = Tokenizer(\n",
    "    WordPiece(unk_token='<unk>')\n",
    ")\n",
    "# pre tokenizer 설정\n",
    "wp_tokenizer.pre_tokenizer = Whitespace()\n",
    "# Trainer 설정\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=20000,\n",
    "    special_tokens=[\"<unk>\", \"<pad>\", \"<eos>\", \"<sos>\", \"<mask>\", \"<sep>\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.172205209732056 초\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "wp_tokenizer.train([\"data/petitions/petition_corpus.txt\"], trainer)\n",
    "print(time.time() - s, \"초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer.save(\"saved_models/petitions_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_wp_tokenizer = Tokenizer.from_file(\"saved_models/petitions_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.get_vocab_size() # 총 어휘수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'彗'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.id_to_token(2000) # id -> 토큰문자열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.token_to_id('彗') # 토큰문자열 -> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=34, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코딩\n",
    "encoding = wp_tokenizer.encode(sports_txt)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '##리',\n",
       " '##미',\n",
       " '##어',\n",
       " '##리',\n",
       " '##그',\n",
       " '역',\n",
       " '##대',\n",
       " '개인',\n",
       " '최',\n",
       " '##다',\n",
       " '##골',\n",
       " '기록',\n",
       " '##을',\n",
       " '보유',\n",
       " '##하고',\n",
       " '있는',\n",
       " '시',\n",
       " '##어',\n",
       " '##러',\n",
       " '##가',\n",
       " '손',\n",
       " '##흥',\n",
       " '##민',\n",
       " '##의',\n",
       " '골',\n",
       " '결정',\n",
       " '##력을',\n",
       " '재',\n",
       " '##차',\n",
       " '극',\n",
       " '##찬',\n",
       " '##했다',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens\n",
    "##토큰 : 연결 subword(토큰) 구분자.\n",
    "# ## 이 없는 토큰은 원래 문자열로 되돌릴 때 앞에 공백을 붙인다.\n",
    "# ## 이 있는 토큰은 그대로 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7394,\n",
       " 8255,\n",
       " 8497,\n",
       " 8364,\n",
       " 8255,\n",
       " 8481,\n",
       " 6184,\n",
       " 8412,\n",
       " 15091,\n",
       " 6895,\n",
       " 8209,\n",
       " 8863,\n",
       " 16825,\n",
       " 8230,\n",
       " 16337,\n",
       " 14896,\n",
       " 14908,\n",
       " 5912,\n",
       " 8364,\n",
       " 8233,\n",
       " 8228,\n",
       " 5806,\n",
       " 9029,\n",
       " 8429,\n",
       " 8225,\n",
       " 4031,\n",
       " 15626,\n",
       " 15272,\n",
       " 6453,\n",
       " 8408,\n",
       " 4133,\n",
       " 8899,\n",
       " 15376,\n",
       " 19]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'프 ##리 ##미 ##어 ##리 ##그 역 ##대 개인 최 ##다 ##골 기록 ##을 보유 ##하고 있는 시 ##어 ##러 ##가 손 ##흥 ##민 ##의 골 결정 ##력을 재 ##차 극 ##찬 ##했다 .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_str = []\n",
    "for token in encoding.tokens:\n",
    "    if token.startswith('##'):\n",
    "        decode_str.append(token[2:])\n",
    "    else:\n",
    "        decode_str.append(' '+token)\n",
    "\n",
    "# print(decode_str)\n",
    "''.join(decode_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Unigram 모델 학습\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "# 토크나이저 객체 생성\n",
    "uni_tokenizer = Tokenizer(\n",
    "    Unigram()\n",
    ")\n",
    "# Pre tokenizer 생성\n",
    "uni_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# trainer 생성\n",
    "trainer = UnigramTrainer(\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<unk>\",\"<pad>\"],\n",
    "    min_frequency=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291.62924098968506 초\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "uni_tokenizer.train([\"data/petitions/petition_corpus.txt\"], trainer=trainer)\n",
    "print(time.time() - s, \"초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tokenizer.save(\"saved_models/petitions_unigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_uni_tokenizer = Tokenizer.from_file(\"saved_models/petitions_unigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=47, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = uni_tokenizer.encode(comment_txt)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['멋',\n",
       " '진',\n",
       " '식',\n",
       " '사',\n",
       " '를',\n",
       " '즐',\n",
       " '기',\n",
       " '기에',\n",
       " '좋은',\n",
       " '장',\n",
       " '소',\n",
       " '-',\n",
       " '채',\n",
       " '식',\n",
       " '메',\n",
       " '뉴',\n",
       " '가',\n",
       " '정말',\n",
       " '훌',\n",
       " '륭',\n",
       " '했습니다',\n",
       " '.',\n",
       " '당',\n",
       " '근',\n",
       " '케',\n",
       " '이',\n",
       " '크',\n",
       " '는',\n",
       " '아',\n",
       " '마',\n",
       " '도',\n",
       " '내가',\n",
       " '먹',\n",
       " '어',\n",
       " '본',\n",
       " '디',\n",
       " '저',\n",
       " '트',\n",
       " '중',\n",
       " '최',\n",
       " '고',\n",
       " '였',\n",
       " '을',\n",
       " '거',\n",
       " '예',\n",
       " '요',\n",
       " '.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2663,\n",
       " 89,\n",
       " 162,\n",
       " 36,\n",
       " 10,\n",
       " 2231,\n",
       " 25,\n",
       " 710,\n",
       " 482,\n",
       " 60,\n",
       " 67,\n",
       " 114,\n",
       " 354,\n",
       " 162,\n",
       " 789,\n",
       " 2401,\n",
       " 8,\n",
       " 250,\n",
       " 2675,\n",
       " 2756,\n",
       " 282,\n",
       " 2,\n",
       " 85,\n",
       " 471,\n",
       " 1553,\n",
       " 3,\n",
       " 509,\n",
       " 11,\n",
       " 63,\n",
       " 127,\n",
       " 9,\n",
       " 1192,\n",
       " 435,\n",
       " 41,\n",
       " 311,\n",
       " 740,\n",
       " 123,\n",
       " 300,\n",
       " 68,\n",
       " 739,\n",
       " 13,\n",
       " 978,\n",
       " 4,\n",
       " 71,\n",
       " 298,\n",
       " 69,\n",
       " 2]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
