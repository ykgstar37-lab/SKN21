{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81228c24-0333-4690-a40a-4110e0c0f6b4",
   "metadata": {},
   "source": [
    "# Contextualized Word Embedding\n",
    "\n",
    "- **Contextualized Word Embedding(문맥 기반 단어 임베딩)** 은 단어를 벡터 공간에 표현하는 과정에서, 해당 단어가 **문장에서 실제로 어떤 의미**로 사용되었는지(문맥·맥락)를 반영하여 벡터를 산출하는 방식을 말한다.\n",
    "- 이는 전통적 Word Embedding(예: Word2Vec, GloVe)처럼 단어별로 고정된 하나의 벡터를 사용하는 것이 아니라, **동일한 단어라도 문맥이 바뀌면 서로 다른 벡터를 생성**한다는 점에서 차이가 있다.\n",
    "  \n",
    "## Word Embedding과의 차이\n",
    "\n",
    "- 기존 Word Embedding 방식은 **동음이의어**(같은 발음에 여러 뜻을 가진 단어)가 **같은 벡터**로 표현되기 때문에, 문장의 의미(내용)이  제대로 표현되지 못할 수 있다.  \n",
    "    - 예: \"밤에 만나자.\"와 \"어제 먹은 밤 정말 맛있었다.\"라는 두 문장에서 '밤'은 서로 다른 의미이지만, word2vec과 같은 Word Embedding 방식에서는 동일한 벡터로 표현된다.\n",
    "- **Contextualized Word Embedding**은 동일한 발음의 단어라도 **문맥에 따라 각기 다른 벡터를 생성하여** 단어의 의미를 정확히 표현하는 것을 목표로 한다.  \n",
    "    - 예: Contextualized Word Embedding은 **동의 이의어의 단어라도 각 단어가 문맥 속에서 어떤 의미로 사용되는지를 반영하여 다르게 벡터화**한다. 따라서 위의 예의 두 문장에서 사용된 '밤'은 서로 다른 벡터로 표현된다.\n",
    "    - 이를 통해 자연어 이해(NLP)에서 동음이의어 처리, 문장 의미 분석, 문맥 기반 질의응답 등에서 정확도를 크게 높인다.\n",
    " \n",
    "## Contextualized Word Embedding 구현 방식\n",
    "\n",
    "### 기본 아이디어\n",
    "\n",
    "Contextualized Word Embedding은 일반적으로 다음 단계로 생성된다.\n",
    "\n",
    "1. **문장 전체를 입력으로 받는다.**\n",
    "\n",
    "   - 입력된 문장은 토큰 단위(단어 또는 서브워드)로 분리되며, 각 토큰은 먼저 **초기 임베딩(Embedding Layer)**을 거친다.\n",
    "   - 이 초기 임베딩은 전통적 Word Embedding과 동일하게 **동음이의어도 하나의 벡터로 표현되는 정적(static) 임베딩**이다.\n",
    "\n",
    "2. **깊은 신경망(Deep Learning Layers)을 통과하며 문맥 정보를 통합한다.**\n",
    "   - RNN 기반 모델을 입력된 토큰을 순차적으로 적용하고 Transformer 기반 모델에서는 Self-Attention을 반복적으로 적용하며 각 토큰이 **앞뒤 단어**, **전체 문장 구조**, **주요 문맥 신호**를 참조하도록 만든다.\n",
    "   - 이 과정에서 동일한 단어라도 문맥에 따라 서로 다른 의미적 표현을 학습하게 된다.\n",
    "\n",
    "3. **문맥 정보를 반영한 최종 토큰 벡터를 생성한다.**\n",
    "   - 여러 층(Attention Layer)을 거친 후, 각 토큰은 문맥적 의미가 반영된 **Contextualized Embedding Vector**를 출력한다.\n",
    "\n",
    "\n",
    "### 사용되는 모델 구조\n",
    "\n",
    "| 세대 | 모델 유형            | 특징                                                                    |\n",
    "| ---- | ------------------- | -----------------------------------------------------------------------|\n",
    "| 1세대 | RNN 기반(LSTM, GRU) | 순차적 처리. 긴 문맥 학습에 어려움. ELMo가 대표.                           |\n",
    "| 2세대 | Transformer 기반    | 병렬 처리, 장기 의존성(Long-range dependency) 해결. BERT, GPT 계열이 대표. |\n",
    "\n",
    "현재는 **Transformer 구조**가 사실상 표준이며, 이는 \"Attention Is All You Need\"(2017) 논문에서 제안된 **Self-Attention** 메커니즘을 기반으로 한다.\n",
    "\n",
    "\n",
    "\n",
    "> - **동음이의어**\n",
    ">     - 같은 발음은 같은데 뜻이 다른 단어를 **동음이의어** 라고 한다.\n",
    ">     - 예로 먹는 \"밤\" 과 해가 떨어진 시간대를 나타내는 \"밤\", 먹는 \"배\"와 타는 \"배\"와 사람의 \"배\"\n",
    "> - **동음다의어**\n",
    ">     - 하나의 단어가 여러개의 관련된 뜻을 가진 것을 **동음다의어**라고 한다.\n",
    ">     - 예) \"머리를 끄덕이다.\", \"머리가 좋다.\" , \"머리가 길다\"  셋다 관련되 있지만 다른 뜻으로 쓰인다.\n",
    ">         - [네이버사전 머리뜻](https://ko.dict.naver.com/#/entry/koko/a90afd48126045a09bc9b6c634b7ff54)을 보면 머리에서 파생된 의미가 11가지가 나와 있다. (중심의미로 부터 파생된 파생의미들.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7778c-0d43-4d4a-a495-ecf78a8cdd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a60c40-ad77-40c4-a89a-0215dfb0e30f",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "- Sequential data 의 특성을 추출하는데 좋은 성능을 보이는 Recurrent Layer를 Feature Extractor로 사용하는 딥러닝 모델.\n",
    "- 시계열 데이터, 자연어처리, 음성데이터 처리에 많이 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28078254-f3d9-4917-868a-6c9e684f385f",
   "metadata": {},
   "source": [
    "# Sequential 데이터란\n",
    "\n",
    "- 데이터의 **순서가 있고 그 순서 정보가 중요한 데이터셋**\n",
    "    - 순서가 달라질 경우 의미가 바뀌거나 손상되는 데이터.\n",
    "    - 처리시 **순서의 흐름이(Context:문맥) 중요한 데이터**\n",
    "        - 현재 순서의 값을 처리할 때 이전 순서까지의 처리 결과들을 모두 같이 고려해야 하는 경우.\n",
    "- 예\n",
    "    - 자연어 텍스트\n",
    "    - 일정한 주기로 샘플링된 영상, 음성\n",
    "    - 시계열(time series) 데이터\n",
    "> - Time series\n",
    ">    - Sequential 데이터 중 데이터의 순서에 뿐 아니라 해당 **데이터가 발생한 시점 정보가 중요한 데이터셋을** 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67f648-4c58-4f51-af08-785ce7e85937",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Network) 구조\n",
    "\n",
    "## 개요\n",
    "- RNN은 Feature Extractor로 Recurrent Layer을 사용하는 딥러닝 모델(네트워크)을 말한다.\n",
    "- Recurrent Layer는 sequential 데이터 처리에 좋은 성능을 낸다.\n",
    "\n",
    "![rnn_outline](figures/rnn/RNN_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e30211-1b9b-42cb-9625-1fbb789fc6aa",
   "metadata": {},
   "source": [
    "## Recurrent Layer 구조\n",
    "- Sequential 데이터는 순서에 맞춰 처리하는 것이 중요하다. 그러나 DNN과, CNN에서 사용되는 Fully Connected Layer나 Convolutional Layer는 순서를 고려하지 않고 특성 추출 한다.\n",
    "- RNN은 순서대 입력되는 데이터를 반복 처리하는  **Recurrent Layer**를 이용해 **순서를 가만해서 Feature vector를 추출하고** 그 Feature vector를 Estimator Layer에 전달해 추론한다.\n",
    "- **순서를 가만한다는 것**은 이전 순서의 내용을 기억한다는 것을 말한다. 이것을 memory system이라고 한다.\n",
    "\n",
    "### 메모리 시스템(Memory system)\n",
    "- **이전 시점까지의 특성들을 이용해 현재 시점의 특성을 추출한다.**\n",
    "    - Sequential Data는 단순히 순서대로 처리하는 것 뿐 만 그 내용들을 **기억(memory)** 하고 **참고** 해서 현재 순서의 내용을 처리해야 한다.\n",
    "    - 메모리 시스템이란 **\"이전 단계의 처리결과를 기억하고 그것을 현재 단계 처리에 사용하는\"** 것으로 Recurrent layer의 핵심 로직이다.\n",
    "\n",
    "> Sequential 데이터 처리에서 순서대로 처리할 때 각각의 단계(순서)를 **time step** 이라고 한다.\n",
    " \n",
    "![memory system](figures/rnn/01_memory_system.png)\n",
    "\n",
    "- 예를 들어 4일간의 주가 변화로 5일째 주가를 예측하려면 입력받은 4일간의 주가 정보를 순서대로 기억하고 있어야 한다.\n",
    "- Fully Connected Layer나 Convolution Layer의 출력은 이전 Data에 대한 처리와 상관없이 현재 시점의 데이터만을 기준으로 특성(feature)을 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a4b01-660e-46b7-85ff-ea513a4e3bb4",
   "metadata": {},
   "source": [
    "### Simple RNN 구조\n",
    "![rnn arch](figures/rnn/02_simplernn.png)\n",
    "\n",
    "\n",
    "![rnn_arch2](figures/rnn/03_simplernn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefa278-785b-4dcb-aaa7-622a6be6c162",
   "metadata": {},
   "source": [
    "- Recurrent Layer는 Linear layer 구조에 **재귀 순환(반복)의 로직**이 들어간 것으로 이해할 수 있다.\n",
    "- Layer의 한 step은 입력으로 **현재 시점(time step)의 입력 데이터($X_{t}$)** 와 **이전 시점까지의 처리결과($h_{t-1}$)** 를 같이 받는다.  (t: time step)\n",
    "- $X_{t}$와 $h_{t-1}$ 에 각각의 weight($W_{xh}\\,, W_{hh}$)를 이용해 가중합을 구하고 그 둘의 합계를 activation(tanh) 함수에 넣어 비선형성을 추가한 결과가 현재 time step의 출력값이된다. **이 출력값이 현재 time step의 feature(특성)값** 이다.\n",
    "    - 이 출력값은 현재 time step의 feature 이자 **다음 time step의 feature 계산의 hidden state 입력값으로 사용된다.**\n",
    "    - $X_{t}$(현재 time step의 입력)와 $h_{t-1}$(hidden state) 에는 각각 다른 weight($W_{xh}$ , $W_{hh}$)들이 적용된다.\n",
    "- 매 time step마다 동일한 weight $W_{xh}$와 $W_{hh}$ 가 사용된다.\n",
    "    - time step 마다 새로운 weight들이 적용되는 것이 아니다.\n",
    "\n",
    "> #### Hidden state\n",
    "> - 이전 step까지의 처리결과를 말한다..\n",
    "> - **메모리 시스템의 메모리(기억)** 에 해당한다.\n",
    "> - 현재 timestep의 feature를 추출할 때 현재 시점의 입력과 함께 recurrent layer의 입력으로 들어간다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe180f6-3fe2-4b25-b629-c4f55ad4113e",
   "metadata": {},
   "source": [
    "## NLP 문제 유형과 RNN 구조\n",
    "\n",
    "|유형|구조1|구조2|구현서비스(App)|\n",
    "|:-|:-|:-:|:-|\n",
    "|**Many to One**|입력: Sequence Vector(여러개의 Vector)<br>출력: 1개 Vector|![mto](figures/rnn/04_many_to_one.png)|$\\scriptscriptstyle\\blacksquare$ Text classification(감성 분석)|\n",
    "|**One to Many**|입력: 1개 Vector<br>출력: Sequence Vector|![otm](figures/rnn/04_one_to_many.png)|$\\scriptscriptstyle\\blacksquare$ Image captioning<br>$\\scriptscriptstyle\\blacksquare$ 문장생성|\n",
    "|**Many to Many**|입력: Sequence Vector<br>출력: Sequence Vector|![mtm1](figures/rnn/04_many_to_many.png)|$\\scriptscriptstyle\\blacksquare$ 개체명인식<br>$\\scriptscriptstyle\\blacksquare$ 품사태깅|\n",
    "|**Seq2Seq**|many to one과 one to many 를 연결한 구조.<br>Encoder, Decoder 구조<br>Encoder는 특성을 추출하고 <br>Decoder는 Encoder가 추출한 특성을 이용해 결과를 생성.|![seq2seq](figures/rnn/04_seq2seq.png)|$\\scriptscriptstyle\\blacksquare$ Machine Translation<br>$\\scriptscriptstyle\\blacksquare$ Chatbot|\n",
    "\n",
    "\n",
    "- **텍스트 분류(Text classification)**\n",
    "   - 입력받은 문장을 분류하는 문제로 대표적으로 감성분석(sementic analysis)가 있다.\n",
    "   - 감성분석\n",
    "       -  입력받은 텍스트가 어떤 감정의 글인지를 분류한다. 일반적으로 긍정, 중립, 부정적 인지를 분류한다.\n",
    "- **Image captioning**\n",
    "   - 입력받은 이미지를 설명하는 문장을 생성하는 문제.\n",
    "- **개체명인식(Named Entity Recognition)**\n",
    "   - 문장의 각 단어가 어떤 종류(의미) 인지를 찾는 문제\n",
    "   - 미국에 사는 톰은 스무살입니다. ==> 미국: 위치, 톰: 이름, 스므살: 나이\n",
    "- **품사태깅(Pos tagging)**\n",
    "   - 문장의 각 토큰의 품사를 찾는 문제\n",
    "   - 미국에 사는 톰은 스무살입니다. ==> 미국: 대명사, 예: 조사,  톰: 대명사, 은: 조사, 스무: 명사, 살: 명사, 이다: 동사\n",
    "- **Chatbot**\n",
    "   - 입력받은 문장에 대한 답을 하는 시스템.\n",
    "   - Encoder는 질문을 받아 처리하고 Decoder는 답변을 생성하는 seq2seq 구조를 사용한다.\n",
    "- **Machine translation**\n",
    "   - 번역 시스템\n",
    "   - Encoder는 번역 대상문장을 입력받아 처리하고 Decoder는 번역 문장을 생성하는 seq2seq 구조를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212f68d-b27c-4725-901c-7e98a422ef8d",
   "metadata": {},
   "source": [
    "## Recurrent Layer 구조들\n",
    "\n",
    "### Bidirectional RNN\n",
    "- 같은 정보를 정방향과 역방향 두 방향으로 주입해 정확도를 높인다.\n",
    "- Non-Auto Regressive 모델의 경우 bidirectional RNN 사용이 권장된다.\n",
    "\n",
    "> - **Auto Regressive 모델**\n",
    ">     - 이전 time step에 대한 출력결과를 다음 time step의 입력으로 사용하는 구조.\n",
    ">         - Text 생성 모델이 대표적인 Auto Regressive 모델이다.\n",
    ">     - 이전 출력의 결과에 의존하기 때문에 bidirectional RNN을 사용할 수 없다.\n",
    ">     - Non Auto Regressive 모델은 현재의 상태가 앞/뒤 상태에 따라 정해지는 경우로 보통 앞/뒤의 상태를 모두 참조할 경우 성능이 올라간다. 그래서 단방향 RNN보다 bidirectional RNN의 사용이 권장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a16bb-d113-4d08-814a-6065b57f96c2",
   "metadata": {},
   "source": [
    "![bidirectional](figures/rnn/08_bidirectional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb53f18-66db-474d-acdf-2963c6837916",
   "metadata": {},
   "source": [
    "### Stacking(Multi Layer) RNN\n",
    "\n",
    "- Recurrent Layer를 쌓아 모델의 용량을 늘려 표현능력(Represenational capacity)를 증가시킨다.\n",
    "- 여러층을 쌓은 경우 먼저 쌓은 layer의 모든 time step별 출력이 다음 Layer의 입력 데이터로 사용된다.\n",
    "- Layer을 쌓으면 표현능력은 증가하는 대신 계산 비용이 많이들고 **과대적합(Overfitting)이** 발생할 수 있다.\n",
    "    - 과대적합을 막기 위해 드롭아웃 레이어를 추가할 수있다.\n",
    "    - Recurrent Layer에서 Dropout layer는 매 time step 마다 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbc98e-5794-41d4-9f86-4339790557fd",
   "metadata": {},
   "source": [
    "![stacking rnn](figures/rnn/08_multi_layer_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aeefa1-2cdf-4535-9227-c7a125fd8883",
   "metadata": {},
   "source": [
    "## Pytorch RNN Layer\n",
    "\n",
    "- [nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "    - **파라미터**\n",
    "        - input_size: 입력 데이터의 크기 (feature수)\n",
    "        - hidden_size: Layer의 Hidden size\n",
    "        - num_layers: 몇 층으로 Layer을 쌓을지 개수\n",
    "        - nonlinearity: 활성함수(Activation Function) 지정. 문자열로 'tanh' (Default) 또는 'relu' 둘 중 하나 지정.\n",
    "        - batch_first: True - (batch, seqence len, ..) False - (sequence len, batch, ..). Default: False\n",
    "        - dropout: Dropout rate 비율\n",
    "        - bidirectional:  양방향 적용 여부. Default: False\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966f316-99c6-408d-8b34-63f83eefbf61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "  \n",
    "### RNN Layer의 input / output tensor 의 shape\n",
    "\n",
    "### 추론시 Input\n",
    "- 입력으로 두개를 받는다.\n",
    "    - Input_Data, Hidden_state\n",
    "- **Input_data의 shape**\n",
    "    - (Sequence_legnth, batch_size, feature_shape)\n",
    "        - pytorch는 입력으로 batch보다 sequence length가 먼저 온다.\n",
    "        - `batch_first=True`로 설정하면 (**batch_size**, seq_len, feature_shape) 순이 된다.\n",
    "        - ex)  주가 데이터\n",
    "            - feature: 시가, 종가, 최고가, 최저가 (4개)\n",
    "            - sequence: 100일치( 100개의 feature가 하나의 입력이 된다.)\n",
    "            - batch size: 30 (100일치 데이터 30개)\n",
    "            - (100, 30, 4)\n",
    "    ![input tensor shape](figures/rnn/07_input_shape.png)\n",
    "- **Hidden state의 shape**\n",
    "    - 시작(초기) hidden state로 입력하지 않으면 0이 들어간다.\n",
    "    - shape은 아래 hidden state 설명 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04175c-65d5-43dd-b3dc-5646d390b47a",
   "metadata": {},
   "source": [
    "### Output\n",
    "- **(output_data, hidden_state)**\n",
    "    - output_data과 hidden state를 tuple로 묶어서 반환한다.\n",
    "- **Output_data**:\n",
    "    - 매 time step의 출력결과\n",
    "    - Many to Many일 경우 이것을 출력 Feature들로 사용한다.\n",
    "- **hidden_state**\n",
    "    - 마지막 time step의 출력결과\n",
    "    - Many to one의 경우 이것을 출력 Feature로 사용한다.\n",
    "  \n",
    "#### Output shape\n",
    "- **(Sequence length, batch_size, hidden_size * D)**\n",
    "    - D: 양방향(bidirectional) 이면 2 아니면 1\n",
    "    - batch_first=True 로 설정하면 (**batch_size**, seq_len, hidden_size * D) 순이 된다.\n",
    "    - ex)\n",
    "        - RNN Layer의 hidden size가 256 인 경우. (sequence length: 100, batch size: 30, bidirection=False)\n",
    "        -  (100, 30, 256)\n",
    "![hidden state shape](figures/rnn/07_hidden_state_shape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e97aff-48db-4083-b6fb-10651ac518ba",
   "metadata": {},
   "source": [
    "#### Hidden state\n",
    "- 마지막 time step의 출력결과\n",
    "    - **(D * layer수, batch_size, hidden_size)**\n",
    "    -  D: 양방향(bidirectional) 이면 2 아니면 1\n",
    "    -  layer수: multi layer일 경우 layer stack 수\n",
    "        -  각 Layer 별 마지막 time step의 결과들이 묶여서 hidden state 가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963ca81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m10 packages\u001b[0m \u001b[2min 235ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 5.40s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch==2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2227eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624755a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummpy 입력데이터 생성 \n",
    "# (batch_first=False)\n",
    "#  - 3차원 shpae: (seq_length, batch_size, feature수-embedding 차원)\n",
    "\n",
    "input_data = torch.randn((30, 100, 4))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02616da3-d879-4a11-b63e-804ede8241b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 256])\n",
      "torch.Size([1, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "# 입력 shape: (30, 100, 4)\n",
    "rnn1 = nn.RNN(\n",
    "    input_size=4, # 개별 sequence의 feature 수\n",
    "    hidden_size=256, # CNN: filter수, Linear: output_features 수 -> 몇개 특성을 추출할지 개수.\n",
    ")\n",
    "# 나머지는 기본값\n",
    "# num_layers: 1 (몇개 layer를 연결할 것인지)\n",
    "# nonlinearity: tanh (활성함수)\n",
    "# bidirectional: False (단방향)\n",
    "# batch_first: False (seq, batch, ...)\n",
    "\n",
    "###### 특성 추출\n",
    "out1, hidden1 = rnn1(input_data)  \n",
    "# 입력: rnn1(input_data, hidden_state) hidden_state: 첫번째 timestep에 입력할 hidden state값. 생략: 0\n",
    "\n",
    "# num_layers: 1, bidirectional: False\n",
    "print(out1.shape) # [30: seq_len, 100: batch_size, 256: hidden_size] -> 모든 seq에 대한 hidden state들\n",
    "print(hidden1.shape) # [1, 100, 256] => 마지막 timestep의 hidden state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d084c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 256])\n",
      "torch.Size([3, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "rnn2 = nn.RNN(\n",
    "    input_size=4,\n",
    "    hidden_size=256,\n",
    "    num_layers=3   # stacked layer => 3개를 연결.\n",
    ")\n",
    "\n",
    "out2, hidden2 = rnn2(input_data)\n",
    "print(out2.shape)    # [30, 100, 256] - stacked layer의 마지막 layer출력한 모든 timestep의 hidden state들\n",
    "print(hidden2.shape) # [3, 100, 256]  - (각 layer들의) 마지막 timestep의 hidden state들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b4496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 512])\n",
      "torch.Size([6, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "rnn3 = nn.RNN(\n",
    "    input_size=4,\n",
    "    hidden_size=256,\n",
    "    num_layers=3,\n",
    "    bidirectional=True  # 양방향 RNN.\n",
    ")\n",
    "out3, hidden3 = rnn3(input_data)\n",
    "print(out3.shape) # [30, 100, 512]  # 512 - 정방향 256, 역방향 256 둘을 concatenate(붙임)\n",
    "print(hidden3.shape) [6, 100, 256]  # 6: layer수-3, 각layer들: 2 방향 -> 마지막 hidden state개수: 3 * 2 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9f640-e5cb-4439-9e34-40d7cb702a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fe880b3-a1ca-4158-ad64-0856b90bea78",
   "metadata": {},
   "source": [
    "# RNN의 Back Propagation \n",
    "- **BPTT(Back Propagation Through Time)** 이라고 한다.\n",
    "- RNN time step 별로 순환 반복는 hidden state의 흐름의 역방향으로 전파되는 gradient 들이 chain rule에 의해 곱해지면서 weight가 업데이트 된다.\n",
    "    - 각 time step 마다 사용되는 weight는 같기 때문에 1 step에서 weight는 여러번(time step수만큼)에 걸쳐 업데이트 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f75381-a29a-4e97-93ff-0bea964a798f",
   "metadata": {},
   "source": [
    "## RNN(Simple RNN)의 문제 \n",
    "- 입력 데이터의 sequence가 길수록 Gradient Vanishing이 발생해 초기 Sequence에 대한 학습이 안되는 문제가 RNN의 고질적인 문제이다.\n",
    "    - RNN은 activation 함수로 tanh()를 사용한다. tanh()의 gradient는 0 ~ 1 사이의 실수가 나온다. 그래서 sequence가 길어지면 **초기 time step의 값에 대한 weight가 업데이트가 되지 않게** 된다.  \n",
    "- Time step이 길어지면 초기 Sequence에 정보가 hidden state에서 점점 사라지는 **기억력 소실문제**가 발생한다.\n",
    "      \n",
    "![BPTT](figures/rnn/09_bptt.png)\n",
    "\n",
    "- 이런 Simple RNN의 문제 모델 구조로 해결한 모델이 **LSTM이나 GRU** 모델이다. Sequence 데이터 처리 모델로 이 둘을 주로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2ca22-3cea-4d5f-b9b1-33fb14998d43",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "- Long Short Term Memory (장기, 단기 기억)\n",
    "\n",
    "## 개요\n",
    "- Simple RNN을 개선한 변형 알고리즘\n",
    "    - **오래 기억할 것은 유지하고 잊어버릴 것은 빨리 잊어버리자**\n",
    "    - 바로 전 time step의 처리 결과와 전체 time step의 처리 결과를 같이 받는다.\n",
    "\n",
    "\n",
    "![lstm](figures/rnn/09_lstm_layer.png)\n",
    "\n",
    "- Simple RNN과 다르게 두개의 이전 time step까지의 처리결과를 사용한다.\n",
    "    - **Cell State**\n",
    "        - Long term memory 로 전체 step에 대한 누적 기억값\n",
    "    - **Hidden State**\n",
    "        - Short term memory 로 이전 sequence 에 대한 기억값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a87ce-dd27-42ec-90f6-80d64b20f686",
   "metadata": {},
   "source": [
    "## LSTM  Gate\n",
    "- Gate\n",
    "    - LSTM은 Layer내에 여러개의 **Gate 연산** 통해 성능을 높인다.\n",
    "    - Sigmoid연산을 이용해 값의 얼마를 적용할지 정한다. 이것을 문이 열리고 닫히는 의미로 Gate라고 한다.\n",
    "- LSTM은 3개 Gate를 사용한다.\n",
    "    -  **Forget gate**\n",
    "    -  **Input gate**\n",
    "    -  **Output gate**\n",
    "-  각 Gate는 hidden state와 Input data에 곱하는 Weight들을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7827d9d-bc6f-4e8e-ae6f-79f9f83ceaa0",
   "metadata": {},
   "source": [
    "![lstm](figures/rnn/10_lstm_layer_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa52849-2742-48a8-91ba-b73ecf2d1a3b",
   "metadata": {},
   "source": [
    "### Forget gate\n",
    "![forget gate](figures/rnn/11_forget_gate.jpg)\n",
    "\n",
    "- 이전 time step의 hidden state와 현재 time step의 입력데이터를 기준으로 장기 기억인 cell state에서 얼마나 잊을 지 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511772d5-da37-4504-adde-143906285e20",
   "metadata": {},
   "source": [
    "### Input Gate\n",
    "\n",
    "![input gate](figures/rnn/12_input_gate.jpg)\n",
    "- 이전 time step의 hidden state와 현재 time step의 입력데이터를 cell state에서 얼마나 반영할 지 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69ac70-0a6e-4926-8971-9297f8ceab9b",
   "metadata": {},
   "source": [
    "### Cell State update\n",
    "\n",
    "![update](figures/rnn/13_cell_state_update.jpg)\n",
    "- cell state는 forget gate의 결과를 곱해서 얼마나 잊을지 결정하고 input gate의 결과를 더해서 현재 입력에 대한 정보를 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ac17d-3a9b-42c2-b7ec-4eac217cbb60",
   "metadata": {},
   "source": [
    "### Output Gate\n",
    "![output gate](figures/rnn/14_output_gate.jpg)\n",
    "- 이전 time step의 hidden state와 현재 time step의 입력데이터, 현재 step의 입력이 반영된 Cell state를 이용해 출력 결과를 계산한다.\n",
    "-  **Output gate의 계산 결과는 다음 step에 hidden state(단기기억)로 전달된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c487433-a5bd-4754-aecc-587147430e05",
   "metadata": {},
   "source": [
    "<Image 참조> https://www.pluralsight.com/guides/introduction-to-lstm-units-in-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3387a-4990-45e7-81be-ce555a7ea692",
   "metadata": {},
   "source": [
    "## Pytorch LSTM layer\n",
    "\n",
    "- [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "    - **파라미터**\n",
    "        - input_size: 입력 데이터의 shape\n",
    "        - hidden_size: Layer의 Hidden size\n",
    "        - num_layers: 몇층으로 Layer을 쌓을지 개수\n",
    "        - batch_first: True - (batch, seqence len, ..) False - (sequence len, batch, ..). Default: False\n",
    "        - dropout: Dropout rate 비율\n",
    "        - bidirectional:  양방향 적용 여부. Default: False\n",
    "     \n",
    "### 추론시 Input tensor 구조\n",
    "- Input_data, (Hidden_state, Cell_state)\n",
    "- Input_data의 shape\n",
    "    - **(sequence_length, batch_size, input_feature_shape)**\n",
    "- (Hidden_state, Cell_state)는 생략시 0 입력된다.\n",
    "    - Hidden_state의 shape\n",
    "        - **(D * layer수, batch_size, hidden_size)**\n",
    "        - D: 양방향(bidirectional) 이면 2 아니면 1\n",
    "    - Cell_state의 shape\n",
    "        - **(D * layer수, batch_size, hidden_size)**\n",
    "        - D: 양방향(bidirectional) 이면 2 아니면 1\n",
    "### Output tentor 구조\n",
    "- Output_data, (Hidden_state, Cell_state)\n",
    "  \n",
    "#### Output_data의 shape\n",
    "- 모든 timestep의 출력결과를 묶어서 반환\n",
    "- **(sequence length, batch_size, D * hidden_size)**\n",
    "#### Hidden_state의 shape\n",
    "- 마지막 timestep의 출력결과\n",
    "- **(D * layer수, batch_size, hidden_size)**\n",
    "- D: 양방향(bidirectional) 이면 2 아니면 1\n",
    "#### Cell_state의 shape\n",
    "- cell state(장기기억) 값\n",
    "- **(D * layer수, batch_size, hidden_size)**\n",
    "- D: 양방향(bidirectional) 이면 2 아니면 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e034c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_data = torch.randn(30, 100, 4)  \n",
    "# (seq_len, batch, feature수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808747f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 256])\n",
      "torch.Size([1, 100, 256])\n",
      "torch.Size([1, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "# 레이어수: 1, 단방향\n",
    "lstm1 = nn.LSTM(\n",
    "    input_size=4,\n",
    "    hidden_size=256 # 추출할 특성개수.\n",
    ")\n",
    "\n",
    "out1, (hidden1, cell1) = lstm1(input_data)\n",
    "\n",
    "print(out1.shape) # 30, 100, 256\n",
    "print(hidden1.shape)\n",
    "print(cell1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad394529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 256])\n",
      "torch.Size([3, 100, 256])\n",
      "torch.Size([3, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "# 레이어수 : 3, 단방향\n",
    "lstm2 = nn.LSTM(\n",
    "    input_size=4,\n",
    "    hidden_size=256,\n",
    "    num_layers=3\n",
    ")\n",
    "\n",
    "out2, (hidden2, cell2) = lstm2(input_data)\n",
    "print(out2.shape)\n",
    "print(hidden2.shape)\n",
    "print(cell2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9c4e11-9932-4e9b-a0bc-13733a4891f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 512])\n",
      "torch.Size([6, 100, 256])\n",
      "torch.Size([6, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "# 레이어수 : 3, 양방향\n",
    "lstm3 = nn.LSTM(\n",
    "    input_size=4,\n",
    "    hidden_size=256,\n",
    "    num_layers=3,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "out3, (hidden3, cell3) = lstm3(input_data)\n",
    "print(out3.shape)\n",
    "print(hidden3.shape)\n",
    "print(cell3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48123082-fc82-40bb-b514-95131fbc4dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4898ebeb",
   "metadata": {},
   "source": [
    "# GRU(Gated Recurrent Units) 모델\n",
    "- https://arxiv.org/pdf/1406.1078\n",
    "- LSTM의 RNN의 한계점인 기억력 소실문제를 해결하여 긴 sequence의 데이터에서도 좋은 성능을 내는 모델이다. 그러나 복잡한 구조로 parameter가 많아지게 되었고 연산량이 많은 문제점이 있다.\n",
    "    - parameter가 많아지면서 데이터양이 부족할 경우 과대적합이 발생하고 연산량이 많아 학습에 많은 시간이 걸리게 된다.\n",
    "- LSTM의 이런 문제를 개선하기 위한 변형 모델이 GRU이다.\n",
    "\n",
    "## LSTM과 차이\n",
    "1. LSTM은 forget gate, input gate, output gate 세개의 Gate연산을 함. GRU는 **reset gate와 update gate** 로 흐름을 제어한다.\n",
    "2. LSTM은 이전 처리결과로 Cell State, Hidden State 두개가 있었는데 이것을 하나로 합쳐 **Hidden State**로 출력한다.\n",
    "\n",
    "## GRU 성능\n",
    "- GRU는 적은 파라미터 수와 연산비용이 적게 드는 것에 비해 LSTM과 비슷한 성능을 내는 것으로 알려졌다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85572303",
   "metadata": {},
   "source": [
    "\n",
    "## GRU Cell 구조\n",
    "\n",
    "![gru_cell](figures/rnn/23_gru_cell.png)    \n",
    "[이미지 Source](https://www.oreilly.com/library/view/advanced-deep-learning/9781789956177/8ad9dc41-3237-483e-8f6b-7e5f653dc693.xhtml)\n",
    "\n",
    "- **Reset Gate**\n",
    "    - 이전 timestep까지의 hidden state(feature)를 현재 timestep의 hidden state(feature) 계산시 얼마나 반영할 지 비율을 결정하는 gate.\n",
    "    - $r_{t} = \\sigma(h_{t-1}\\cdot U_{r} + X_{t}\\cdot W_{r})$\n",
    "        - $U_{r},\\, W_{r}$ 는 파라미터\n",
    "        - $\\sigma$: sigmoid(logisic) 함수\n",
    "- **Update Gate**\n",
    "    - 현재 timestep의 hidden state($h_t$)를 계산할 때 이전 time step까지 정보($h_{t-1}$)와 현재 time step의 정보($X_t$)를 각각 얼마나 반영할지 비율을 정의한다.\n",
    "    - $z_{t} = \\sigma(h_{t-1}\\cdot U_{z} + X_{t}\\cdot W_{z})$\n",
    "        - $U_{z},\\, W_{z}$ 는 파라미터\n",
    "        - $\\sigma$: sigmoid(logisic) 함수\n",
    "    - $h_t$를 계산할 때 $z_{t}$ 는 이전 정보인 $h_{t-1}$을 얼마나 반영할지 $1-z_{t}$는 현재 정보를 얼마나 반영할 지를 정한다.\n",
    "- **Cell의 출력값인 $h_t$ 계산**\n",
    "    - $z_{t}\\times h_{t-1} + tanh(h_{t-1} * r_{t}+X_{t}\\cdot W)\\times(1-z_{t})$\n",
    "    - 이전 정보에는 $z_t$를 곱해 얼마나 $h_t$ 에 더할지 연산\n",
    "    - 현재 정보($X_t$)에는 이전 정보를 일부를 반영한다. 이전 정보를 얼마나 반영할지를 reset gate 결과를 곱해 결정한다. 활성화 함수 tanh를 이용해 비선형성을 추가 한 결과에 $1-z_t$를 곱한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd31663",
   "metadata": {},
   "source": [
    "## Pytorch GRU\n",
    "- `nn.GRU` 클래스 이용\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "- **입력**\n",
    "    - **input**: (seq_length, batch, hidden_size) shape의 tensor. (batch_first=False), batch_first=True이면 `seq_length`와 `batch` 위치가 바뀐다.\n",
    "    - **hidden**: (D * num_layers, batch, hidden_size) shape의 Tensor. D(양방향:2, 단방향:1), hidden은 생략하면 0이 입력됨.\n",
    "- **출력** - output과 hidden state가 반환된다.\n",
    "    - **output**\n",
    "        - 모든 sequence의 처리결과들을 모아서 제공.\n",
    "        - shape: (seq_length, batch, D * hidden_size) : D(양방향:2, 단방향:1), batch_first=True이면 `seq_length`와 `batch` 위치가 바뀐다.\n",
    "    - **hidden**\n",
    "        - 마지막 time step 처리결과\n",
    "        - shape: (D * num_layers, batch, hidden) : D(양방향:2, 단방향:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475b2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_data = torch.randn((30, 100, 4))\n",
    "# batch_first: False 기준 - (30: seq_len, 100: batch_size, 4: timestep별 feature수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a19063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 256])\n",
      "torch.Size([1, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "# 단방향, 레이어수 1\n",
    "gru1 = nn.GRU(\n",
    "    input_size=4, # feature수\n",
    "    hidden_size=256 # (timestep별로)추출할 특성 개수\n",
    ")\n",
    "# num_layers=1, bidirectional=False, batch_first=False (default)\n",
    "\n",
    "out1, hidden1 = gru1(input_data)\n",
    "\n",
    "print(out1.shape)    # 모든 timestep의 hidden state를 모아서 반환.\n",
    "print(hidden1.shape) # 마지막 timestep의 hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1758596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 512])\n",
      "torch.Size([2, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "gru2 = nn.GRU(\n",
    "    input_size=4, hidden_size=256,\n",
    "    bidirectional=True   # 양방향 GRU\n",
    ")\n",
    "out2, hidden2 = gru2(input_data)\n",
    "\n",
    "print(out2.shape)\n",
    "print(hidden2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0266741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 100, 512])\n",
      "torch.Size([6, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "# 양방향, layer 수: 3\n",
    "gru3 = nn.GRU(\n",
    "    input_size=4, hidden_size=256, bidirectional=True,\n",
    "    num_layers=3\n",
    "\n",
    ")\n",
    "out3, hidden3 = gru3(input_data)\n",
    "print(out3.shape)\n",
    "print(hidden3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d159411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4516a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
