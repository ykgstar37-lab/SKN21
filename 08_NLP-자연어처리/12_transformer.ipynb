{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de11e1a-c190-4432-9fcd-2a604abcd715",
   "metadata": {},
   "source": [
    "# Transformer \n",
    "\n",
    "- [Attention Is All You Need 2017](https://arxiv.org/pdf/1706.03762) 논문에서 제시한 모델로 RNN모듈을 사용하지 않고 **Attention 모듈로만 구성한 Encoder-Decoder 구조의 seq2seq 모델**이다.\n",
    "    - RNN → RNN + attention → Transformer\n",
    "- Transformer 이후 자연어 관련 pretrained 모델들은 대부분 Transformer의 Encoder나 Decoder를 변형하거나 단순히 층을 깊게 쌓는 형태로 구현하여 성능을 올렸다.\n",
    "    - Google의 BERT(Bidirectional Encoder Representations from Transformers)는 Transformer의 Encoder를 이용해 구현했다.\n",
    "    - OpenAI의 GPT(Generative Pre-trained Transformer)는 Transformer의 Decoder를 이용해 구현했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b40ae2-ac88-4572-bda4-0752f5eb14a6",
   "metadata": {},
   "source": [
    "## Seq2Seq의 문제와 Transformer\n",
    "- Seq2Seq 의 문제\n",
    "    - 장기기억 소실의 문제.(long term dependency problem)\n",
    "        - seq가 길어지면 초기 timestep의 정보가 점점 소실된다.\n",
    "    - 병렬처리를 못해 연산 효율이 떨어진다.\n",
    "        - time step **순서대로** 처리해야 하기 때문에 병렬처리가 안된다.\n",
    "- **장기기억 소실문제는** seq2seq에 **Attention mechanism**을 적용해 해결할 수 있다.\n",
    "  \n",
    "![img](figures/transformer_advantage.gif)\n",
    "- Transformer는  encoder와 decoder에서 RNN을 제거하고 **Attention 모듈로 변경하여 병렬 처리가 가능**하도록 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d1b48-938f-4ab4-935e-3af1c0928ac4",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "- Encoder - Decoder  구조\n",
    "- Encoder와 Decoder가 **self-attention 모듈**로 구성된다. 그래서 RNN을 쓰지 않고 입력 sequence와 출력 sequence를 처리한다.\n",
    "- **Positional Encoding**\n",
    "    - RNN을 사용하지 않기 때문에 입력 sequence의 토큰들에 순서정보가 없다. 그 순서정보를 Embedding에 추가하는 역할.\n",
    "    - 문장을 구성하는 각 토큰들의 embedding vector에 그 토큰 순서 정보를 가지는 positional encoding 값을 더해준다.\n",
    "    - Positional Encoding 값은 sine/cosin 주기함수를 이용해 계산한다.\n",
    "\n",
    "![img](figures/transformer_architecture2.png)\n",
    "\n",
    "\\[**transformer의 구조**\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713c7bb-49ec-4980-839f-3bb8c5e562cb",
   "metadata": {},
   "source": [
    "![img](figures/transformer_architecture1.png)\n",
    "\n",
    "\\[**Encoder와 Decoder의 구조**\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5d460-ad2f-460f-a617-a328bbc6d150",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "- Attention을 자기 자신한테 적용한다는 의미\n",
    "    - seq2seq 의 attention은 decoder의 sequence가 encoder의 hidden state에 attention을 취한다. (디코더가 인코더의 어디에 집중할 지 찾는다.)\n",
    "    - self attention은 attention을 자기 자신에게 취한다. (같은 문장 내에서 단어들이 서로 어떤 관계를 가지는지 찾는다.)\n",
    "- Self-attention은 하나의 단어가 자신이 속한 문장의 다른 단어들과 어떤 연관성을 가지는지 찾는다.\n",
    "\n",
    "![img](figures/transformer_self-attention.png)\n",
    "\n",
    "- 예를 들어 \"The animal didn't cross the street because it was too tired.\" 라는 문서에 있는  `it`을 encoding 한려고 한다. \n",
    "- `it`을 문맥에 맞게 embedding하기 위해서는 이 문서안에서 어떤 의미로 쓰였는지 알아야 한다. \n",
    "- 그것을 알기 위해서는 `it` 이 가리키는(관련있는) 것이 어떤 것인지 알아야 한다. 그것을 다른 문서에서 찾는 것이 아니라 `it` 있는(대상 단어가 있는) 문서에서 다른 단어들과의 연관성에서 찾는다.\n",
    "- **의미를 파악하려는 문서와 그 의미를 찾는 문서가 동일하기 때문에 self-attention**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5498c014-90d9-48cc-9a3c-3e2cd1a99970",
   "metadata": {},
   "source": [
    "### Self Attetion 과정\n",
    "- 같은 문장내에서 단어들 간의 관계를 고려하는 것.\n",
    "  \n",
    "![self-attention](figures/transformer_self_attention.png)\n",
    "\n",
    "- **수식**\n",
    "\\begin{align}\n",
    "&\\text{Attention(Q,K,V)} = \\text{softmax}(\\cfrac{QK^T}{\\sqrt{d_k})})V \\\\\n",
    "&\\small \\text{Q: Query, K: Key, V: Value,}\\;d_{k}: \\text{embedding 차원수}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af4e5c",
   "metadata": {},
   "source": [
    "## Multi-Head Attention에서 Q, K, V의 역할\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b08d5e-9741-480f-8f06-2ed77a2cbc6d",
   "metadata": {},
   "source": [
    "#### **1. Query, Key, Value 생성**\n",
    "- **Query (Q)**: 현재 위치에서 \"무엇을 찾고 있는지\"를 나타내는 벡터로, 다른 위치들과의 관련성을 질의하는 역할.\n",
    "- **Key (K)**: 각 위치가\"어떤 정보를 가지고 있는지\"를 나타내는 벡터로, **Query와 매칭되어 attention score를 계산하는 기준**이 된다.\n",
    "- **Value (V)**: 실제로 **\"전달할 정보의 내용\"**을 담고 있는 벡터로, attention weight와 가중합되어 최종 출력을 생성한다.  이 최종 출력값이 context vector(문맥 정보)가 된다.\n",
    "- Self attention은 Query, Key, Value 모두 입력 Sequence(X)로 부터 생성한다.\n",
    "    - 입력 sequence에 각각 query, key, value를 만드는 Weight들(Linear)를 내적하여 만든다.\n",
    "      - $Query=X\\cdot W_q$ \n",
    "      - $Key=X\\cdot W_k$ \n",
    "      - $Value=X\\cdot W_v$\n",
    "\n",
    "![img](figures/transformer_query_key_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aae5c0-949f-48a5-bed6-43166ec6c0c2",
   "metadata": {},
   "source": [
    "#### **2. Attention Score**\n",
    "- Query 와 Key를 내적(dot product)하여 유사도를 계산한다. embedding vector의 차원의 제곱근으로 나눠서 정규화한다.\n",
    "- Scaled Dot Product Attention\n",
    "  \n",
    "   $$\n",
    "   \\text{Attention Score} = \\cfrac{Q\\cdot K^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "![img](figures/transformer_query_key_matmulpng.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef626d8-437a-4f44-baf0-30dadf786a4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. **Attention Weight**\n",
    "   - 위에서 계산된 Attention score에 softmax를 적용해 0 ~ 1 사이 비율로 바꾼다.\n",
    "     \n",
    "   $$\n",
    "    \\text{Attention Weight} = softmax(\\text{Attention Score})\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451330a8-fada-4326-8072-e8e42e799fd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **4. Attention value**\n",
    "   - 최종 attention 연산의 결과로 **Attention weight를 Value에 내적**해서 Attention Value를 만든다.\n",
    "   - Attention에서 Attention value값이 **입력 sequence의 context vector**가 된다.\n",
    "   $$\n",
    "   \\text{Attention Value} = \\text{Attention Weight}\\cdot\\text{Value}\n",
    "   $$ \n",
    "![img](figures/transformer_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f5de2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fae8ca3-2797-42ce-929f-f525cfcc2ee1",
   "metadata": {},
   "source": [
    "## Multi-Head attention\n",
    "- \"Multi-Head Attention은 입력 데이터에 서로 다른 가중치 행렬을 적용하여 **여러 개의 Q, K, V 세트**를 생성하고, 각 세트마다 병렬적으로 attention을 수행한다.\"\n",
    "- 이렇게 나눈 것을 head라고 하고 여러개를 만들어 사용하므로 multi-head attention이라고 한다.\n",
    "- **Multi-Head 생성 과정**\n",
    "    - 입력 Embedding Vector가 512차원이고 Head를 8개 사용한다고 할 경우\n",
    "    - 각 Head는 입력 전체(512차원)에 대해 서로 다른 학습 가능한 가중치 행렬을 적용하여 64차원(512/8)의 독립적인 Query, Key, Value를 생성한다. (Single Head는 512차원 전체를 사용하여 512차원의 Query, Key, Value 생성)\n",
    "        - 효율적인 병렬 처리를 위해 일반적으로 `d_model % head수 = 0`이 되도록 설정한다. (필수 조건은 아니지만 구현상 편의를 위함)\n",
    "        - ex) head 개수: 8, embedding 차원: 512, head 차원: 64\n",
    "            - 512 x 64 (embedding_dim x head_dim)의 가중치 행렬(Linear)를 8 개 생성하고 입력 데이터와 행렬곱을 통해 64차원의 Query 8개를 생성한다. Key와 Value도 각각의 같은 방식으로 8개씩 head를 생성한다.\n",
    "    - 각 head는 독립적으로 Attention 연산을 수행한다. (**병렬 처리**)\n",
    "    - 모든 head의 출력을 concat한 후(8×64 = 512차원), 최종적으로 출력 차원을 원하는 차원으로 매핑하기 위해 추가적인 선형 변환을 수행한다.\n",
    "\n",
    "- **장점**\n",
    "    - 동일한 입력 시퀀스에 대해 여러 관점(문법적 관계, 의미적 관계 등)에서의 정보를 동시에 학습할 수 있다.\n",
    "    - 각 head가 서로 다른 특성을 학습하여 더 풍부한 표현이 가능하다.\n",
    "    - 여러 head의 연산이 병렬적으로 처리될 수 있어 GPU와 같은 하드웨어에서 연산 속도를 극대화할 수 있다.\n",
    "\n",
    "\n",
    "![img](figures/transformer_multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a6601-e563-467f-883b-c19526eaca9d",
   "metadata": {},
   "source": [
    "## Masked self Attention\n",
    "- 입력 sequence 가 순서대로 입력되어 처리 되는 경우 i번째 입력 단어의 경우 그 이후의 단어는 모르는 상태이다. \n",
    "- Attention은 입력된 모든 토큰을 한번에 처리해서 attention score를 구한다. 이것은 주어진 토큰(i번째)이 미래시점의 입력토큰(i+1 이후 번째)과의 유사도를 계산한 것이 된다. \n",
    "- 이 문제를 해결하기 위해 i번째 토큰에 대한 attention score는 i번째 까지의 토큰들과만 계산하도록 한다.\n",
    "- Attention Score를 Softmax 계산하기 전에 적용한다.\n",
    "- Attention 계산시 선택적으로 mask를 사용하면 Masked self attention이 된다. (Masked Multi-Head Attention)\n",
    "  \n",
    "![img](figures/transformer_masked_self_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d18bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([10, -torch.inf, -torch.inf])\n",
    "torch.nn.Softmax(dim=-1)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83bf67-5280-4173-8523-9618c2a29781",
   "metadata": {},
   "source": [
    "# Transfomer 모델의 구조적 분류와 대표모델\n",
    "\n",
    "- Transfomer 모델을 기반으로 구현된 다양한 모델들이 있다. \n",
    "- 트랜스포머 모델은 구조에 따라 크게 세 가지로 분류할 수 있었다.\n",
    "\n",
    "## 인코더 전용 모델 (Encoder-Only)\n",
    "- Transformer Encoder 구조를 이용해 구현된 모델들. \n",
    "- **대표 모델들**:\n",
    "    - BERT: 마스크드 언어 모델링과 다음 문장 예측을 목표로 하는 모델.\n",
    "    - RoBERTa: BERT를 개선하여 더 많은 데이터와 더 큰 배치로 성능을 향상시킨 모델.\n",
    "    - ALBERT: 파라미터 수를 줄이고 레이어 간 파라미터를 공유하도록 개선한 모델.\n",
    "\n",
    "## 디코더 전용 모델 (Decoder-Only)\n",
    "- Transformer Decoder 구조를 이용해 구현된 모델들. \n",
    "- **대표 모델들**:\n",
    "    - GPT 시리즈: 텍스트 생성에 특화된 모델로, GPT-1부터 GPT-3까지 발전했다\n",
    "    - CTRL: 제어 토큰을 추가하여 생성 문장의 스타일을 제어할 수 있는 모델\n",
    "    - OPT: GPT와 유사한 구조를 가진 오픈 소스 모델\n",
    "\n",
    "## 인코더-디코더 모델 (Encoder-Decoder)\n",
    "- Transformer의 Encoder-Decoder 구조를 이용해 구현된 모델들.\n",
    "- **대표 모델들**:\n",
    "    - T5: 모든 NLP 태스크를 텍스트-투-텍스트 형식으로 변환하여 처리하는 모델\n",
    "    - BART: BERT의 인코더와 GPT의 디코더를 결합한 모델\n",
    "    - M2M-100: 100개 언어 간 번역이 가능한 다국어 번역 모델\n",
    "    - BigBird: 긴 시퀀스를 처리할 수 있도록 개선된 모델\n",
    "\n",
    "![transformer_models.png](figures/transformer_models.png)\n",
    "\n",
    "<대표적인 transformers 기반 모델들>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
