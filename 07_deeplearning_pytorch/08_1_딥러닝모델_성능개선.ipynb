{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 딥러닝 모델 성능 개선 기법들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 최적화 \n",
    "- train data에서 최고의 성능을 얻으려고 모델 파라미터들을 조정하는 과정을 최적화라고 한다.\n",
    "- 모델을 학습하는 과정 하는 과정이 최적화 과정이다.\n",
    "\n",
    "## 모델의 성능 - 일반화, 과대적합, 과소적합\n",
    "\n",
    "- **일반화(Generalization)**\n",
    "    - 훈련된 모델이 **처음 보는 데이터**에서 대해 잘 추론할 수 있는 상태. \n",
    "    - 학습을 통해 일반화된 특징들을 잘 찾은 상태.\n",
    "- **과대적합(Overfitting)**\n",
    "    - 검증 결과 Train set에 대한 성능은 좋은데 Validation set에 대한 성능은 안좋은 상태로 모델이 **학습을 과하게(overfitting)한** 상태를 말한다.\n",
    "    - 학습이 과하게 되어 쓸데 없는 패턴을 모두 외워버려 오히려 처음 본(새로운) 데이터에 대한 예측 성능이 떨어진다.\n",
    "    - 보통 Train dataset의 크기에 비해 모델이 너무 **복잡한 경우** 발생한다.\n",
    "    - 보통 과적합이라고 하면 Overfitting을 말한다.\n",
    "- **과소적합(Underfitting)**\n",
    "    - 검증 결과 Train set과 Validation set 모두 성능이 안좋은 상태로 모델의 **학습이 덜(underfitting)된** 상태를 말한다.\n",
    "    - Train dataset의 크기에 비해 모델이 **너무 단순해서** 데이터에 대한 특징들을 다 찾지 못한 상태이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![over_underfitting](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/07_over_underfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 모델 복잡도에 따른 train/test set에 대한 성능 변화\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_epoch_complex.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 과소적합(Underfitting) 개선\n",
    "- 모델의 복잡도를 높인다.\n",
    "    - 모델 네트워크의 크기를 키운다. \n",
    "        - Layer나 Unit 개수를 늘린다.\n",
    "- 데이터셋 종류에 맞는 Feature 추출 Layer를 hidden layer에 사용한다. (Convolution layer, Recurrent layer 등) \n",
    "- Epoch (또는 Step) 수를 늘려 더 학습시킨다. \n",
    "    - Train/Validation의 성능이 계속 좋아지는 상태에서 끝난 경우 더 학습을 시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 과대적합(Overfitting) 개선\n",
    "- **더 많은 data를 수집**\n",
    "    - 데이터 추가 수집\n",
    "        - 일반적으로 데이터를 추가 수집하여 늘리는데는 시간과 돈이 많이 들기 때문에 힘들다.\n",
    "    - 다양한 Upsampling 기법을 이용해 데이터를 증식시킨다.\n",
    "- **모델의 복잡도를 낮춰 단순한 모델을 만든다.**\n",
    "    - 네트워크 모델의 크기를 작게 만든다.\n",
    "    - 모델의 학습을 규제하는 기법을 적용한다.\n",
    "- **Epoch(또는 step) 수를 줄인다.**\n",
    "    - Validation의 성능 지표가 가장 좋았던 Epoch 까지만 학습시킨다.\n",
    "- <b style='font-size:1.2em'>과대적합을 방지하기 위한 규제 방식은 모두 모델의 복잡도를 낮추는 방법들이다.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch과 과적합\n",
    "- 데이터셋을 여러번 반복 학습을 하게 되면 **초반에는** train, validation 성능이 모두 개선된다. 그러나 **학습이 계속 반복되면** train 의 성능은 계속 좋아지지만 어느시점 부터는 Overfitting이 발생하여 **validation 의 성능은 나빠진다.**\n",
    "- Epoch을 반복하는 것은 같은 데이터셋을 반복적으로 학습하는 것이기 때문에 적절한 Epoch수를 넘어가면 Overfitting이 발생한다.\n",
    "- Validation set의 성능이 나빠지기 전의 반복횟수를 모델의 최적의 Epoch으로 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DNN 모델  크기 변경\n",
    "\n",
    "- 모델의 layer나 unit 수가 많을 수록 복잡한 모델이 된다.\n",
    "- Overfitting일 경우 모델을 간단하게 만들고 Underfitting일 경우 모델을 크게 만든다.\n",
    "    - Layer의 수, 각 layer의 unit 수를 조정 \n",
    "- 큰 모델에서 시작하여 layer나 unit수를 줄여가며 validation loss의 감소 추세를 관찰한다. (또는 반대로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- #### 데이터에 비해 작은 모델의 특징\n",
    "    - Train/Validation 성능 개선의 속도가 느리다.\n",
    "        - 반복횟수가 충분하지 않으면 학습이 덜된 상태에서 중단될 수 있다.\n",
    "    - Underfitting이 발생할 가능성이 크다.\n",
    "    \n",
    "- #### 데이터에 비해 큰 모델의 특징\n",
    "    - Validation 이 성능이 학습 초반부터 나빠진다. \n",
    "    - Train에 대한 성능은 빠르게 개선된다.\n",
    "    - Overfitting 발생할 가능성이 크다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Dropout Layer 추가를 통한 Overfitting 규제\n",
    "\n",
    "Dropout layer는 딥러닝 모델에서 과적합(overfitting)을 방지하기 위한 정규화(regularization-규제) 기법 중 하나\n",
    "1. **기본 개념**\n",
    "   - 학습 과정 중 무작위로 일정 비율의 뉴런을 비활성화(dropout)시켜 네트워크가 특정 뉴런에 과도하게 의존하지 않도록 한다.\n",
    "2. **작동 방식**:\n",
    "    - **학습 시**\n",
    "      -  각 학습 반복(step)마다 지정된 확률(예: 0.5)로 뉴런이 무작위로 제외한다.\n",
    "      -  `model.train()` 모드로 적용한다.\n",
    "    - **추론/테스트 시**\n",
    "      -  모든 뉴런이 활성화한다. \n",
    "      -  `model.eval()` 모드로 적용한다.\n",
    "\n",
    "3. **효과**:\n",
    "   - **앙상블 효과**\n",
    "     - 매번 다른 네트워크 구조로 학습하는 효과를 내어 앙상블과 유사한 결과를 얻을 수 있다.\n",
    "     - 앙상블의 효과로 **노이즈가 있는 데이터에도 더 잘 대응한다.**\n",
    "   - **상호 의존성(co-adaptation) 감소**\n",
    "     - 뉴런들이 서로 의존적이지 않고 더 견고한 특성을 학습하도록 유도한다.\n",
    "     - 예를 들어 A노드는 코의 특징을 감지하고 B노드는 눈의 특성을 감지한다. co-adaptation 이 발생하면 A노드는 B노드가 눈을 감지 했을 때만 코를 감지하는 식으로 두 노드가 과도하게 서로 의존하게 된다. Dropout은 이런 상황을 완화시켜준다.\n",
    "    - **모델 복잡도 감소**\n",
    "        - Dropout은 효과적으로 모델의 복잡도를 줄여 과적합을 방지한다.\n",
    "        - 과적합이 적은 모델은 훈련 데이터의 노이즈까지 학습하지 않고 일반적인 모델을 만든다.\n",
    "4. **하이퍼파라미터**: \n",
    "   - Dropout 비율(p)\n",
    "     - 뉴런의 비활성화 비율\n",
    "     - 일반적으로 0.2~0.5 사이의 값을 사용한다.\n",
    "       - Fully connected Layer에는 0.5를 Convolution Layer에는 0.2나 0.3을 주로 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_dropout1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dropout](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_dropout2.gif)   \n",
    "출처: https://wandb.ai/authors/ayusht/reports/Implementing-Dropout-in-PyTorch-With-Example--VmlldzoxNTgwOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization (배치정규화)\n",
    "- 각 Layer 에서 출력된 값을 평균=0, 표준편차=1로 정규화 하여 **각 Layer의 입력분포를 균일하게 만들어 준다.**\n",
    "- Batch 별로 정규화를 진행한다.\n",
    "  - Fully Connected Layer: 각 feature별로 배치의 모든 샘플에 대해 계산\n",
    "  - Convolutional Layer: 각 채널별로 배치의 모든 샘플과 모든 위치(height × width)에 대해 계산\n",
    "- BatchNormalization은 보통 4차원의 이미지 데이터(N, C, H, W) 처리하는 CNN 모델에 사용된다.\n",
    "- 정규화 단위에 따라 **LayerNormalization** 방식도 있다.\n",
    "  - LayerNormalization은 데이터포인트(sample) 단위로 정규화 하며 자연어처리 딥러닝모델(Transformer, RNN)에서 사용된다.\n",
    "\n",
    "### 정규화의 필요성 - Internal Covariate Shift(내부 공변량 변화) 문제\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_internal_covariate_shift.png)\n",
    "- 내부 공변량 변화란 학습 과정에서 각 층을 통과할 때 마다 입력 데이터 분포가 달라지는 현상이다.\n",
    "- 입력 데이터의 분포가 정규분포를 따르더라도 레이어를 통과하면서 그 분포가 바뀌어 성능이 떨어지는 문제가 발생한다. \n",
    "- 각 레이어를 통과할때 마다 분포를 정규분포로 정규화하여 성능을 올린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_bn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\gamma$: scaling 파라미터, $\\beta$: shift 파라미터\n",
    "    - 항상 일정한 분포로 나오는 것을 방지하기 위해 $\\gamma$와  $\\beta$를 이용해 분포에 약간의 변화를 준다.\n",
    "    - $\\gamma$와  $\\beta$는 학습 과정에서 최적화 되는 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Normalization은 입력데이터와 파라미터의 가중합을 구한 결과에 적용한 뒤 그 결과를 Activation 함수에 전달한다.<br><br>\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_bn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 효과\n",
    "- 랜덤하게 생성되는 초기 가중치에 대한 영향력을 줄일 수있다.\n",
    "- 학습하는 동안 과대적합에 대한 규제의 효과를 준다.\n",
    "- Gradient Vanishing, Gradient exploding을 막아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizer의 Learning rate(학습율) 조정을 통한 성능향상\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/08_lr.png)\n",
    "- Optimizer의 Learning rate이 너무 크거나 너무 작으면 최적의 파라미터를 찾지 못할 수 있다. 그래서 Learning rate는 모델 성능과 밀접한 관계가 있는 아주 중요한 Hyper Parameter이다. \n",
    "- 학습 하는 동안 고정된 하나의 Learning rate를 사용할 수도 있으나 **학습이 반복되는 동안 학습률을 변경하여 성능을 향상시킬 수 있다.**\n",
    "    - 학습 도중 학습률을 어떻게 조정할 지 다양한 알고리즘들이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyper parameter tuning\n",
    "\n",
    "- Parameters\n",
    "    - 모델이 학습하여 데이터에 가장 적합한 값을 찾아내는 파라미터\n",
    "        - Weights\n",
    "        - Bias\n",
    "- Hyper parameters\n",
    "    - 모형의 구조를 결정하거나 optimization 방법을 결정하는 변수들로 개발자가 직접 설정하는 파라미터\n",
    "    - Optimizer의 종류\n",
    "    - learning rate($\\alpha$)\n",
    "    - Hidden layer의 수 \n",
    "    - Layer들의 unit(node)의 수 \n",
    "    - Activation function의 종류\n",
    "    - Epoch 수    \n",
    "    - Mini batch size \n",
    "    - Regularization\n",
    "    - dropout rate\n",
    "    \n",
    "- 다양한 조합의 hyper parameter를 시도해서 loss 함수가 빠르게 감소하는 hyper parameter를 찾아내는 시도가 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
