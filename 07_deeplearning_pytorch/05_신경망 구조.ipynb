{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DNN (Deep Neural Network)\n",
    "\n",
    "## 신경망 구성요소\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Train(학습) 프로세스\n",
    "![image-2.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_train_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 역할\n",
    "1. **Model/Network**\n",
    "    - 기존 데이터의 패턴을 학습하여 새로운 데이터를 추론하기 위한 알고리즘, 함수. 딥러닝(Neural Network) 모델은 Network (model) 이라고 한다.\n",
    "2. **Loss Function (손실 함수)**\n",
    "    - 모델 학습 과정에서 모델이 추론한 결과와 정답(Ground truth)간의 Loss(손실/오차)를 계산하는 함수\n",
    "3. **Optimizer(옵티마이저)**\n",
    "    - 모델 학습 과정에서 loss function이 계산한 Loss를 기반으로 Loss가 줄어들도록 모델의 파라미터들(weight)을 업데이트하여 모델의 성능을 최적화하는 함수."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 유닛/노드/뉴런 (Unit, Node, Neuron)\n",
    "- 함수에 입력된 데이터에서 추출된 데이터의 특성(특징)을 말한다.\n",
    "    - Hidden layer(은닉층)의 Unit/Node/Neron들은 추론에 도움이 되는 데이터의 주요 특성들이다.\n",
    "    - Output layer(출력층)의 Unit/Node/Neron들은 모델의 최종 추론 결과이다.\n",
    "- Node를 추출하는 함수는 Input data의 각 Feature들에 Weight(가중치)를 곱하고 bias(편향)을 더한 결과를 Activation 함수(활성함수)에 입력해 비선형성을 추가하여 최종 출력한다.\n",
    "    \n",
    "![04_neuron](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_unit.png)\n",
    "\n",
    "- **Input vector(입력값)**: $\\large\\mathbb x=(x_1, x_2, x_3)$\n",
    "- **Weights(가중치)**: $\\large\\mathbb w = (w_1, w_2, w_3)$\n",
    "- **Bias(편향)**: $\\large b \\in \\mathbb R$\n",
    "- **Activation function(활성함수)**: $\\large\\sigma(z)$\n",
    "    - 선형결합한 결과를 비선형화 시키는 목적\n",
    "    - 다양한 비선형 함수들을 사용한다.    \n",
    "    $\\LARGE z=w_1 x_1 + w_2 x_2 + w_3 x_3 + b \\Leftrightarrow z = \\mathbb w^T \\mathbb x + b$    \n",
    "    $\\LARGE a=\\sigma( z)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 레이어/층(Layer)\n",
    "- Node 추출 함수들을 모아놓은 함수.\n",
    "    - 한개의 노드는 입력 데이터에서 추출한 한개의 특성이다.\n",
    "    - 정확한 추론 결과를 위해서는 입력데이터에서 많은 특성들을 추출해야 한다. 그래서 **Node 추출하는 함수들을 묶어 하나의 입력으로 부터 여러개의 특성들을 추출하도록 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Input Layer(입력층)**\n",
    "    - 모델에 주입되는 입력데이터의 feature들을 말한다.     \n",
    "- **Output Layer(출력층)**\n",
    "    - 모델의 최종 예측결과를 Node들로 구성된 Layer\n",
    "- **Hidden Layer(은닉층)**\n",
    "    - Input Layer와 Output Layer사이에 존재하는 Layer.\n",
    "    - 정답을 추론하기 위해 추출된 특성값들(Feature vector).\n",
    "    - 이 특성값들은 한번에 찾지 않고 여러 단계에 걸쳐 찾는다. \n",
    "        - Feature Vector를 찾는 단계를 얼마로 할 것인지에 따라 연결 Layer 개수를 정한다.\n",
    "- pytorch등 딥러닝 framework들은 Layer들 처리하는 class들을 제공한다.\n",
    "- Layer들의 연결한 것을 **<font size=5>딥러닝 모델(Network)</font>** 이다.\n",
    "    - 딥러닝은 Layer들을 깊게 쌓은 것을 말한다.(여러 Layer들을 연결한 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 목적, 구현 방식에 따라 다양한 종류의 Layer들이 있다.\n",
    "    - **Fully Connected Layer (Dense layer)**\n",
    "        - 추론 단계에서 주로 사용\n",
    "    - **Convolution Layer**\n",
    "        - 이미지 Feature extraction으로 주로 사용\n",
    "    - **Recurrent Layer** \n",
    "        - Sequential(순차) 데이터의 Feature extraction으로 주로 사용\n",
    "    - **Embedding Layer**\n",
    "        - Text 데이터의 Feature extraction으로 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **API** \n",
    "    - https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 모델 (Network)\n",
    "- Layer를 연결한 것이 Deep learning 모델이다.\n",
    "- 각 Layer는 이전 레이어의 Node들 input으로 받아 처리 된 Node이다.\n",
    "- 적절한 network 구조(architecture)를 찾는 것은 **공학적(Engineering)이기 보다는 경험적(Art)접근**이 필요하다.\n",
    "\n",
    "![image-2.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_model_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 활성 함수 (Activation Function)\n",
    "- 각 선형처리 결과 unit(neuron)에 비선형성을 추가하는 함수.\n",
    "    - $Node = activation(X \\cdot W + b)$\n",
    "- Activation 함수는 Layer단위로 설정하여 그 layer에 속한 unit들에 동일한 함수를 적용한다.\n",
    "\n",
    "### 활성 함수 목적\n",
    "- **출력 레이어**의 경우 출력하려는 문제에 맞춰 활성함수를 결정한다.\n",
    "- **은닉층 (Hidden Layer)** 의 경우 **비선형성을** 준다. \n",
    "    - 공식적으로도 비선형함수를 사용하지 않으면 Layer들을 여러개 연결하는 의미가 없어진다.\n",
    "    - **ReLU** 함수를 주로 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 주요 활성함수(Activation Function)\n",
    "\n",
    "- ### Sigmoid (logistic function)\n",
    "    ![image-2.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_sigmoid.png)\n",
    "    \n",
    "    - $$\\large \\sigma(z) = \\frac 1 {1+e^{-z}}$$\n",
    "    - 출력값의 범위\n",
    "        - $0<sigmoid(z)<1$\n",
    "    - 한계\n",
    "        - 초기 딥러닝의 hidden layer(은닉층)의 activation function(활성함수)로 많이 사용 되었다.\n",
    "        - 층을 깊게 쌓을 경우 **기울기 소실(Gradient Vanishing)** 문제를 발생시켜 학습이 안되는 문제가 있다.\n",
    "    - **Binary classification(이진 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 사용된다.**\n",
    "        - 모델이 positive(1)의 확률을 출력결과로 추론하도록 할 경우 사용. (unit개수 1, activation함수 sigmoid)\n",
    "        - 위와 같은 한계때문에 hidden layer(은닉층)의 activation function(활성함수)로는 잘 사용되지 않는다.\n",
    "\n",
    "> **<span style=\"font-size:1.2em\">기울기 소실(Gradient Vanishing) 문제란</span>**\n",
    "> - 최적화 과정에서 gradient가 0이 되어서 Bottom Layer의 가중치들이 학습이 안되는 현상\n",
    ">     - Bottom Layer: Input Layer(입력층)에 가까이 있는 Layer들. \n",
    ">     - Top layer: Output Layer(출력층)에 가까이 있는 Layer들."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ### Hyperbolic tangent\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_tanh.png)\n",
    "\n",
    "- $$\\large  tanh(z) = \\cfrac{e^{z} - e^{-z}}{{e^{z} + e^{-z}}}$$\n",
    "- 출력값의 범위\n",
    "    - $-1<tanh(z)<1$\n",
    "   - sigmoid보다 학습할 때 기울기 소실 문제(gradient vanishing)를 완화시킬 수 있다.\n",
    "- RNN의 활성함수로 사용된다.\n",
    "- 출력값의 범위가 -1 ~ 1 사이인 경우 출력 Layer의 활성함수로 사용한다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ### ReLU(Rectified Linear Unit)\n",
    "![image-2.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_relu.png)\n",
    "    $$\\large  ReLU(z)=max(0,z)$$\n",
    "- 기울기 소실(Gradient Vanishing) 문제를 어느정도 해결\n",
    "- 0 이하의 값(z <= 0)들에 대해 뉴런이 죽는 단점이 있다. (Dying ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ### Leaky ReLU\n",
    "    ![image-4.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_leaky_relu.png)\n",
    "    <br><br>\n",
    "$$\n",
    "  \\begin{align}\n",
    "      \\large  \\text{LeakyReLU}(z)=max(\\alpha z,z) \\\\\n",
    "     0< \\alpha <1\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "- ReLU의 Dying ReLU 현상을 해결하기 위해 나온 함수\n",
    "- 음수 z를 0으로 반환하지 않고 alpah (0 ~ 1 사이 실수)를 곱해 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.65079365, 0.01587302])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = np.array([2.1, 4.1, 0.1])\n",
    "p = r / r.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1, 4.1, 0.1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r#.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11731043, 0.86681333, 0.01587624])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er = np.exp(r)\n",
    "p2 = er / er.sum()\n",
    "p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ### Softmax\n",
    "    \n",
    "  - $$\n",
    "        \\begin{align}\n",
    "            \\large  Softmax(z_j) = \\frac{exp(z_j)}{\\sum_{k=1}^K exp(z_k)}\\\\ \n",
    "            \\\\\n",
    "            \\:j=1,2, \\ldots, K\n",
    "        \\end{align}          \n",
    "$$\n",
    "  - **Multi-class classification(다중 분류)를 위한 네트워크 모델의 Output layer(출력층)의 활성함수로 사용된다.**\n",
    "      - 은닉층의 활성함수로 사용되지 않는다.\n",
    "  - Layer의 Node들을 정규화 하여 확률값으로 변환한다.\n",
    "      - 출력노드들의 값은 0 ~ 1사이의 실수로 변환되고 그 값들의 총합은 1이 된다.\n",
    "      - pytorch에서 **분류 모델의 출력값을 확률값으로 변환하지 않은 것을 logit** 이라고 한다.\n",
    "    \n",
    "\n",
    "\n",
    "- **API**\n",
    "    - https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_activation_fn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Universal_Approximation_Theorem\n",
    "> 비선형 Activation 함수가 추가된 hidden layer가 있으면 어떤 함수도 근사할 수 있다는 이론    \n",
    "> [Universal_Approximation_Theorem 노트북](05_Universal_Approximation_Theorem.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 손실함수(Loss function, 비용함수)\n",
    "- Model이 출력한 예측값(prediction) $\\hat y^{(i)}$와 정답값(output, ground truth) $y^{(i)}$ 사이의 차이를 즉 **오차를 계산**하는 함수\n",
    "- 모델을 훈련하는 동안 Loss 함수가 계산한 Loss값(손실)이 최소화 되도록 파라미터(weight와 bias)을 업데이트한다.\n",
    "    - Loss함수는 최적화 시작이 되는 값이다.\n",
    "- <span style=\"font-size:1.2em;color:red\">모델이 해결하려는 문제의 종류에 따라 표준적인 Loss function이 있다.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ## Classification (분류)\n",
    "    - cross entropy (log loss) 사용\n",
    "$$\n",
    "\\large -log(모델이\\,출력한\\,정답에\\,대한\\,확률)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ### Binary classification (이진 분류)\n",
    "    - 특정 클래스인지 아닌지를 추론하는 문제.\n",
    "        - 모델은 Positive(양성-1)의 확률을 출력한다.\n",
    "        - 모델 출력 값을 Threshold(보통 0.5)와 비교해 작으면 0, 크면 1로 후처리를 통해 Label을 계산한다.\n",
    "        - ex) 환자인지 여부, 스팸메일인지 여부\n",
    "    - Output Layer의 unit 개수를 1로 하고 activation 함수로 sigmoid를 사용하여 positive(1)의 확률( 0 ~ 1사이 값)로 예측 결과를 출력하도록 모델을 정의 한다.   \n",
    "    이때 Loss함수는  **binary_crossentropy**를 loss function으로 사용한다.\n",
    "        - **nn.BCELoss()** loss function을 사용    \n",
    "    \\begin{align}\n",
    "        &\\large Loss(\\hat y^{(i)} ,y^{(i)}) = - y^{(i)} log(\\hat y^{(i)} ) - (1- y^{(i)}) log (1-\\hat y^{(i)} ) \\\\\\\\\n",
    "        &y^{(i)}: \\text{실제 값(Ground Truth)}, \\hat y^{(i)}: \\text{모델이 예측한 확률(Positive일 확률)}        \n",
    "    \\end{align}\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ### Multi-class classification (다중 클래스 분류)\n",
    "    - 두 개 이상의 클래스를 분류 \n",
    "        - 추론 대상이 여러개이고 그중 하나를 정답으로 예측한다.\n",
    "        - 모델은 각 범주값에 대해 정답일 확률을 출력한다. (0일확률, 1일확률, 2일확률,....,N일확률)\n",
    "    - **categorical crossentropy**를 loss function으로 사용 \n",
    "        - **nn.CrossEntropyLoss()** loss function 사용\n",
    "    \n",
    "     \\begin{align}\n",
    "          &\\large Loss(\\hat y_c^{(i)} ,y^{(i)}) = - \\sum_{c=1}^C y_c^{(i)} log(\\hat y_c^{(i)} )\\\\ \\\\\n",
    "          &y^{(i)}: \\text{실제 값(Ground Truth)}, \\hat y_c^{(i)}: \\text{모델이 예측한 class별 예측확률}\n",
    "      \\end{align}\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **nn.CrossEntropyLoss() 계산 순서**\n",
    "    1. 정답을 one hot encoding 처리해서 OneHot Vector로 만든다.\n",
    "        - 정답은 Label Encoding 상태의 값을 사용한다.\n",
    "    2. 모델이 예측한 결과에 Softmax를 적용해 확률값으로 바꿔준다.\n",
    "        - **Softmax 적용 전 값**을 입력한다.\n",
    "    3. Categorical Crossentropy 공식을 이용해 오차를 계산한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ### Regression (회귀)\n",
    "    - 정답이 수치형(연속형)값인 경우의 추론\n",
    "        - 예) 주가 예측 , 집값 예측, 점수 예측\n",
    "    - **Mean squared error** 를 loss function으로 사용 \n",
    "        - **nn.MSELoss()**  함수 사용\n",
    "    \\begin{align}\n",
    "&\\large Loss(\\hat y^{(i)} ,y^{(i)}) = \\frac  1 2 (\\hat y^{(i)} - y^{(i)})^2 \\\\\\\\    \n",
    "&y^{(i)}: \\text{실제 값(Ground Truth)}, \\hat y^{(i)}: \\text{모델이 예측한 값}\n",
    " \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **API**\n",
    "    - https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T07:06:19.300768Z",
     "start_time": "2021-11-01T07:06:19.282735Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제별 출력레이어 Activation 함수, Loss 함수\n",
    "\n",
    "문제형태|출력 Activation함수|Loss 함수\n",
    " :- | :- | :-\n",
    "이진분류(Binary Classification)|sigmoid| binary crossentropy\n",
    "다중분류(Multi-class Classification)|softmax| categorical crossentropy \n",
    "회귀(Regression)|None|MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizer (최적화 방법)\n",
    "\n",
    "- 학습할 때 모델이 정답에 가까운 추론을 하도록 parameter(weight, bias)를 최적화 하는 알고리즘.\n",
    "    - Deep Learning은 경사하강법(Gradient Descent)와 오차 역전파(back propagation) 알고리즘을 기반으로 파라미터들을 최적화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Gradient Decent (경사하강법)\n",
    "- ### 최적화 \n",
    "    - 모델 네트워크가 출력한 결과와 실제값(Ground Truth)의 차이를 정의하는 함수를 **Loss function(손실함수, 비용함수)** 라고 한다.\n",
    "    - Training 시 Loss function이 출력하는 값을 줄이기 위해 파라미터(weight, bias)를 update 과정을 **최적화(Optimization)** 이라고 한다.\n",
    "- ### Gradient Decent(경사하강법)\n",
    "    - 최적화를 위해 파라미터들에 대한 Loss function의 Gradient값을 구해 Gradient의 반대 방향으로 일정크기 만큼 파라미터들을 업데이트 하는 것을 경사하강법이라고 한다.\n",
    "\\begin{align}       \n",
    "&\\large W_{new} = W-\\alpha\\frac{\\partial{Loss(W)}}{\\partial {W}}\\\\\n",
    "&W: \\text{파라미터}\\: \\alpha:\\;\\text{학습률}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 오차 역전파(Back Propagation) \n",
    "\n",
    "- 딥러닝 학습시 파라미터를 최적화 할 때 추론한 역방향으로 loss를 전달하여 단계적으로 파라미터들을 업데이트한다.\n",
    "    - Loss에서부터(뒤에서부터) 한계단씩 미분해 gradient 값을 구하고 이를 Chain rule(연쇄법칙)에 의해 곱해가면서 파라미터를 최적화한다.\n",
    "    - 출력에서 입력방향으로 계산하여 역전파(Back propagation)라고 한다.\n",
    "        - 추론의 경우 입력에서 출력 방향으로 계산하며 이것은 순전파(Forward propagation)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 계산 그래프 (Computational Graph)\n",
    "- 계산 과정을 자료구조의 하나인 그래프 형태로 표현한 것\n",
    "    - 복잡한 계산을 순서대로 나눠서 그래프로 표현한 것.\n",
    "- 그래프는 노드(Node)와 엣지(Edge)로 구성됨.\n",
    "    - **노드**: 연산을 정의\n",
    "    - **엣지**: 피연산자 값의 흐름을 표시\n",
    "- **Deep learning model은 계산그래프로 구성된다**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 계산 그래프의 예\n",
    "- 슈퍼에서 1개에 100원인 사과를 2개 샀을 때 지불할 금액은 어떻게 될까? 단 부가세는 10% 부과된다.\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_compute_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 계산 그래프 절차 및 특장점\n",
    "- 계산 그래프를 사용한 문제 풀이 절차\n",
    "    - 계산 그래프를 구성\n",
    "    - 계산 방향을 결정\n",
    "        - 계산 시작에서 계산 결과 방향으로 순서대로 계산: **순전파(Forward propagation)**\n",
    "        - 계산 결과에서 계산 시작 역방향으로 계산: **역전파(Back propagation)**\n",
    "- 특징/장점\n",
    "    - **국소적 계산**을 통해 결과를 얻는다.\n",
    "        - 각 노드의 계산은 자신과 관계된 정보(입력 값들)만 가지고 계산한 뒤 그 결과를 다음으로 출력한다.\n",
    "    - 복잡한 계산을 단계적으로 나눠 처리하므로 문제를 단순하게 만들어 계산할 수 있다.\n",
    "        - **딥러닝에서 역전파를 이용해 각 가중치 업데이트를 위한 미분(기울기) 계산을 효율적으로 할 수 있게 된다.**\n",
    "    - 중간 계산결과를 보관할 수 있다.\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 합성함수(Composition function)의 미분\n",
    "- **<font size='5'>합성함수 : 함수의 처리결과를 다른 함수의 입력으로 넣어 처리하는 함수</font>**\n",
    "- $t\\circ z$\n",
    "  \n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_composition_function.png)    \n",
    "집합 X를 두번의 함수(f, g)를 거쳐 Z로 보내는 함수가 합성함수\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    & \\large z = (x+y)^2\\\\ \n",
    "    & \\large z = t^2\\\\ \n",
    "    & \\large t=x+y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **<font size='5'>연쇄 법칙(Chain Rule)</font>**\n",
    "    - 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
    "\\begin{align}\n",
    "    & \\large\\cfrac{\\partial z}{\\partial x} = \\cfrac{\\partial z}{\\partial t} \\cfrac{\\partial t}{\\partial x} \\\\\n",
    "    & \\cfrac{\\partial z}{\\partial t} = 2t,\\;\\cfrac{\\partial t}{\\partial x} = 1 \\\\\n",
    "    & \\large\\cfrac{\\partial z}{\\partial x}=2t \\times 1 = 2(x+y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 연쇄 법칙과 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![computation graph](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_computation_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 딥러닝 네트워크에서 최적화 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/backpropagation1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "& \\textbf{z11} = x1 \\cdot w11 + x2 \\cdot w12 + x3 \\cdot w13 \\\\\n",
    "& \\textbf{a11} = \\sigma(z11) = \\cfrac{1}{1+e^{-z11}} \\\\\n",
    "& \\textbf{z2} = a11 \\cdot w21 + a12 \\cdot w22\\\\\n",
    "& \\textbf{a2} = z2 \\\\\n",
    "& \\textbf{L} = (y - a2)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ### $W_{11}$을 업데이트 하기 위한 미분값은?\n",
    "$$\n",
    "\\large\\frac{\\partial}{\\partial {W_{11}}}{Loss} = ????\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/backpropagation2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "& \\cfrac{\\partial L}{\\partial a2} = -2(y - a2) \\\\\n",
    "& \\cfrac{\\partial a2}{\\partial z2} = 1 \\\\\n",
    "& \\cfrac{\\partial z2}{\\partial a11} = w21 \\\\\n",
    "& \\cfrac{\\partial a11}{\\partial z11} = \\sigma(z11) \\cdot (1-\\sigma(z11)) \\\\\n",
    "& \\cfrac{\\partial z11}{\\partial w11} = x1\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    " \\huge{\\cfrac{\\partial L}{\\partial w11} = \\cfrac{\\partial L}{\\partial a2} \\cdot \\cfrac{\\partial a2}{\\partial z2} \\cdot \\cfrac{\\partial z2}{\\partial a11} \\cdot \\cfrac{\\partial a11}{\\partial z11} \\cdot \\cfrac{\\partial z11}{\\partial w11} }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **순전파(forward propagation): 추론**\n",
    "- **역전파(back propagation): 학습시 파라미터(weight) 업데이트**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 파라미터 업데이트 단위\n",
    "\n",
    "- **Batch Gradient Decent (배치 경사하강법)**\n",
    "    - Loss를 계산할 때 전체 학습데이터를 사용해 그 평균값을 기반으로 파라미터를 최적화한다.\n",
    "    - 많은 계산량이 필요해서 속도가 느리다. 학습 데이터가 클 경우 메모리가 부족할 수 있다.\n",
    "\n",
    "- **Mini Batch Stochastic Gradient Decent (미니배치 확률적 경사하강법)**\n",
    "    - Loss를 계산할 때 전체 데이터를 다 사용하지 않고 지정한 데이터 양(batch size) 만큼 마다 계산해 파라미터를 업데이트 한다.\n",
    "    - 계산은 빠른 장점이 있지만 최적값을 찾아 가는 방향이 불안정 하여 부정확 하다. 그러나 반복 횟수를 늘리면 Batch 방식과 유사한 결과로 수렴한다.\n",
    "\n",
    "> **스텝(Step)**:  한번 파라미터를 업데이트하는 단위\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGD를 기반으로 한 주요 옵티마이저\n",
    "- 방향성을 개선한 최적화 방법\n",
    "    - **Momentum**\n",
    "        - 학습률을 계산할 때 이전 step 까지의 gradient의 (누적)합을 더해서 계산한다.\n",
    "- 학습률을 개선한 최적화 방법\n",
    "    - **Adagrad**\n",
    "        - 파라미터 별로 다른 학습률을 계산한다. 각 파라미터의 학습률은 파라미터가 업데이트될 수록 반비례하여 작아지도록 계산한다.\n",
    "            - 이전 기울기의 제곱들을 누적한 값의 제곱근의 역수를 학습율에 곱한다.\n",
    "    - **RMSProp**\n",
    "        - Adagrad는 학습이 진행될 수록 이전 기울기의 제곱들을 누적한 값이 커지므로 학습률이 점점 0에 수렴하는 문제가 있다.(학습율에 누적값의 역수를 곱하므로). RMSProp은 기울기를 단순 누적하지 않고 지수 가중 이동 평균 사용하여 최신 기울기들이 더 크게 반영하여 0에 수렴하는 문제를 해결함.\n",
    "- 방향성 + 학습률 개선 최적화 방법\n",
    "    - **Adam**\n",
    "        - Momentum와 RMSProp을 합친 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image.png](https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/05_optimizer.png)\n",
    "\n",
    "[출처] https://www.slideshare.net/yongho/ss-79607172\n",
    "\n",
    "- **API**\n",
    "    - https://pytorch.org/docs/stable/optim.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
