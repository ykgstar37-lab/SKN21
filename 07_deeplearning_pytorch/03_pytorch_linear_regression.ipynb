{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LinearRegression from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현할 것\n",
    "- 공부시간과 성적간의 관계를 모델링한다.\n",
    "    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.\n",
    "    - 이 예제에서는 공부한 시험시간으로 점수를 예측하는 모델을 정의한다.\n",
    "    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.\n",
    "\n",
    "# 데이터 확인\n",
    "- 입력데이터: 공부시간\n",
    "- 출력데이터: 성적\n",
    "\n",
    "|공부시간|점수|\n",
    "|-|-|\n",
    "|1|20|\n",
    "|2|40|\n",
    "|3|60|\n",
    "\n",
    "우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   \n",
    "수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(훈련) 데이터셋 만들기\n",
    "- 모델을 학습시키기 위한 데이터셋을 구성한다.\n",
    "- 입력데이터와 출력데이터을 각각 다른 행렬로 구성한다.\n",
    "- 하나의 데이터 포인트의 입력/출력 값은 같은 index에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 (Linear Regression)\n",
    "- Feature들의 가중합을 이용해 Target을 추정한다.\n",
    "- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.\n",
    "    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.\n",
    "    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 준다.\n",
    "- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.\n",
    "      \n",
    "\n",
    "\\begin{align}\n",
    "&\\large \\hat{y} = W\\cdot X + b\\\\\n",
    "&\\small \\hat{y}: \\text{모델추정값}\\\\\n",
    "&\\small W: \\text{가중치}\\\\\n",
    "&\\small X: \\text{Feature(입력값)}\\\\\n",
    "&\\small b: \\text{bias(편향)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset 구성\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `공부시간` 1개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다.   \n",
    "  이 문제에서 예측할 항목은 `시험점수` 한개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.tensor([[1], [2], [3]], dtype=torch.float32)    # 공부시간\n",
    "y_train = torch.tensor([[20], [40], [60]], dtype=torch.float32) # 시험점수)\n",
    "X_train.size(), y_train.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 (weight, bias) 정의\n",
    "- 학습대상/최적화 대상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) torch.Size([1])\n",
      "tensor([[0.5099]], requires_grad=True)\n",
      "tensor([-0.2757], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "weight = torch.randn(1, 1, requires_grad=True)  # feature에 곱할 값. (가중치) \n",
    "                            # (1: 입력 feature개수, 1: 출력 값(예측)의 개수)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "print(weight.size(), bias.size())\n",
    "print(weight)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2342],\n",
       "        [0.7441],\n",
       "        [1.2540]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론\n",
    "pred = X_train @ weight + bias\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.],\n",
       "        [40.],\n",
       "        [60.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1794.2670], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차계산\n",
    "loss = torch.mean((y_train - pred)**2, dim=0)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient 계산 -> loss에 대한 weight, bias의 순간변화율(gradient) 를 계산.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-183.0103]]), tensor([-78.5118]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad, bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.9940]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.1\n",
    "weight.data = weight.data - weight.grad * lr\n",
    "weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.7325])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.data = bias.data - bias.grad * lr\n",
    "bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "weight = torch.randn(1, 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# 추론 모델\n",
    "def linear_model(X):\n",
    "    return X @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss(오차) 계산 함수\n",
    "def mse_loss_fn(pred, y):\n",
    "    return torch.mean((pred-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "1. 모델을 이용해 추정한다.\n",
    "   - pred = model(input)\n",
    "1. loss를 계산한다.\n",
    "   - loss = loss_fn(pred, target)\n",
    "1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.\n",
    "   - loss.backward()\n",
    "1. optimizer를 이용해 파라미터를 update한다.\n",
    "   - optimizer.step()  \n",
    "1. 파라미터의 gradient(미분값)을 0으로 초기화한다.\n",
    "   - optimizer.zero_grad()\n",
    "- 위의 단계를 반복한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2000] - loss: 1687.3866\n",
      "[100/2000] - loss: 4.4887\n",
      "[200/2000] - loss: 2.7737\n",
      "[300/2000] - loss: 1.7140\n",
      "[400/2000] - loss: 1.0591\n",
      "[500/2000] - loss: 0.6545\n",
      "[600/2000] - loss: 0.4044\n",
      "[700/2000] - loss: 0.2499\n",
      "[800/2000] - loss: 0.1544\n",
      "[900/2000] - loss: 0.0954\n",
      "[1000/2000] - loss: 0.0590\n",
      "[1100/2000] - loss: 0.0364\n",
      "[1200/2000] - loss: 0.0225\n",
      "[1300/2000] - loss: 0.0139\n",
      "[1400/2000] - loss: 0.0086\n",
      "[1500/2000] - loss: 0.0053\n",
      "[1600/2000] - loss: 0.0033\n",
      "[1700/2000] - loss: 0.0020\n",
      "[1800/2000] - loss: 0.0013\n",
      "[1900/2000] - loss: 0.0008\n",
      "[1999/2000] - loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000  # train set을 몇번 학습할지 (몇번 학습을 반복할지.). \n",
    "lr = 0.01\n",
    "for epoch in range(epochs):\n",
    "    # 1. 추론(예측)\n",
    "    pred = linear_model(X_train)\n",
    "    # 2. 오차 계산\n",
    "    loss = mse_loss_fn(pred, y_train)\n",
    "    # 3. 파라미터들(weight, bias)에 대한 gradient 계산\n",
    "    loss.backward()\n",
    "    # 4. 파라미터 업데이트\n",
    "    weight.data = weight.data - lr * weight.grad\n",
    "    bias.data = bias.data - lr * bias.grad\n",
    "    # 5. gradient 초기화\n",
    "    weight.grad = None\n",
    "    bias.grad = None\n",
    "\n",
    "    # 마지막 epoch, 100 epoch 마다 loss를 출력(성능 개선 상황을 모니터링)\n",
    "    if epoch % 100 == 0 or epoch == (epochs-1):\n",
    "        print(f\"[{epoch+1}/{epochs}] - loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[19.9746]]), tensor([0.0578]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data, bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 99.9307],\n",
      "        [139.8799]])\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "with torch.no_grad():\n",
    "    new_X = torch.tensor([[5], [7]], dtype=torch.float32)\n",
    "    pred = linear_model(new_X)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력, 다중 출력\n",
    "- 다중입력: Feature가 여러개인 경우\n",
    "- 다중출력: Output 결과가 여러개인 경우\n",
    "\n",
    "다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  \n",
    "[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)\n",
    "\n",
    "\n",
    "|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|\n",
    "|-|-|-|-:|-:|\n",
    "|73|67|43|56|70|\n",
    "|91|88|64|81|101|\n",
    "|87|134|58|119|133|\n",
    "|102|43|37|22|37|\n",
    "|69|96|70|103|119|\n",
    "\n",
    "```\n",
    "사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1\n",
    "오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2\n",
    "```\n",
    "\n",
    "- `온도`, `강수량`, `습도` 값이 **사과**와, **오렌지 수확량**에 어느정도 영향을 주는지 가중치를 찾는다.\n",
    "    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.\n",
    "    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.\n",
    "        - `개별 과일를 예측하기 위한 weight들 @ feature들` 의 계산 결과를  **Node, Unit, Neuron** 이라고 한다.\n",
    "        - 두 과일에 대한 Unit들을 묶어서 **Layer** 라고 한다.\n",
    "- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias 들**을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `온도, 강수량, 습도` 세개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다. 이 문제에서 예측할 항목은 `사과수확량, 오렌지 수확량` 2개의 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  input: 생산환경 (temp, rainfall, humidity) : (5, 3)\n",
    "environs = [\n",
    "    [73, 67, 43], \n",
    "    [91, 88, 64], \n",
    "    [87, 134, 58], \n",
    "    [102, 43, 37], \n",
    "    [69, 96, 70]\n",
    "]\n",
    "\n",
    "# Targets: 생산량 - (apples, oranges) - (5, 2)\n",
    "apple_orange_output = [\n",
    "    [56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 2]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Dataset을 torch.Tensor로 생성\n",
    "X = torch.tensor(environs, dtype=torch.float32)\n",
    "y = torch.tensor(apple_orange_output, dtype=torch.float32)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight와 bias\n",
    "- weight: 각 feature들이 생산량에 영향을 주었는지의 가중치로 feature에 곱해줄 값.\n",
    "    - 사과, 오렌지의 생산량을 구해야 하므로 가중치가 두개가 된다.\n",
    "    - weight의 shape: `(3, 2)`\n",
    "- bias는 모든 feature들이 0일때 생산량이 얼마일지를 나타내는 값으로 feature와 weight간의 가중합 결과에 더해줄 값이다.\n",
    "    - 사과, 오렌지의 생산량을 구하므로 bias가 두개가 된다.\n",
    "    - bias의 shape: `(2, )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model\n",
    "모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 함수.\n",
    "\n",
    "$$\n",
    "\\hspace{2.5cm} X \\hspace{1.1cm} \\cdot \\hspace{1.2cm} W \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "73 & 67 & 43 \\\\\n",
    "91 & 88 & 64 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "69 & 96 & 70\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\cdot\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$w_{11},\\,w_{12},\\,w_{13}$: 사과 생산량 계산시 각 feature들(생산환경)에 곱할 가중치   <br>\n",
    "$w_{21},\\,w_{22},\\,w_{23}$: 오렌지 생산량 계산시 각 feature들(생산환경)에 곱할 가중치    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/kgmyhGit/image_resource/main/deeplearning/figures/3_unit_layer.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight/bias 를 정의 -> 초기값은 random 값을 이용해서 생성.\n",
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "weight.size(), bias.size()\n",
    "# weight: (3:input feature개수  ,  2:output 개수)\n",
    "# bias  : (2:output 개수, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3439, -0.0658],\n",
       "        [-0.2001,  1.8045],\n",
       "        [-0.1878, -0.5427]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5498, -0.1455], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번 학습(최적화)\n",
    "## 추론\n",
    "pred = X @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-46.0337,  92.6196],\n",
       "        [-60.3684, 117.9341],\n",
       "        [-67.0691, 204.4590],\n",
       "        [-50.0789,  50.6610],\n",
       "        [-55.5295, 130.5607]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred.size())\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10156.9668, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loss 계산(MSE)\n",
    "loss = torch.mean((pred - y)**2) # 전체 추론한 결과의 평균오차를 계산.\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss를 가지고 파라미터들(weight들, bias들)의 gradient 계산.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3439, -0.0658],\n",
       "        [-0.2001,  1.8045],\n",
       "        [-0.1878, -0.5427]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10958.3174,   2320.0571],\n",
       "        [-12505.6328,   2855.6956],\n",
       "        [ -7598.2046,   1503.1517]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5498, -0.1455]), tensor([-132.0159,   27.2469]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.data, bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 업데이트\n",
    "lr = 0.00001\n",
    "weight.data = weight.data - lr * weight.grad\n",
    "bias.data = bias.data - lr * bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2343, -0.0890],\n",
       "        [-0.0750,  1.7759],\n",
       "        [-0.1118, -0.5577]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5511, -0.1458])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 업데이트된 파라미터로 추정 -> loss 계산\n",
    "pred2 = X @ weight + bias\n",
    "loss2 = torch.mean((pred2 - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10156.966796875 6969.875\n"
     ]
    }
   ],
   "source": [
    "print(loss.item(), loss2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "## 모델 정의\n",
    "def model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "## loss 함수(MSE)\n",
    "def loss_fn(pred, y):\n",
    "    return torch.mean((pred - y)**2) # 전체 오차의 평균."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/5000] - 33740.33594\n",
      "[0101/5000] - 123.50134\n",
      "[0201/5000] - 45.45717\n",
      "[0301/5000] - 21.78205\n",
      "[0401/5000] - 13.66584\n",
      "[0501/5000] - 10.16646\n",
      "[0601/5000] - 8.17079\n",
      "[0701/5000] - 6.76981\n",
      "[0801/5000] - 5.67896\n",
      "[0901/5000] - 4.79402\n",
      "[1001/5000] - 4.06549\n",
      "[1101/5000] - 3.46264\n",
      "[1201/5000] - 2.96295\n",
      "[1301/5000] - 2.54852\n",
      "[1401/5000] - 2.20474\n",
      "[1501/5000] - 1.91953\n",
      "[1601/5000] - 1.68291\n",
      "[1701/5000] - 1.48662\n",
      "[1801/5000] - 1.32377\n",
      "[1901/5000] - 1.18866\n",
      "[2001/5000] - 1.07657\n",
      "[2101/5000] - 0.98358\n",
      "[2201/5000] - 0.90643\n",
      "[2301/5000] - 0.84242\n",
      "[2401/5000] - 0.78932\n",
      "[2501/5000] - 0.74527\n",
      "[2601/5000] - 0.70872\n",
      "[2701/5000] - 0.67840\n",
      "[2801/5000] - 0.65324\n",
      "[2901/5000] - 0.63237\n",
      "[3001/5000] - 0.61506\n",
      "[3101/5000] - 0.60069\n",
      "[3201/5000] - 0.58878\n",
      "[3301/5000] - 0.57889\n",
      "[3401/5000] - 0.57069\n",
      "[3501/5000] - 0.56388\n",
      "[3601/5000] - 0.55824\n",
      "[3701/5000] - 0.55355\n",
      "[3801/5000] - 0.54967\n",
      "[3901/5000] - 0.54644\n",
      "[4001/5000] - 0.54377\n",
      "[4101/5000] - 0.54155\n",
      "[4201/5000] - 0.53970\n",
      "[4301/5000] - 0.53818\n",
      "[4401/5000] - 0.53691\n",
      "[4501/5000] - 0.53586\n",
      "[4601/5000] - 0.53499\n",
      "[4701/5000] - 0.53426\n",
      "[4801/5000] - 0.53366\n",
      "[4901/5000] - 0.53316\n",
      "[5000/5000] - 0.53276\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 0.00001  # 1e-5\n",
    "for epoch in range(epochs):\n",
    "    # 1. 추론\n",
    "    pred = model(X)\n",
    "    \n",
    "    # 2. loss 계산\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # 3. 파라미터들의 gradient 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. 파라미터 업데이트\n",
    "    weight.data = weight.data - lr * weight.grad\n",
    "    bias.data = bias.data - lr * bias.grad\n",
    "\n",
    "    # 5. gradient 초기화\n",
    "    weight.grad = None\n",
    "    bias.grad = None\n",
    "\n",
    "    ## 100 epoch, 마지막 epoch에서 loss를 출력 => 학습 과정 log를 출력\n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f\"[{epoch+1:04d}/{epochs}] - {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.4360,  70.1218],\n",
       "        [ 82.0100, 100.7566],\n",
       "        [118.7106, 133.0594],\n",
       "        [ 21.0649,  37.0589],\n",
       "        [101.9024, 119.0374]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터로 추론\n",
    "p = model(X)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x = torch.tensor([68, 82, 56], dtype=torch.float32)\n",
    "new_x = new_x.unsqueeze(dim=0)\n",
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[80.9923, 95.3980]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p = model(new_x)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch built-in 모델을 사용해 Linear Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[73, 67, 43], \n",
    "     [91, 88, 64], \n",
    "     [87, 134, 58], \n",
    "     [102, 43, 37], \n",
    "     [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(\n",
    "    [[56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Linear\n",
    "Pytorch는 torch.nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  \n",
    "torch.nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다.\n",
    "- `torch.nn.Linear(input feature의 개수 , output 값의 개수)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer와 Loss 함수 정의\n",
    "- **Optimizer**: 계산된 gradient값을 이용해 파라미터들을 업데이트 하는 함수\n",
    "- **Loss 함수**: 정답과 모델이 예측한 값사이의 차이(오차)를 계산하는 함수.\n",
    "  - 모델을 최적화하는 것은 이 함수의 값을 최소화하는 것을 말한다. \n",
    "- `torch.optim` 모듈에 다양한 Optimizer 클래스가 구현되있다.\n",
    "- `torch.nn` 또는 `torch.nn.functional` 모듈에 다양한 Loss 함수가 제공된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀 모델을 정의. torch.nn.Linear 클래스\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(3, 2)  # 3: input feature 개수, 2: output 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1032, -0.4685,  0.5696],\n",
       "        [-0.0471,  0.5237, -0.1146]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4290, 0.5182], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4911, -0.1199, -0.3582],\n",
      "        [-0.1004,  0.0509, -0.3623]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1019,  0.0784], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for a in model.parameters():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 함수\n",
    "loss_fn = torch.nn.MSELoss()  # 클래스\n",
    "# loss_fn = torch.nn.functional.mse_loss # 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer (torch.optim 모듈에 정의): weight.data = weight.data - lr * weight.grad\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), # 최적화 대상 파라미터들을 model에서 조회해서 전달.\n",
    "    lr = 0.00001,       # Learning Rage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.4911, -0.1199, -0.3582],\n",
       "         [-0.1004,  0.0509, -0.3623]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1019,  0.0784], requires_grad=True)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 2)\n",
    "loss_fn = torch.nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test:\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        print(x, y)\n",
    "        return x + y\n",
    "    \n",
    "t = Test()\n",
    "t(10, 20) # __call__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/5000] - 10458.666015625\n",
      "[0101/5000] - 274.3469543457031\n",
      "[0201/5000] - 109.3341293334961\n",
      "[0301/5000] - 57.58159255981445\n",
      "[0401/5000] - 38.539100646972656\n",
      "[0501/5000] - 29.445507049560547\n",
      "[0601/5000] - 23.782577514648438\n",
      "[0701/5000] - 19.612375259399414\n",
      "[0801/5000] - 16.300819396972656\n",
      "[0901/5000] - 13.595011711120605\n",
      "[1001/5000] - 11.361883163452148\n",
      "[1101/5000] - 9.512500762939453\n",
      "[1201/5000] - 7.979090213775635\n",
      "[1301/5000] - 6.70725154876709\n",
      "[1401/5000] - 5.652121067047119\n",
      "[1501/5000] - 4.776791095733643\n",
      "[1601/5000] - 4.050631046295166\n",
      "[1701/5000] - 3.4481582641601562\n",
      "[1801/5000] - 2.9483489990234375\n",
      "[1901/5000] - 2.533675193786621\n",
      "[2001/5000] - 2.1896605491638184\n",
      "[2101/5000] - 1.9042599201202393\n",
      "[2201/5000] - 1.667473554611206\n",
      "[2301/5000] - 1.471048355102539\n",
      "[2401/5000] - 1.308068037033081\n",
      "[2501/5000] - 1.1728603839874268\n",
      "[2601/5000] - 1.0606943368911743\n",
      "[2701/5000] - 0.9676464200019836\n",
      "[2801/5000] - 0.8904365301132202\n",
      "[2901/5000] - 0.8263825178146362\n",
      "[3001/5000] - 0.7732499837875366\n",
      "[3101/5000] - 0.7291625738143921\n",
      "[3201/5000] - 0.6925899982452393\n",
      "[3301/5000] - 0.6622457504272461\n",
      "[3401/5000] - 0.6370759010314941\n",
      "[3501/5000] - 0.6161928176879883\n",
      "[3601/5000] - 0.5988658666610718\n",
      "[3701/5000] - 0.5844855308532715\n",
      "[3801/5000] - 0.5725630521774292\n",
      "[3901/5000] - 0.5626700520515442\n",
      "[4001/5000] - 0.5544630885124207\n",
      "[4101/5000] - 0.547648549079895\n",
      "[4201/5000] - 0.5420025587081909\n",
      "[4301/5000] - 0.5373163819313049\n",
      "[4401/5000] - 0.5334265232086182\n",
      "[4501/5000] - 0.5301986932754517\n",
      "[4601/5000] - 0.5275241136550903\n",
      "[4701/5000] - 0.5253020524978638\n",
      "[4801/5000] - 0.5234593749046326\n",
      "[4901/5000] - 0.5219299793243408\n",
      "[5000/5000] - 0.5206743478775024\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 추론\n",
    "    pred = lr(inputs)  \n",
    "\n",
    "    # loss 계산\n",
    "    loss = loss_fn(pred, targets) # torch.nn.functional.mse_loss(pred, targets) # (모델추정값, 정답)\n",
    "    \n",
    "    # gradient 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 파라미터 업데이트: optimizer.step()\n",
    "    optimizer.step()   # w = w - lr*g \n",
    "\n",
    "    # 파라미터 초기화 w.grad=None, b.grad=None\n",
    "    optimizer.zero_grad()\n",
    "    # 현재 epoch 학습 결과를 log로 출력\n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f\"[{epoch+1:04d}/{epochs}] - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 => gradient 계산을 할 필요가 없다. ==> grad_fn을 만들 필요가 없다. 그래서 torch.no_grad() 블록에서 추론 작업을 실행한다.\n",
    "with torch.no_grad():\n",
    "    pred = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.1539,  70.3301],\n",
       "        [ 82.1670, 100.6019],\n",
       "        [118.8016, 133.0800],\n",
       "        [ 21.1133,  37.0482],\n",
       "        [101.8120, 119.0164]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 로직을 함수 구현\n",
    "def train(inputs, targets, epochs, model, loss_fn, optimizer):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 추론\n",
    "        pred = model(inputs)\n",
    "        # loss 계산\n",
    "        loss = loss_fn(pred, targets) # torch.nn.functional.mse_loss(pred, targets) # (모델추정값, 정답)\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "        # 파라미터 업데이트: optimizer.step()\n",
    "        optimizer.step()\n",
    "        # 파라미터 초기화 w.grad=None, b.grad=None\n",
    "        optimizer.zero_grad()\n",
    "        # 현재 epoch 학습 결과를 log로 출력\n",
    "        if epoch % 100 == 0 or epoch == epochs-1:\n",
    "            print(f\"[{epoch+1:04d}/{epochs}] - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train(inputs, targets, 5000, model, nn.functional.mse_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
