{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4e21b4",
   "metadata": {},
   "source": [
    "# Pretrained Model\n",
    "\n",
    "-   미리 학습된 모델.\n",
    "-   **Pretrained model을 이용해 현재 문제를 해결한다.**\n",
    "    -   새로 모델을 정의 하고 학습시키는 것 보다 Pretrained 모델을 사용해 모델을 정의하면 훨씬 좋은 성능의 모델을 만들 수 있다.\n",
    "-   **Pretrain model을 사용하는 방식**\n",
    "    -   제로샷 전이학습(Zero shot transfer learning)\n",
    "        -   추가 학습 없이, Pretrained 모델을 그대로 사용한다.\n",
    "    -   전이학습 (Transfer learning)\n",
    "        -   Feature 추출기는 그대로 사용하고 추론기를 새로 만들어 학습시킨다.\n",
    "    -   미세조정 (Fine tuning)\n",
    "        -   Pretrained 모델의 모든 파라미터를 Custom Dataset으로 재학습시킨다.\n",
    "        -   모델을 해당 데이터에 최적화시킨다.\n",
    "\n",
    "> - **Custom Dataset**(맞춤 데이터셋)\n",
    ">     - Pretrained 모델 학습 데이터와 구별되는 새로운 학습 데이터.\n",
    ">     - 특정 문제 해결을 위한 맞춤형 데이터셋."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026b24d",
   "metadata": {},
   "source": [
    "# Transfer learning (전이학습)\n",
    "![transfer_learning](figures/transfer_learning_1.png)\n",
    "\n",
    "-   사전에 학습된 딥러닝모델을 재사용해서 새로운 모델(우리가 만드는 모델)의 시작점으로 삼고 해결하려는 문제를 위해 다시 학습시킨다.\n",
    "-   전이 학습을 통해 다음을 해결할 수 있다.\n",
    "    1. **데이터 부족문제**\n",
    "        - 딥러닝은 대용량의 학습데이터가 필요하다.\n",
    "        - 충분한 데이터를 수집하는 것은 항상 어렵다.\n",
    "    2. **과다한 계산량**\n",
    "        - 신경망 학습에는 엄청난 양의 계산 자원이 필요하다.\n",
    "-   보통 **Pretrained Feature Extractor 를 재사용하고** 거기에 목적에 맞는 추론기를 추가하여 모델을 만든다. \n",
    "- Train시 pretrained feature extractor의 파라미터들은 학습이 되지 않도록 하고 새로 추가한 추론기만 학습이 되도록 한다.\n",
    "\n",
    "![transfer_learning](figures/transfer_learning_2.png)\n",
    "\n",
    "> ## Backbone, Head\n",
    "> -  Backbone network: 전체 모델 구조에서 특징 추출(Feature Extraction)을 담당하는 network block을 일컫는다.\n",
    "> -  Head: 최종 task에 대한 추론을 담당하는 network block을 일컫는다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b22465",
   "metadata": {},
   "source": [
    "# Fine tuning(미세조정)\n",
    "\n",
    "-   Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)으로 **추가 학습**시키는 것을 fine tuning 이라고 한다.\n",
    "-   주어진 문제에 더 적합하도록 Feature Extractor의 가중치들 추가로 학습하여 조정 한다.\n",
    "\n",
    "## Fine tuning 전략\n",
    "\n",
    "![transfer02](figures/finetuning_1.png)\n",
    "\n",
    "-   **세 전략 모두 추론기는 trainable로 한다.**\n",
    "   \n",
    "**<span style='font-size:1.2em'>1. 전체 모델을 전부 학습시킨다.(1번)</span>**\n",
    "\n",
    "-   Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n",
    "-   **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n",
    "-   학습에 시간이 많이 걸린다.\n",
    "\n",
    "**<span style='font-size:1.2em'>2. Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.<span>**\n",
    "\n",
    "-   **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
    "-   **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n",
    "\n",
    "**<span style='font-size:1.2em'>3. Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시ont3번)</span>**\n",
    "\n",
    "-   **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
    "\n",
    "> 1번 2번 전략을 Fine tuning 이라고 한다.\n",
    "\n",
    "![transfer02](figures/finetuning_2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54d428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d23187-2b5d-4ef4-aed4-1792210d2c54",
   "metadata": {},
   "source": [
    "# [Huggingface ](https://huggingface.co)\n",
    "\n",
    "Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다. 2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\n",
    "\n",
    "## 주요 서비스\n",
    "1. **Transformers 라이브러리**: 다양한 NLP 작업을 위한 사전 학습된 모델(Pretrained Model)들을 제공하는 라이브러리로, BERT, GPT-3, T5 등의 모델과 huggingface 생태계의 다양한 서비스들을 이용할 수있는 파이썬 기반의 오픈소스 라이브러리.\n",
    "3. **Huggingface Model Hub**: 사용자들이 자신이 훈련한 모델들을 공유 할 수 있는 git 기반 플랫폼. \n",
    "2. **Datasets 라이브러리**: 다양한 NLP 데이터셋을 손쉽게 활용할 수 있도록 하는 라이브러리로, 데이터 전처리 및 관리에 유용하다.\n",
    "4. **API 서비스**: 모델 배포를 위한 API 서비스를 제공하여, 사용자가 쉽게 모델을 배포하고 활용할 수 있도록 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be8364-f75e-41fa-94f5-aeda630a7261",
   "metadata": {},
   "source": [
    "## Transformers 라이브러리\n",
    "\n",
    "Hugging Face의 Transformers는 자연어 처리(NLP) 분야에서 가장 널리 사용되는 라이브러리 중 하나로, 다양한 사전 학습된 모델을 제공하여 연구자들과 개발자들이 손쉽게 NLP 애플리케이션을 구축할 수 있도록 돕는다. 이 라이브러리는 언어 모델링, 텍스트 생성, 번역, 감성 분석, 질문 응답 시스템과 같은 다양한 NLP 작업을 수행할 수 있도록 설계되었다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "1. **광범위한 모델 지원**: BERT, GPT-3, RoBERTa, T5 등 다양한 최첨단 모델들을 지원한다. 이러한 모델들은 텍스트 분류, 감성 분석, 질문 답변, 텍스트 생성 등 다양한 NLP 작업에 활용될 수 있다.\n",
    "2. **파이토치(PyTorch)와 텐서플로우(TensorFlow) 호환**: Transformers 라이브러리는 두 가지 주요 딥러닝 프레임워크와 호환되어, 사용자가 자신의 선호에 따라 선택하여 사용할 수 있다. \n",
    "3. **사용자 친화적인 인터페이스**: 간단하고 직관적인 API를 통해 복잡한 모델을 쉽게 불러오고 사용할 수 있다. 이를 통해 빠르게 프로토타입을 만들고 실험할 수 있다.\n",
    "4. **모델 허브**: HuggingFace Model Hub를 통해 공유되는 수많은 모델들을 Transformers 라이브러리를 이용해 다운받아서 추론까지 쉽게 사용할 수있다.\n",
    "5. **자동화된 파이프라인**: 파이프라인(pipeline) API를 사용하면 몇 줄의 코드로 다양한 NLP 작업을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003e60a-17df-427c-926f-56cc22c4e303",
   "metadata": {},
   "source": [
    "### [주요 라이브러리](https://huggingface.co/docs)\n",
    "\n",
    "- [Transformers 라이브러리](https://huggingface.co/docs/transformers/v4.46.3/ko/index)\n",
    "    - Hugging Face Hub에 저장된 많은 모델들을 쉽게 사용할 수 있도록 해주는 라이브러리.\n",
    "    - 텍스트 생성, 번역, 감성 분석 등 다양한 NLP 작업 뿐 아니라 computer vision 작업등에 활용할 수 있는 모델과 도구를 제공하고 있다.   \n",
    "- [Datasets 라이브러리](https://huggingface.co/docs/datasets/index)\n",
    "    - Hugging Facee Hub에 저장된 다양한 많은 데이터셋을 쉽게 불러오고 관리할 수 있게 해주는 라이브러리.\n",
    "- [Tokenizers 라이브러리](https://huggingface.co/docs/tokenizers/index)\n",
    "    - 텍스트 데이터를 모델에 입력하기 위한 토큰화 작업을 수행하는 라이브러리. 빠르고 효율적인 토크나이징을 지원하며, 다양한 언어와 모델에 맞는 토크나이저를 제공하고 있다. 토크나이저는 대규모 텍스트를 빠르게 처리할 수 있도록 최적화되어 있다.\n",
    "- [PEFT](https://huggingface.co/docs/peft/index)\n",
    "  - LoRA, Prefix-Tuning, Adapters 등 방식의 파라미터 효율적 LLM 파인튜닝 기법 제공."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72a164-75d3-468b-a977-74b55341a098",
   "metadata": {},
   "source": [
    "## Hugging Face Hub\n",
    "- https://huggingface.co/\n",
    "- Hugging Face Hub는 AI 모델과 데이터셋을 공유하고 협업할 수 있는 중앙 플랫폼이다.\n",
    "\n",
    "### 핵심 기능\n",
    "\n",
    "![huggingface_main.png](figures/huggingface_main.png)\n",
    "\n",
    "- **모델 저장소**\n",
    "    - 다양 종류의 많은 공개 모델을 저장하고 제공한다.\n",
    "    - 자연어 처리, 컴퓨터 비전, 음성 인식 등 다양한 분야의 모델을 제공한다.\n",
    "    - 사용자는 원하는 모델을 검색하여 다운 받아 사용, 파인튜닝 등의 작업을 할 수있다.\n",
    "        - transformers 라이브러리 이용. \n",
    "    - 사용자가 파인 튜닝한 모델을 저장할 수있다.\n",
    "    - 개별 모델 페이지에는 모델에 대한 설명, 사용 법, 예제 코드들이 포함 되어 있다. \n",
    "- **데이터셋 관리**\n",
    "    - 다양 종류의 많은 공개 데이터셋을 저장하고 제공한다.\n",
    "    - 사용자는 datasets 라이브러리를 이용해 데이터를 쉽게 다운로드 및 loading할 수있다.\n",
    "    - 사용자는 자신이 만든 데이터셋을 저장하여 공개할 수있다.\n",
    "- **Spaces**\n",
    "    - 사용자가 Streamlit이나 Gradio 기반 AI 웹 어플리케이션을 쉽게 배포할 수있다.\n",
    "    - 모델의 실시간 테스트와 시연이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd474410-af44-4e11-9cc9-ca01a5344f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
