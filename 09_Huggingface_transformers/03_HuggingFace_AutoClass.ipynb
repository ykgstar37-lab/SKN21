{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers AutoClass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩한다.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장된다.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용한다.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "- 기본 모델 Loading\n",
    "    1. **AutoModel**\n",
    "       - 주어진 모델 이름에 맞는 사전 학습된 모델을 자동으로 로드한다.\n",
    "       - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델을 로드한다.\n",
    "    2. **AutoTokenizer**\n",
    "       - 해당 모델에 적합한 토크나이저를 자동으로 로드한다.\n",
    "       - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드한다.\n",
    "    3. **AutoConfig**\n",
    "       - 모델의 설정(config)을 자동으로 로드한다. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델을 생성할 수있다.\n",
    "       - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`\n",
    "- Task 처리 모델 Loading\n",
    "    - Pretrained backbone 모델에 각 task 에 맞는 estimator layer를 추가한 모델을 생성해 제공한다.\n",
    "    - 주요 모델들\n",
    "        1. **AutoModelForSequenceClassification**\n",
    "           - 시퀀스(Text) 분류 작업을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`\n",
    "        2. **AutoModelForQuestionAnswering**\n",
    "           - 질문-응답 작업을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`\n",
    "        3. **AutoModelForTokenClassification**\n",
    "           - 토큰 분류 작업(예: 개체명 인식)을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4d0d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "model_id = \"bert-base-uncased\"\n",
    "# model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "print(type(model))\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "print(type(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c176a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.json - 모델 아키텍쳐 관련 메타정보파일\n",
    "# config\n",
    "model2 = AutoModel.from_config(config) # 학습이 안된 모델.\n",
    "type(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712789f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, GPT2Model\n",
    "b_model = BertModel.from_pretrained(model_id)\n",
    "b_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d351c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# raw-text ==> (Tokenizer) ==> token ids ==> (model) ==> 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79481252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 1037, 2879, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저\n",
    "raw_text = \"I am a boy.\"\n",
    "# 토큰화\n",
    "token = tokenizer(\n",
    "    raw_text,\n",
    "    return_tensors=\"pt\"  # 토큰화 결과 타입 지정. default: list, pt: pytorch, tf: tensorflow, np-numpy\n",
    "    \n",
    ")\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c89c032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token(결과)의 key 들 조회\n",
    "dict(token).keys()\n",
    "\n",
    "# 'input_ids': 입력 토큰 id값.\n",
    "# 'token_type_ids': 입력이 문장 쌍 일 때 문장을 구분하는 id. 0: 첫번째 문장, 1: 두번째 문장. \n",
    "#                   각 토큰이 어느 문장에 속했는지 토큰별로 지정됨.\n",
    "#                   문장쌍: QA(질문-지문)\n",
    "# 'attention_mask': 실제 문장의 토큰과 Padding을 구분한다. 토큰 위치별로 지정. 0: padding, 1: 실제 문장 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af485017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token['input_ids']), type(token['token_type_ids']), type(token['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b381cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# token 결과 -> model\n",
    "\n",
    "# keyword 가변인자에 맞춰서 name=value 형태로 입력. input_ids=[.....], \n",
    "# model에 맞춤 타입으로 입력. (pytorch 모델: torch.tensor)\n",
    "context_vector = model(**token) # BertModel -> feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de815f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.last_hidden_state.shape\n",
    "# 모든 입력토큰들의 hidden state들.\n",
    "# [batch: 1, seq_len: 7, hidden_size(embedding vector):768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 문장(문서)에 대한 context vector - 문장(문서)의 특성값.\n",
    "## Bert는 last_hidden_state에서 첫번째 토큰([CLS]의 값을 가공(특성추출)해서 context vector로 사용.\n",
    "context_vector.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 모델을 한글 텍스트로 학습 시킨 Pretrained model.\n",
    "    - BERT는 Transformer의 Encoder 부분을 이용해 구현된 언어모델\n",
    "    - https://arxiv.org/abs/1810.04805 \n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저, 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6579df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.bert.tokenization_bert_fast.BertTokenizerFast,\n",
       " transformers.models.bert.modeling_bert.BertModel)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = \"beomi/kcbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "type(tokenizer), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34107aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  8616,  9909,  9025,  4196,  4578,  4027, 21452,  4020,    17,\n",
       "             3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 문서 입력: padding-pad추가, truncation-잘라내기 (토큰수 맞추는 작업) 신경쓸 필요없다.\n",
    "tokens = tokenizer(\n",
    "    \"나는 어제 친구와 밥을 먹었다.\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "# tokenizer(토큰화할 문장): 모델에 입력할 수있는 형태로 토큰결과를 반환.\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "461ed8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 어제 친구와 밥을 먹었다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"나는 어제 친구와 밥을 먹었다.\") # input_id만 출력\n",
    "tokenizer.decode([2, 8616, 9909, 9025, 4196, 23905, 21452, 4020, 17, 3]) # token들 -> 문장\n",
    "tokenizer.decode(\n",
    "    [2, 8616, 9909, 9025, 4196, 23905, 21452, 4020, 17, 3],\n",
    "    skip_special_tokens=True # 스페셜 토큰은 제거\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a1a06bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9909, 9039, 1482]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 토큰 관련 조회\n",
    "tokenizer.convert_ids_to_tokens(8616) # 토큰 ID -> 토큰 문자열\n",
    "tokenizer.convert_ids_to_tokens([2, 8616, 9909])\n",
    "\n",
    "tokenizer.convert_tokens_to_ids(\"어제\")\n",
    "tokenizer.convert_tokens_to_ids([\"어제\", \"너는\", \"밖\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c34f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 전체 어휘수\n",
    "print(tokenizer.vocab_size)\n",
    "# tokenizer.get_vocab() # 어휘사전 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 여러 문서 입력\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78342de-1a9c-4b70-a997-c7bca15432ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"안녕\",\n",
    "    \"Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다.\",\n",
    "    \"2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8539917",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = tokenizer(\n",
    "    sentences\n",
    "    # return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db4fce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [[2, 19017, 3], [2, 41, 4223, 4403, 4403, 18940, 39, 4243, 4773, 4226, 4008, 14583, 25061, 11, 22502, 12, 321, 10459, 4071, 9810, 11, 47, 4450, 4579, 12, 16029, 7971, 13064, 8097, 867, 4228, 4196, 16505, 4027, 13248, 7966, 22502, 12296, 4104, 22192, 4020, 17, 3], [2, 26182, 4113, 20684, 4130, 2451, 22088, 20002, 15999, 4266, 4103, 17564, 4408, 4053, 4038, 4196, 11202, 20323, 4130, 47, 4450, 4579, 16505, 4027, 13248, 4027, 13248, 8008, 17, 3]]\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(sent_tokens['input_ids']), sent_tokens['input_ids'])\n",
    "print(len(sent_tokens['attention_mask']))\n",
    "print(len(sent_tokens['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "43\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# 토큰수\n",
    "for ids in sent_tokens['input_ids']:\n",
    "    print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length - 최대길이(토큰수)를 지정. \n",
    "\n",
    "# truncation - 길이를 맞추기 위해서 토큰의 일부를 잘라내는 것.\n",
    "##           - True(\"longest\"): 배치중에서 가장 긴 문서에 맞춘다. max_length 설정이 된 경우에는 max_length에 맞춘다.\n",
    "##           - False(\"do_not_truncate\"): 기본값. Truncation을 안함.\n",
    "\n",
    "# padding - 길이를 맞추기 위해서 [PAD] 토큰으로 채우는 것.\n",
    "##        - True(\"longest\"): 배치중에 가장 긴 문서에 맞춘다.\n",
    "##        - \"max_length\": max_length 설정에 맞춘다. (max_length가 없으면 model의 default max_length에 맞춘다.)\n",
    "##        - False(\"do_not_pad\"): 기본값. Padding 처리 하지 안는다.\n",
    "\n",
    "# 순서: truncation -> padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens2 = tokenizer(\n",
    "    sentences,\n",
    "    max_length=20,\n",
    "    padding=True,  \n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99565463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "43\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokens2['input_ids']:\n",
    "    print(len(sent))#, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27ff7795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens2['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5e879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 context vector 추출\n",
    "#### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 context vector 이다.\n",
    "    - 이 값은 **문장을 입력받아 처리하는 task**(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae0ce7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    sentences,\n",
    "    max_length=20,\n",
    "    padding=True,  \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5787c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad062b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state.shape\n",
    "# [3:batch-문서개수, 20:seq_len(토큰수), 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763d370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.pooler_output.shape\n",
    "# [3, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca04ed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(300, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
