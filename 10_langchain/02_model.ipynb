{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "<!-- # Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74743a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 가상환경\n",
    "# uv venv .venv --python=3.12\n",
    "\n",
    "# .venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd979cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install ipykernel ipywidgets\n",
    "\n",
    "# uv pip install langchain langchain-classic langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - **신용카드가 등록된 경우 바로 충전할 수 있다.**\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "   - 금액 입력 후 `continue`\n",
    "   - 다음 페이지에서 세금 포함 결제 금액 나오면 `Confirm`  한다.\n",
    "\n",
    "2. 신용카드 등록\n",
    "   - Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API Key Load -> OS 환경변수로 등록.(OPENAI_API_KEY)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69de1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI() #api key가 환경변수에 없으면 직접 입력.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\", # 사용할 모델 종류의 이름\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"OpenAI의 LLM 모델 종류는 뭐가 있어?\"}\n",
    "    ]\n",
    ")\n",
    "# chat 요청 프롬프트 형식\n",
    "# \"role\":\"누가 말하는지 - user, ai, system\"\n",
    "# \"content\": 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851be9d2-58e5-464b-87cb-52f09a9a146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CmslY9DLcvrdGRYOQqCSxif81Z7xN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='주로 OpenAI의 LLM은 GPT 계열로 나뉩니다. 주요 모델과 간단한 특징은 아래와 같습니다.\\n\\n- GPT-3 계열\\n  - text-davinci-003, ada/curie/babbage 등 과거 라인업이 남아 있음. 현재 API에서 덜 사용되지만 여전히 일부 용도에 남아 있습니다.\\n\\n- GPT-3.5 계열\\n  - gpt-3.5-turbo: 일반적인 대화형 용도로 가장 많이 쓰임.\\n  - gpt-3.5-turbo-16k: 컨텍스트 윈도우 확장 버전(큰 입력이 필요할 때).\\n\\n- GPT-4 계열\\n  - gpt-4: 고성능 일반 모델(8k/32k 컨텍스트 윈도우 버전이 혼재, API에서 버전에 따라 다르게 제공)\\n  - gpt-4-turbo: 비용 효율이 개선된 GPT-4 계열의 속도 좋은 버전\\n  - gpt-4o: 멀티모달 모델로 이미지 입력 등을 처리할 수 있음(챗에서 이미지 입력 지원)\\n  - 주의: 컨텍스트 윈도우 크기(8k, 32k 등) 버전에 따라 사용 가능 여부가 다릅니다.\\n\\n- Codex 계열(코드용)\\n  - code-davinci-002 등 예전 코드 작성용 모델들. 현재는 GPT 계열로의 통합 방향으로 변화 중이므로 필요 시 문서 확인 권장.\\n\\n참고 및 팁\\n- 어떤 모델을 쓸지 선택은 비용, 응답 속도, 입력 길이 및 멀티모달 필요성 등에 따라 달라집니다.\\n- ChatGPT 같은 대화형 서비스는 사용 중인 플랜에 따라 GPT-3.5-turbo나 GPT-4를 기본으로 제공합니다.\\n- 최신 정보와 컨텍스트 윈도우 크기 및 구체적인 사용 가능 모델은 OpenAI 공식 문서를 확인하는 것이 가장 정확합니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1765766300, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=3200, prompt_tokens=18, total_tokens=3218, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=2752, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7056dd-ddc5-4c0f-8653-47be64287d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주로 OpenAI의 LLM은 GPT 계열로 나뉩니다. 주요 모델과 간단한 특징은 아래와 같습니다.\n",
      "\n",
      "- GPT-3 계열\n",
      "  - text-davinci-003, ada/curie/babbage 등 과거 라인업이 남아 있음. 현재 API에서 덜 사용되지만 여전히 일부 용도에 남아 있습니다.\n",
      "\n",
      "- GPT-3.5 계열\n",
      "  - gpt-3.5-turbo: 일반적인 대화형 용도로 가장 많이 쓰임.\n",
      "  - gpt-3.5-turbo-16k: 컨텍스트 윈도우 확장 버전(큰 입력이 필요할 때).\n",
      "\n",
      "- GPT-4 계열\n",
      "  - gpt-4: 고성능 일반 모델(8k/32k 컨텍스트 윈도우 버전이 혼재, API에서 버전에 따라 다르게 제공)\n",
      "  - gpt-4-turbo: 비용 효율이 개선된 GPT-4 계열의 속도 좋은 버전\n",
      "  - gpt-4o: 멀티모달 모델로 이미지 입력 등을 처리할 수 있음(챗에서 이미지 입력 지원)\n",
      "  - 주의: 컨텍스트 윈도우 크기(8k, 32k 등) 버전에 따라 사용 가능 여부가 다릅니다.\n",
      "\n",
      "- Codex 계열(코드용)\n",
      "  - code-davinci-002 등 예전 코드 작성용 모델들. 현재는 GPT 계열로의 통합 방향으로 변화 중이므로 필요 시 문서 확인 권장.\n",
      "\n",
      "참고 및 팁\n",
      "- 어떤 모델을 쓸지 선택은 비용, 응답 속도, 입력 길이 및 멀티모달 필요성 등에 따라 달라집니다.\n",
      "- ChatGPT 같은 대화형 서비스는 사용 중인 플랜에 따라 GPT-3.5-turbo나 GPT-4를 기본으로 제공합니다.\n",
      "- 최신 정보와 컨텍스트 윈도우 크기 및 구체적인 사용 가능 모델은 OpenAI 공식 문서를 확인하는 것이 가장 정확합니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "        - gpt-5 모델은 추론과정을 거칠 수있고 이 경우 토큰을 소비한다. 그래서 max_tokens를 너무 작게 잡으면 응답이 안올 수있다.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "# 질의\n",
    "response = model.invoke(\"Langchain은 어떤 Framework인지 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "다음처럼 요약해 드릴게요.\n",
      "\n",
      "- LangChain은 무엇인가\n",
      "  - LLM(대형 언어 모델)을 활용한 애플리케이션을 쉽게 만들 수 있게 도와주는 오픈소스 프레임워크입니다.\n",
      "  - 단순한 LLM 호출뿐 아니라, 여러 단계의 처리, 도구(외부 API 등)와의 연동, 기억(memory) 관리까지 복잡한 흐름을 잘 구성하고 관리하도록 도와줍니다. 즉, 모델 자체는 LLM이고, LangChain은 그 모델을 어떻게 호출하고 조합할지에 대한 \"운영 체계\"를 제공합니다.\n",
      "\n",
      "- 핵심 구성 요소\n",
      "  - LLM 및 프롬프트: 여러 LLM 공급자(OpenAI, Cohere 등)를 추상화하고, 프롬프트 템플릿으로 입력 형식을 표준화합니다.\n",
      "  - 체인(Chains): 여러 단계의 순차적 로직을 연결한 흐름. 예를 들어 프롬프트를 만들고 LLM에 보내고, 응답을 후속 단계에 넘겨 추가 처리하는 식으로 구성합니다.\n",
      "  - 에이전트(Agents)와 도구(Tools): 에이전트는 LLM이 입력에 따라 사용할 도구를 스스로 결정하고 실행합니다. 도구는 API 호출, 데이터베이스 질의, 코드 실행 등 외부 행동을 래핑한 것들입니다.\n",
      "  - 기억(memory): 대화 맥락이나 과거 결과를 보존해 다음 응답에 활용합니다. 예: 대화 기록을 유지하는 메모리.\n",
      "  - 벡터 스토어/리트리버(Retrievers): 문서 검색 기반의 RAG( Retrieval-Augmented Generation) 흐름을 구성할 때 문서를 검색하고 LLM에 제공하는 부분.\n",
      "  - 도구 생태계 및 핸들링(Callbacks): 실행 중의 이벤트를 로깅하고 스트리밍 결과를 다루는 등의 확장 포인트.\n",
      "  - 파이프라인 구성의 표준화: Python용 LangChain, TypeScript/JavaScript용 LangChain.js 등으로 구현되어 다양한 런타임에서 사용 가능.\n",
      "\n",
      "- 왜 유용한가\n",
      "  - 복잡한 LLM 응용 프로그램에서의 보일러플레이트를 크게 줄여줍니다.\n",
      "  - 프롬프트 설계, 도구 사용, 메모리 관리 같은 공통 패턴을 재사용 가능한 구성 요소로 제공합니다.\n",
      "  - 에이전트 기반의 자율적 작업 흐름이나 RAG 기반의 정보 검색 애플리케이션을 비교적 쉽게 구축할 수 있습니다.\n",
      "\n",
      "- 일반적인 사용 사례\n",
      "  - 대화형 비서/챗봇: 대화 맥락을 유지하면서 사용자 질의에 응답하고, 필요 시 외부 도구로 웹 검색, 일정 관리 등 수행.\n",
      "  - 문서 질의 응답(RAG): 관련 문서를 검색하고, 그 정보를 바탕으로 질문에 대한 정확한 응답 생성.\n",
      "  - 연구 보조 도구: 데이터 분석 파이프라인에서 step-by-step로 모델의 판단을 도구 호출로 보완.\n",
      "  - 코드 보조/자동화 에이전트: 코드 저장소를 조회하고, 간단한 코드 실행이나 테스트를 도구로 수행.\n",
      "\n",
      "- 시작하기 간단한 방향\n",
      "  - LangChain은 Python 버전과 TypeScript 버전이 있어서, 사용 언어에 맞는 패키지를 설치하고 기본 예제를 따라가면 됩니다.\n",
      "  - 기본 흐름은 보통: LLM 객체 생성 → PromptTemplate 작성 → LLMChain(또는 Sequential/Map-Reduce Chain) 구성 → 필요에 따라 Tools나 Memory 추가 → 실행.\n",
      "  - 공식 문서나 튜토리얼에서 간단한 예제부터 차근차근 따라가면 빠르게 익힐 수 있습니다.\n",
      "\n",
      "추가로 원하시면:\n",
      "- 특정 예시(예: 간단한 Q&A 체인 만들기)나 비교 포인트(파이프라인 vs 에이전트 차이), 시작하기 위한 최소 코드 예시를 한국어로 단계별로 자세히 설명해 드릴게요.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fa2895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 Large Language Model(LLM)을 활용한 애플리케이션 개발을 쉽게 만들어 주는 프레임워크입니다. 프롬프트를 관리하고, 여러 단계의 LLM 실행 흐름을 연결하며, 외부 도구(웹 검색, 계산기, API 호출 등)를 사용해 문제를 해결하는 에이전트를 구성하는 데 필요한 추상화와 도구를 제공합니다.\n",
      "\n",
      "주요 개념과 구성 요소\n",
      "- 프롬프트 관리(Prompt templates): 재사용 가능한 프롬프트 템플릿과 파라미터 바인딩으로 프롬프트를 체계적으로 구성합니다.\n",
      "- 체인(Chains): 여러 LLm 호출을 순서대로 연결해 하나의 복합 작업을 수행하는 파이프라인입니다. 예: 질의 이해 → 정보 검색 → 요약/답변 생성 등.\n",
      "- 에이전트(Agents)와 도구(Tools): LLM이 상황에 따라 외부 도구를 선택하고 사용할 수 있도록 하는 구성요소입니다. 예를 들면 웹 검색 도구, 계산기, 데이터베이스 쿼리, 파일 시스템 작업 등을 연결합니다.\n",
      "- 메모리(Memory): 대화의 맥락이나 상태를 유지하기 위한 저장소로, 대화형 챗봇이나 문서 Q&A에서 과거 대화를 기억하도록 합니다(예: ConversationBufferMemory).\n",
      "- 데이터 인덱스/저장소(Vector Stores): 문서 벡터를 관리하고, 검색/공유를 통해 Retrieval-Augmented Generation(RAG)을 구현합니다. 예: FAISS, Pinecone, Weaviate 등과 연동.\n",
      "- 문서 로더와 데이터 소스(Document Loaders): PDF, HTML, 노트북 등 다양한 문서를 로드하고 전처리합니다.\n",
      "- 테스트와 평가/Hub: 재사용 가능한 프롬프트, 체인, 에이전트를 공유하는 LangChain Hub와 테스트 도구들.\n",
      "\n",
      "무엇을 만들 수 있나\n",
      "- 대화형 챗봇이나 에이전트: 도구를 조합해 사용자가 meant한 작업을 수행하는 자동화된 에이전트.\n",
      "- 문서 기반 Q&A 시스템: 벡터 검색으로 관련 문서를 찾아 LLM이 요약·답변.\n",
      "- 자동화 워크플로우: 데이터 파이프라인에서 여러 단계를 거쳐 정보 추출, 확인, 보고서를 생성하는 시스템.\n",
      "- 지식 기반 검색 및 보강형 응답 시스템: 내부 지식과 외부 정보 소스를 결합한 응답 생성.\n",
      "\n",
      "장점과 특징\n",
      "- LLM 중심의 개발을 구조화하고 재사용 가능하도록 도와줌.\n",
      "- 프롬프트 관리와 흐름 제어가 체계적이어서 유지보수 쉽고 디버깅이 용이.\n",
      "- 도구와 데이터 소스의 연결이 쉬워 다양한 애플리케이션에 확장 가능.\n",
      "- Python과 TypeScript/JavaScript 버전이 있어 서로 다른 스택에서도 사용 가능.\n",
      "- LangChain Hub를 통해 커뮤니티 공유 자원(프롬프트, 체인, 에이전트) 활용 가능.\n",
      "\n",
      "주의할 점\n",
      "- 프레임워크 자체가 모델을 대체해 주는 것은 아니며, LLM의 비용과 응답 품질에 좌우됩니다.\n",
      "- 복잡한 파이프라인일수록 설계와 테스트가 중요합니다. 디버깅이 쉽지 않을 수 있습니다.\n",
      "- 보안 및 프라이버시 이슈를 다룰 때는 외부 API 호출과 데이터 저장소 사용에 주의가 필요합니다.\n",
      "\n",
      "언어 및 런타임\n",
      "- Python 버전과 TypeScript/JavaScript 버전이 주요 타깃입니다.\n",
      "- 로컬 환경은 물론 클라우드나 서버리스 환경에서도 실행 가능.\n",
      "\n",
      "시작 포인트\n",
      "- 공식 문서와 튜토리얼에서 기본 개념과 예제 체인을 따라보는 것을 권장합니다.\n",
      "- Python으로 시작하면 pip install langchain으로 설치하고, 간단한 체인부터 만들어 보며 점진적으로 도구와 벡터 스토어를 추가해 보세요.\n",
      "\n",
      "필요하다면 간단한 예시 흐름이나 설치 방법도 구체적으로 안내해 드릴게요. 어떤 언어(Python or TS)로 시작하고 싶은지 알려주면 맞춤형 가이드로 정리해 드리겠습니다."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\", streaming=True)\n",
    "\n",
    "res = model.stream(\"Langchain은 어떤 Framework인지 설명해줘.\")\n",
    "for token in res:\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea73671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasoning 설정(gpt-5추가)\n",
    "## reasoing: 문제 전제 -> 논리를 전개 -> 결론을 도출 하는 과정을 스스로 계획/구성해서 \n",
    "#            답을 도출하는 방식\n",
    "#  effort: 추론의 깊이(low, medium, high), summary: 추론과정을 출력.\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning={\n",
    "        \"effort\": \"medium\", \"summary\":\"auto\"\n",
    "    }\n",
    ")\n",
    "result = model.invoke(\"LLM 모델에 대한 내용을 초심자를 위해 최대한 쉽게 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'rs_0afa01c955ac174b00693f7dd966488193ab2cac2739baf3a2',\n",
       "  'summary': [{'text': '**Reviewing AI concepts**\\n\\nI’m looking over key concepts regarding AI functionality. It all starts with tokens, which are chunks of data, and parameters, which act as a memory of patterns. I think of transformers and attention mechanisms as tools to highlight relevant words, allowing for long-range dependencies. The context window serves as short-term memory for conversations. Next, temperature and top_p affect the balance between creativity and accuracy. AI can summarize, explain, translate, code, and draft emails, but it may hallucinate or show biases. Finally, effective prompts can guide its responses!',\n",
       "    'type': 'summary_text'},\n",
       "   {'text': '**Organizing AI instructions**\\n\\nI’m considering effective ways to provide examples and structure for user tasks. Starting with basic elements like roles, goals, constraints, steps, styles, and examples is key. Using a clear format like “You are..., Task..., Criteria..., Output format...” can help, along with few-shot examples and prompting users to think step-by-step. \\n\\nI also want to clarify RAG vs. fine-tuning: RAG grounds answers with documents while fine-tuning adapts model styles. Plus, privacy tips like avoiding sensitive info and citing sources are crucial! I’ll also touch on myths and simplify explanations with illustrative micro-demos for better understanding.',\n",
       "    'type': 'summary_text'},\n",
       "   {'text': \"**Preparing a clear explanation**\\n\\nWe’re not required to delve into my knowledge; a general explanation will do. I think we should keep the formatting simple, like using bullet lists, and I’m focusing on clarity in Korean. It’s important to use plain language, avoiding jargon unless I explain it. \\n\\nI’ll outline the pipeline as pretraining, instruction tuning, RLHF, and tool use, with tool use including browsing and code execution as extensions. I’ll include relatable analogies like “predictive text” and “highlighting important parts in sentences.” \\n\\nTo help, I can add a short glossary at the end defining terms like token, parameter, prompt, hallucination, RAG, and fine-tuning, all in bullet format. While I want to avoid being verbose, I recognize the user wants it easy to understand, which suggests a balance—maybe around 600-900 words while being thorough with key points. Let's get started!\",\n",
       "    'type': 'summary_text'}],\n",
       "  'type': 'reasoning'},\n",
       " {'type': 'text',\n",
       "  'text': 'LLM이란? 아주 쉽게 말해, “엄청 똑똑한 자동완성”이에요.\\n휴대폰 키보드가 다음 단어를 맞추듯, LLM은 방대한 글을 읽고 배운 패턴을 바탕으로 다음에 올 글자를 잘 예측하는 프로그램입니다. 이 예측 능력이 충분히 좋아지면 설명, 요약, 번역, 코딩, 글쓰기까지 다양하게 할 수 있게 됩니다.\\n\\n왜 잘할까?\\n- 많이 배움: 인터넷 글, 책, 기사 등 엄청난 양의 텍스트로 훈련돼요.\\n- 패턴 찾기: 문장 속 규칙(말투, 사실의 연결, 문법)을 통계적으로 익혀요.\\n- 한 번에 넓게 보기: “주의(attention)”라는 기술로 문장 전체에서 중요한 부분에 집중합니다. 중요한 단어에 형광펜을 칠하듯 골라보는 느낌이에요.\\n\\n핵심 개념 몇 가지\\n- 토큰: 단어를 잘게 쪼갠 조각. 모델은 토큰 단위로 다음 조각을 예측해요.\\n- 파라미터: 모델이 배운 “손잡이/설정값”. 많을수록 더 복잡한 패턴을 기억할 수 있어요.\\n- 컨텍스트 윈도우: 대화나 문서 중 “지금 참고 중인” 범위. 일종의 단기 기억이에요.\\n- 온도(temperature): 창의성 조절. 낮추면 정답 지향, 높이면 다양하고 창의적.\\n- 프롬프트: 모델에게 주는 지시문. 잘 쓰면 결과가 확 좋아져요.\\n\\n어떤 일에 쓸 수 있나?\\n- 이해와 설명: 어려운 내용을 쉬운 말로 풀어주기, 비유 만들기\\n- 요약과 정리: 긴 문서 핵심 추리기, 회의록 정리\\n- 글쓰기: 이메일/보고서 초안, 블로그 글, 카피 문구\\n- 번역과 톤 조절: 문체 바꾸기, 독자 수준에 맞추기\\n- 아이디어 발상: 제목 후보, 마케팅 콘셉트, 문제 해결 브레인스토밍\\n- 코드: 예제 만들기, 오류 원인 추정, 간단한 스크립트\\n\\nLLM은 무엇이 아닌가?\\n- 검색 엔진이 아님: 최신 사실을 자동으로 찾아오지 않아요(웹브라우징 기능이 붙은 경우는 예외).\\n- “이해”라기보다 “패턴 예측”: 스스로 의식이 있거나 진짜로 아는 건 아니에요.\\n- 항상 정확하지 않음: 근거 없이 그럴듯하게 지어낼 때가 있어요(이걸 환각, hallucination이라고 해요).\\n\\n왜 가끔 틀릴까?\\n- 훈련 데이터에 없는 최신 정보나 아주 구체적 사실은 약해요.\\n- 질문이 모호하면 엉뚱한 가정을 하고 답하기도 해요.\\n- 자신감 있는 말투는 예측의 산물일 뿐, 확신의 근거가 아닐 수 있어요.\\n\\n잘 쓰는 법(프롬프트 팁)\\n- 역할과 목표를 분명히: “당신은 초등학생에게 설명하는 과학 선생님입니다. 광합성을 쉬운 비유로 설명해줘.”\\n- 조건과 형식 지정: “핵심 3줄, 예시 1개, 전문용어 금지.”\\n- 맥락과 자료 제공: “아래 글을 바탕으로 5문장 요약…” 하고 원문 붙이기.\\n- 단계적 사고 유도: “생각 과정을 단계별로 보여줘.” 또는 “체크리스트로 해결해줘.”\\n- 예시 보여주기: 원하는 입력-출력 샘플을 1~2개 붙이면 성능이 크게 좋아져요.\\n- 사실 확인 요구: “출처를 함께 제공해줘.” “확신이 없으면 모른다고 말해줘.”\\n\\n한 단계 고급 사용\\n- RAG(검색 결합): 내 문서를 같이 넣어 답하게 하면 정확도가 오른다. “이 PDF를 근거로만 답해.”\\n- 미세조정(Fine-tuning): 내 문체/도메인에 맞게 추가 학습해 특정 작업을 더 잘하게 만들기.\\n- 도구 활용: 계산기, 웹브라우저, 데이터베이스 같은 외부 도구를 연결하면 실제 검색, 계산, 코드 실행이 가능.\\n\\n안전·윤리·프라이버시\\n- 민감한 정보(개인정보, 기밀)는 넣지 않는 게 원칙.\\n- 중요한 결정 전에는 반드시 2차 검증. 특히 의료, 법률, 재무는 전문가 확인 필수.\\n- 편향 가능성 인지: 훈련 데이터의 편향이 답변에 스며들 수 있어요.\\n\\n자주 묻는 질문 짧게\\n- 모델 크기가 크면 항상 좋나? 대체로 성능이 좋지만 느리고 비싸요. 작은 모델은 빠르고 비용이 저렴.\\n- 왜 가끔 답변을 거부하나? 안전 정책(유해/불법/위험)에 걸리면 제한해요.\\n- 최신 정보를 모를 때는? “오늘 날짜 기준으로 웹에서 찾아 요약해줘” 같은 브라우징 도구가 있는 모델을 쓰거나, 자료를 같이 제공하세요.\\n- 비용은 어떻게 계산돼? 보통 토큰 수(입력+출력)에 따라 과금돼요.\\n\\n초간단 비유로 다시 정리\\n- LLM = 초강력 자동완성. 다음 말(토큰)을 예측하는 기계.\\n- 주의(attention) = 문장 안 중요한 부분에 형광펜.\\n- 프롬프트 = 모델에게 주는 작업지시서.\\n- 환각 = 그럴듯하지만 틀린 내용.\\n- RAG = “자료를 옆에 두고 답하기”.\\n- 미세조정 = “특정 업무에 맞게 과외시키기”.\\n\\n바로 써먹는 예시 프롬프트\\n- “초등학생도 이해할 수 있게, 빛의 굴절을 생활 속 예로 5문장 설명.”\\n- “다음 회의록을 핵심 결론·할 일·담당자 3가지로 정리해줘: …”\\n- “이 이메일을 공손하고 간결하게 바꿔줘. 길이 120자 이하: …”\\n- “이 코드 에러 메시지 원인 후보 3개와 해결 단계 제시: …”\\n\\n핵심만 기억하세요\\n- LLM은 똑똑한 자동완성이다.\\n- 정확도는 프롬프트와 근거 자료가 좌우한다.\\n- 중요한 건 항상 검증한다.\\n- 민감한 정보는 넣지 않는다.\\n\\n원하면, 직접 예제를 가지고 프롬프트를 다듬어 드리거나, 목적에 맞는 모델 선택을 도와드릴게요.',\n",
       "  'annotations': [],\n",
       "  'id': 'msg_0afa01c955ac174b00693f7deeae648193bfd2dc490ff78c30'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reasoning \n",
    "# content에 추론 내용과 응답 내용을 묶어서 반환.\n",
    "# type: reasoning - 추론과정, text - 최종응답.\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f74871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들에 대해 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "결과는 다음 답변형식에 맞춘다.\n",
    "[답변형식]\n",
    "- 행성한국어이름(영어이름): 간단한 한 줄 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afed81fb-5249-4bd3-91b0-4f86a0a75f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양에서 가장 가까운 작은 암석 행성으로 대기가 거의 없음.\n",
      "- 금성(Venus): 지구와 비슷한 크기이나 두꺼운 이산화탄소 대기로 극심한 온실효과를 겪음.\n",
      "- 지구(Earth): 물과 산소가 풍부하여 현재 알려진 유일한 생명체 거주 행성.\n",
      "- 화성(Mars): 붉은 표면을 가진 암석 행성으로 과거 물의 흔적과 활발한 탐사 대상.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 가스 거성으로 강한 자기장과 많은 위성을 가짐.\n",
      "- 토성(Saturn): 눈에 띄는 고리 시스템을 가진 가스 거성으로 다수의 위성을 보유.\n",
      "- 천왕성(Uranus): 옆으로 기울어진 자전축을 가진 얼음 거성으로 청록색을 띰.\n",
      "- 해왕성(Neptune): 태양에서 가장 먼 얼음 거성으로 강한 바람과 선명한 푸른빛을 지님.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-huggingface transformers\n",
    "```\n",
    "- nvidia GPU가 있는 경우 `torch cuda`  버전을 먼저 설치하고 `langchain-huggingface`를 설치 해야 한다. 아니면 `torch cpu` 버전이 설치된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m36 packages\u001b[0m \u001b[2min 265ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 32ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 501ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-huggingface\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install torch==2.8\n",
    "!uv pip install langchain-huggingface transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97542477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "# pipe = pipeline(task=\"text-generation\", model=model_id)\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    task=\"text-generation\",\n",
    "    model_id=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"Huggingface 를 소개해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f589138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huggingface 를 소개해줘.\n",
      "\n",
      "**1. Hugging Face란?**\n",
      "\n",
      "Hugging Face는 자연어 처리 (NLP) 모델 개발 및 공유를 위한 플랫폼입니다. \n",
      "\n",
      "*   **모델:** 챗봇, 번역, 텍스트 생성 등 다양한 NLP 모델을 제공합니다.\n",
      "*   **라이브러리:** Transformers 라이브러리를 통해 다양한 모델을 쉽게 사용할 수 있도록 지원합니다.\n",
      "*   **커뮤니티:** 연구자, 개발자, 기업들이 함께 참여하여 모델을 개발하고 공유합니다.\n",
      "\n",
      "**2. 주요 기능**\n",
      "\n",
      "*   **모델 저장소:** 다양한 모델을 쉽게 다운로드하고 관리할 수 있습니다.\n",
      "*   **Transformers 라이브러리:** 다양한 NLP 모델을 쉽게 사용할 수 있도록 지원합니다.\n",
      "*   **Spaces:** 모델을 쉽게 배포하고 공유할 수 있는 플랫폼입니다.\n",
      "*   **Datasets:** 데이터셋을 쉽게 다운로드하고 관리할 수 있습니다.\n",
      "*   **Hub:** 다양한 모델, 데이터셋, 튜토리얼 등을 한 곳에서 찾을 수 있습니다.\n",
      "\n",
      "**3. Hugging Face 사용법**\n",
      "\n",
      "1.  **Hugging Face 계정 생성:** [https://huggingface.co/](https://huggingface.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9efc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"서울에 주요 여행지 3곳을 알려줘.\"\n",
    "    }\n",
    "]\n",
    "res = model.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "643e204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 서울에 주요 여행지 3곳을 알려줘.\n",
      "\n",
      "인공지능: \n",
      "1. 경복궁: 조선 시대의 아름다운 궁궐로, 역사와 문화를 체험할 수 있습니다.\n",
      "2. 남산: 서울의 대표적인 산으로, 탁 트인 도시 전망을 감상할 수 있습니다.\n",
      "3. 인사동: 전통적인 한국 문화와 예술을 즐길 수 있는 곳입니다.\n",
      "\n",
      "human: 감사합니다.\n",
      "\n",
      "인공지능:\n",
      "1. 남산: 서울의 대표적인 산으로, 탁 트인 도시 전망을 감상할 수 있습니다.\n",
      "2. 경복궁: 조선 시대의 아름다운 궁궐로, 역사와 문화를 체험할 수 있습니다.\n",
      "3. 인사동: 전통적인 한국 문화와 예술을 즐길 수 있는 곳입니다.\n",
      "\n",
      "human: \n",
      "서울에서 가장 인기 있는 관광지 3곳을 알려줘.\n",
      "\n",
      "인공지능:\n",
      "1. 경복궁: 조선 시대의 아름다운 궁궐로, 역사와 문화를 체험할 수 있습니다.\n",
      "2. 남산: 서울의 대표적인 산으로, 탁 트인 도시 전망을 감상할 수 있습니다.\n",
      "3. 인사동: 전통적인 한국 문화와 예술을 즐길 수 있는 곳입니다.\n",
      "\n",
      "human:\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "1. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "2. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "\n",
    "## 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing \n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "```\n",
    "- `anthropic`package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m31 packages\u001b[0m \u001b[2min 180ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 398ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 428ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.75.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-anthropic\u001b[0m\u001b[2m==1.3.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model=\"claude-sonnet-4-5\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f9e81f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='# Anthropic의 LLM 모델\\n\\nAnthropic의 주요 LLM 모델 라인업을 소개합니다:\\n\\n## **Claude 3 시리즈** (최신)\\n\\n### **Claude 3 Opus**\\n- 가장 강력한 최상위 모델\\n- 복잡한 추론, 분석, 창작 작업에 최적화\\n- 높은 정확도가 필요한 전문적 업무에 적합\\n\\n### **Claude 3 Sonnet**\\n- 성능과 속도의 균형을 맞춘 중급 모델\\n- 대부분의 일반적인 업무에 적합\\n- 가성비가 뛰어남\\n\\n### **Claude 3 Haiku**\\n- 가장 빠르고 경량화된 모델\\n- 간단한 질의응답, 실시간 응답이 필요한 경우\\n- 비용 효율적\\n\\n## **Claude 2 시리즈** (이전 세대)\\n\\n### **Claude 2.1**\\n- 200K 토큰의 긴 컨텍스트 윈도우\\n- 문서 분석에 강점\\n\\n### **Claude 2.0**\\n- 초기 Claude 2 버전\\n\\n## **주요 특징**\\n- ✅ 안전성과 유용성에 중점\\n- ✅ Constitutional AI 기술 적용\\n- ✅ 긴 컨텍스트 처리 능력\\n- ✅ 한국어 포함 다국어 지원\\n\\n현재는 **Claude 3 시리즈**가 주력 모델입니다.', additional_kwargs={}, response_metadata={'id': 'msg_015r2A2FcHK4XPst6giGjcZ9', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 49, 'output_tokens': 464, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929', 'model_provider': 'anthropic'}, id='lc_run--019b207c-e4f0-7f42-bd13-169bc1613bec-0', usage_metadata={'input_tokens': 49, 'output_tokens': 464, 'total_tokens': 513, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebb8db47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Anthropic의 LLM 모델\n",
      "\n",
      "Anthropic의 주요 LLM 모델 라인업을 소개합니다:\n",
      "\n",
      "## **Claude 3 시리즈** (최신)\n",
      "\n",
      "### **Claude 3 Opus**\n",
      "- 가장 강력한 최상위 모델\n",
      "- 복잡한 추론, 분석, 창작 작업에 최적화\n",
      "- 높은 정확도가 필요한 전문적 업무에 적합\n",
      "\n",
      "### **Claude 3 Sonnet**\n",
      "- 성능과 속도의 균형을 맞춘 중급 모델\n",
      "- 대부분의 일반적인 업무에 적합\n",
      "- 가성비가 뛰어남\n",
      "\n",
      "### **Claude 3 Haiku**\n",
      "- 가장 빠르고 경량화된 모델\n",
      "- 간단한 질의응답, 실시간 응답이 필요한 경우\n",
      "- 비용 효율적\n",
      "\n",
      "## **Claude 2 시리즈** (이전 세대)\n",
      "\n",
      "### **Claude 2.1**\n",
      "- 200K 토큰의 긴 컨텍스트 윈도우\n",
      "- 문서 분석에 강점\n",
      "\n",
      "### **Claude 2.0**\n",
      "- 초기 Claude 2 버전\n",
      "\n",
      "## **주요 특징**\n",
      "- ✅ 안전성과 유용성에 중점\n",
      "- ✅ Constitutional AI 기술 적용\n",
      "- ✅ 긴 컨텍스트 처리 능력\n",
      "- ✅ 한국어 포함 다국어 지원\n",
      "\n",
      "현재는 **Claude 3 시리즈**가 주력 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `ollama` package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m27 packages\u001b[0m \u001b[2min 197ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 50ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 51ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-ollama\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma3:1b\" # ollama run 하고 실행할 때 사용한 이름.\n",
    ")\n",
    "res = model.invoke(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 네, 무엇을 도와드릴까요? 😊 혹시 궁금한 점이나 이야기하고 싶은 것이 있다면 편하게 말씀해 주세요.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "611ae85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI(인공지능)란, **인간의 지능을 모방하거나 능가할 수 있도록 컴퓨터 시스템을 설계하고 제작하는 기술**이라고 말할 수 있습니다. 좀 더 구체적으로 말씀드리면, AI는 다음과 같은 특징을 가집니다:\\n\\n**1. 학습 능력:** AI는 데이터를 통해 스스로 학습하고 새로운 지식을 습득하는 능력을 가지고 있습니다. 기존 데이터를 기반으로 패턴을 파악하고, 유사한 데이터에 대한 예측이나 판단을 내릴 수 있습니다.\\n\\n**2. 논리적 사고:** AI는 특정 문제 해결이나 정해진 규칙에 따라 행동하도록 설계될 수 있으며, 문제 해결 과정을 진행하는 인지적인 능력을 발전시키고자 합니다.\\n\\n**3. 추론 능력:** AI는 주어진 정보를 바탕으로 논리적 사고를 통해 새로운 추론과 결론을 이끌어낼 수 있습니다.\\n\\n**4. 의사 결정 능력:** AI는 데이터를 분석하고 다양한 시나리오를 고려하여 최적의 결정을 내릴 수 있습니다. \\n\\n**AI는 현재 다양한 기술 분야에 적용되어 있으며, 그 종류도 매우 다양합니다.**\\n\\n**AI의 주요 분야 및 활용 예시는 다음과 같습니다:**\\n\\n*   **머신러닝 (Machine Learning):** 데이터와 알고리즘을 사용하여 시스템이 스스로를 개선하도록 하는 기술. (예: 추천 시스템, 예측 분석)\\n*   **딥 러닝 (Deep Learning):** 인공 신경망을 사용하여 복잡하고 높은 정밀도의 데이터 분석을 가능하게 하는 기술. (예: 이미지 인식, 자연어 처리)\\n*   **자연어 처리 (Natural Processing):** 컴퓨터가 인간의 언어를 이해하고 처리를 하고 생성할 수 있도록 하는 기술. (예: 챗봇, 음성 인식)\\n*   **컴퓨터 비전 (Computer Vision):** 컴퓨터가 이미지나 영상을 분석하고 이해하는 기술. (예: 자율주행차, 얼굴 인식)\\n*   **로보틱스 (robotics):** AI를 활용하여 로봇을 설계, 개발, 조립, 작동시키는 기술. (예: 자동화, 생산성 향상)\\n\\n**AI가 현재 사용되는 예시**\\n\\n*   **음악 추천 시스템:** 사용자의 음악 취향에 맞춰 음악을 추천해주는 프로그램\\n*   **검색 엔진:** 사용자의 질문에 가장 적절한 답변을 제공하는 검색 엔진\\n*   **음성 인화:** 사람의 음성을 텍스트로 변환해주는 기술\\n*   **온라인 번역:** 웹 페이지에서 어떤 문장이 작성되었는지 자동으로 번역해주는 기술\\n*   **챗봇:** 사람과의 대화이 가능한 컴퓨터 프로그램\\n\\n**AI는 앞으로 사회 전반에 걸쳐 더욱 중요하게 작용하고 있으며, 인간을 대체하거나 협력하는 방식으로 활용될 가능성도 있습니다.**\\n\\n더 궁금한 내용이 있으시면 언제든지 질문해 주세요! 어떤 점이 특히 궁금하신가요? 예를 들어, 다음과 같은 질문을 통해 좀 더 자세한 설명이 가능합니다.\\n\\n*   특정 AI 기술 (예, 딥 러닝)에 대해 궁금하신가요?\\n*   AI가 현재 사회에 미치는 영향에 대해 알고 싶나요?\\n*   AI가 미래에 어떤 분야에서 중요한 역할을 할까요?', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-15T06:10:19.0034372Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19354405400, 'load_duration': 202958500, 'prompt_eval_count': 15, 'prompt_eval_duration': 88851200, 'eval_count': 696, 'eval_duration': 18577303900, 'logprobs': None, 'model_name': 'gemma3:1b', 'model_provider': 'ollama'}, id='lc_run--019b20a1-495f-7411-afe6-b056975b2c50-0', usage_metadata={'input_tokens': 15, 'output_tokens': 696, 'total_tokens': 711})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.invoke(\"AI란 무엇인가요?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67dabebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI(인공지능)란, **인간의 지능을 모방하거나 능가할 수 있도록 컴퓨터 시스템을 설계하고 제작하는 기술**이라고 말할 수 있습니다. 좀 더 구체적으로 말씀드리면, AI는 다음과 같은 특징을 가집니다:\n",
      "\n",
      "**1. 학습 능력:** AI는 데이터를 통해 스스로 학습하고 새로운 지식을 습득하는 능력을 가지고 있습니다. 기존 데이터를 기반으로 패턴을 파악하고, 유사한 데이터에 대한 예측이나 판단을 내릴 수 있습니다.\n",
      "\n",
      "**2. 논리적 사고:** AI는 특정 문제 해결이나 정해진 규칙에 따라 행동하도록 설계될 수 있으며, 문제 해결 과정을 진행하는 인지적인 능력을 발전시키고자 합니다.\n",
      "\n",
      "**3. 추론 능력:** AI는 주어진 정보를 바탕으로 논리적 사고를 통해 새로운 추론과 결론을 이끌어낼 수 있습니다.\n",
      "\n",
      "**4. 의사 결정 능력:** AI는 데이터를 분석하고 다양한 시나리오를 고려하여 최적의 결정을 내릴 수 있습니다. \n",
      "\n",
      "**AI는 현재 다양한 기술 분야에 적용되어 있으며, 그 종류도 매우 다양합니다.**\n",
      "\n",
      "**AI의 주요 분야 및 활용 예시는 다음과 같습니다:**\n",
      "\n",
      "*   **머신러닝 (Machine Learning):** 데이터와 알고리즘을 사용하여 시스템이 스스로를 개선하도록 하는 기술. (예: 추천 시스템, 예측 분석)\n",
      "*   **딥 러닝 (Deep Learning):** 인공 신경망을 사용하여 복잡하고 높은 정밀도의 데이터 분석을 가능하게 하는 기술. (예: 이미지 인식, 자연어 처리)\n",
      "*   **자연어 처리 (Natural Processing):** 컴퓨터가 인간의 언어를 이해하고 처리를 하고 생성할 수 있도록 하는 기술. (예: 챗봇, 음성 인식)\n",
      "*   **컴퓨터 비전 (Computer Vision):** 컴퓨터가 이미지나 영상을 분석하고 이해하는 기술. (예: 자율주행차, 얼굴 인식)\n",
      "*   **로보틱스 (robotics):** AI를 활용하여 로봇을 설계, 개발, 조립, 작동시키는 기술. (예: 자동화, 생산성 향상)\n",
      "\n",
      "**AI가 현재 사용되는 예시**\n",
      "\n",
      "*   **음악 추천 시스템:** 사용자의 음악 취향에 맞춰 음악을 추천해주는 프로그램\n",
      "*   **검색 엔진:** 사용자의 질문에 가장 적절한 답변을 제공하는 검색 엔진\n",
      "*   **음성 인화:** 사람의 음성을 텍스트로 변환해주는 기술\n",
      "*   **온라인 번역:** 웹 페이지에서 어떤 문장이 작성되었는지 자동으로 번역해주는 기술\n",
      "*   **챗봇:** 사람과의 대화이 가능한 컴퓨터 프로그램\n",
      "\n",
      "**AI는 앞으로 사회 전반에 걸쳐 더욱 중요하게 작용하고 있으며, 인간을 대체하거나 협력하는 방식으로 활용될 가능성도 있습니다.**\n",
      "\n",
      "더 궁금한 내용이 있으시면 언제든지 질문해 주세요! 어떤 점이 특히 궁금하신가요? 예를 들어, 다음과 같은 질문을 통해 좀 더 자세한 설명이 가능합니다.\n",
      "\n",
      "*   특정 AI 기술 (예, 딥 러닝)에 대해 궁금하신가요?\n",
      "*   AI가 현재 사회에 미치는 영향에 대해 알고 싶나요?\n",
      "*   AI가 미래에 어떤 분야에서 중요한 역할을 할까요?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "906d1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "다음 내용을 3 문장으로 요약 해줘.\n",
    "\n",
    "<요약할 내용>\n",
    "AI(인공지능)란, **인간의 지능을 모방하거나 능가할 수 있도록 컴퓨터 시스템을 설계하고 제작하는 기술**이라고 말할 수 있습니다. 좀 더 구체적으로 말씀드리면, AI는 다음과 같은 특징을 가집니다:\n",
    "\n",
    "**1. 학습 능력:** AI는 데이터를 통해 스스로 학습하고 새로운 지식을 습득하는 능력을 가지고 있습니다. 기존 데이터를 기반으로 패턴을 파악하고, 유사한 데이터에 대한 예측이나 판단을 내릴 수 있습니다.\n",
    "\n",
    "**2. 논리적 사고:** AI는 특정 문제 해결이나 정해진 규칙에 따라 행동하도록 설계될 수 있으며, 문제 해결 과정을 진행하는 인지적인 능력을 발전시키고자 합니다.\n",
    "\n",
    "**3. 추론 능력:** AI는 주어진 정보를 바탕으로 논리적 사고를 통해 새로운 추론과 결론을 이끌어낼 수 있습니다.\n",
    "\n",
    "**4. 의사 결정 능력:** AI는 데이터를 분석하고 다양한 시나리오를 고려하여 최적의 결정을 내릴 수 있습니다. \n",
    "\n",
    "**AI는 현재 다양한 기술 분야에 적용되어 있으며, 그 종류도 매우 다양합니다.**\n",
    "\n",
    "**AI의 주요 분야 및 활용 예시는 다음과 같습니다:**\n",
    "\n",
    "*   **머신러닝 (Machine Learning):** 데이터와 알고리즘을 사용하여 시스템이 스스로를 개선하도록 하는 기술. (예: 추천 시스템, 예측 분석)\n",
    "*   **딥 러닝 (Deep Learning):** 인공 신경망을 사용하여 복잡하고 높은 정밀도의 데이터 분석을 가능하게 하는 기술. (예: 이미지 인식, 자연어 처리)\n",
    "*   **자연어 처리 (Natural Processing):** 컴퓨터가 인간의 언어를 이해하고 처리를 하고 생성할 수 있도록 하는 기술. (예: 챗봇, 음성 인식)\n",
    "*   **컴퓨터 비전 (Computer Vision):** 컴퓨터가 이미지나 영상을 분석하고 이해하는 기술. (예: 자율주행차, 얼굴 인식)\n",
    "*   **로보틱스 (robotics):** AI를 활용하여 로봇을 설계, 개발, 조립, 작동시키는 기술. (예: 자동화, 생산성 향상)\n",
    "\n",
    "**AI가 현재 사용되는 예시**\n",
    "\n",
    "*   **음악 추천 시스템:** 사용자의 음악 취향에 맞춰 음악을 추천해주는 프로그램\n",
    "*   **검색 엔진:** 사용자의 질문에 가장 적절한 답변을 제공하는 검색 엔진\n",
    "*   **음성 인화:** 사람의 음성을 텍스트로 변환해주는 기술\n",
    "*   **온라인 번역:** 웹 페이지에서 어떤 문장이 작성되었는지 자동으로 번역해주는 기술\n",
    "*   **챗봇:** 사람과의 대화이 가능한 컴퓨터 프로그램\n",
    "\n",
    "**AI는 앞으로 사회 전반에 걸쳐 더욱 중요하게 작용하고 있으며, 인간을 대체하거나 협력하는 방식으로 활용될 가능성도 있습니다.**\n",
    "\n",
    "더 궁금한 내용이 있으시면 언제든지 질문해 주세요! 어떤 점이 특히 궁금하신가요? 예를 들어, 다음과 같은 질문을 통해 좀 더 자세한 설명이 가능합니다.\n",
    "\n",
    "*   특정 AI 기술 (예, 딥 러닝)에 대해 궁금하신가요?\n",
    "*   AI가 현재 사회에 미치는 영향에 대해 알고 싶나요?\n",
    "*   AI가 미래에 어떤 분야에서 중요한 역할을 할까요?\n",
    "</요약할 내용>\n",
    "\"\"\"\n",
    "\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8a25c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음과 같이 요약할 수 있습니다.\n",
      "\n",
      "AI는 인간 지능을 모방하거나 뛰어넘는 컴퓨터 시스템을 설계하여 만든 기술이며, 학습, 논리적 사고, 추론 등 핵심 능력들을 가지고 있습니다. 현재 다양한 분야에 적용되고 있으며, 머신러닝, 딥 러닝, 자연어 처리 등 주요 기술들이 AI를 발전시키는 데 큰 역할을 합니다. AI는 사회 전반에 걸쳐 다양한 분야에서 활용되며, 미래에 더욱 중요해질 것으로 전망됩니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/api-keys\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. `API Key 만들기` 선택\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `키이름 지정`하고 `가져온 프로젝트 선택`에서 프로젝트 선택(없으면 만든다) 하고 키 만들기\n",
    "4. 키가 생성되면 왼쪽에 복사 아이콘을 클릭하면 키가 복사된다.\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "## 설치\n",
    "- `pip install langchain_google_genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m36 packages\u001b[0m \u001b[2min 268ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m9 packages\u001b[0m \u001b[2min 260ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 529ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.55.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-google-genai\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-genai<2.0.0,>=1.53.0 (from langchain_google_genai)\n",
      "  Downloading google_genai-1.55.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.1.2 (from langchain_google_genai)\n",
      "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from langchain_google_genai)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (9.1.2)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (4.15.0)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (3.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (6.2.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (0.16.0)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading langsmith-0.4.59-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\playdata\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (25.0)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain_google_genai)\n",
      "  Downloading zstandard-0.25.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->langchain_google_genai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.0.0->langchain_google_genai)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.0.0->langchain_google_genai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2.5.0)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_genai-1.55.0-py3-none-any.whl (703 kB)\n",
      "   ---------------------------------------- 0.0/703.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 703.4/703.4 kB 31.6 MB/s  0:00:00\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.4.59-py3-none-any.whl (413 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 67.6 MB/s  0:00:00\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.25.0-cp313-cp313-win_amd64.whl (506 kB)\n",
      "Installing collected packages: filetype, zstandard, websockets, uuid-utils, typing-inspection, pyyaml, pydantic-core, pyasn1, orjson, jsonpointer, httpcore, distro, anyio, annotated-types, rsa, requests-toolbelt, pydantic, pyasn1-modules, jsonpatch, httpx, langsmith, google-auth, langchain-core, google-genai, langchain_google_genai\n",
      "\n",
      "   ----------------------------------------  0/25 [filetype]\n",
      "   ----------------------------------------  0/25 [filetype]\n",
      "   ----------------------------------------  0/25 [filetype]\n",
      "   ----------------------------------------  0/25 [filetype]\n",
      "   - --------------------------------------  1/25 [zstandard]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   --- ------------------------------------  2/25 [websockets]\n",
      "   ---- -----------------------------------  3/25 [uuid-utils]\n",
      "   -------- -------------------------------  5/25 [pyyaml]\n",
      "   -------- -------------------------------  5/25 [pyyaml]\n",
      "   -------- -------------------------------  5/25 [pyyaml]\n",
      "   ----------- ----------------------------  7/25 [pyasn1]\n",
      "   ----------- ----------------------------  7/25 [pyasn1]\n",
      "   ----------- ----------------------------  7/25 [pyasn1]\n",
      "   ----------- ----------------------------  7/25 [pyasn1]\n",
      "   ------------ ---------------------------  8/25 [orjson]\n",
      "   ---------------- ----------------------- 10/25 [httpcore]\n",
      "   ---------------- ----------------------- 10/25 [httpcore]\n",
      "   ---------------- ----------------------- 10/25 [httpcore]\n",
      "   ---------------- ----------------------- 10/25 [httpcore]\n",
      "   ----------------- ---------------------- 11/25 [distro]\n",
      "   ------------------- -------------------- 12/25 [anyio]\n",
      "   ------------------- -------------------- 12/25 [anyio]\n",
      "   ------------------- -------------------- 12/25 [anyio]\n",
      "   ------------------- -------------------- 12/25 [anyio]\n",
      "   ------------------- -------------------- 12/25 [anyio]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ---------------------- ----------------- 14/25 [rsa]\n",
      "   ------------------------ --------------- 15/25 [requests-toolbelt]\n",
      "   ------------------------ --------------- 15/25 [requests-toolbelt]\n",
      "   ------------------------ --------------- 15/25 [requests-toolbelt]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   ------------------------- -------------- 16/25 [pydantic]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   --------------------------- ------------ 17/25 [pyasn1-modules]\n",
      "   ---------------------------- ----------- 18/25 [jsonpatch]\n",
      "   ------------------------------ --------- 19/25 [httpx]\n",
      "   ------------------------------ --------- 19/25 [httpx]\n",
      "   ------------------------------ --------- 19/25 [httpx]\n",
      "   ------------------------------ --------- 19/25 [httpx]\n",
      "   ------------------------------ --------- 19/25 [httpx]\n",
      "   ------------------------------ --------- 19/25 [httpx]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   -------------------------------- ------- 20/25 [langsmith]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   --------------------------------- ------ 21/25 [google-auth]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ----------------------------------- ---- 22/25 [langchain-core]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   ------------------------------------ --- 23/25 [google-genai]\n",
      "   -------------------------------------- - 24/25 [langchain_google_genai]\n",
      "   -------------------------------------- - 24/25 [langchain_google_genai]\n",
      "   -------------------------------------- - 24/25 [langchain_google_genai]\n",
      "   ---------------------------------------- 25/25 [langchain_google_genai]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.12.0 distro-1.9.0 filetype-1.2.0 google-auth-2.43.0 google-genai-1.55.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-1.2.0 langchain_google_genai-4.0.0 langsmith-0.4.59 orjson-3.11.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.12.5 pydantic-core-2.41.5 pyyaml-6.0.3 requests-toolbelt-1.0.0 rsa-4.9.1 typing-inspection-0.4.2 uuid-utils-0.12.0 websockets-15.0.1 zstandard-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Playdata\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45de0cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3fa851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", #  무료: 분당 60회, 하루 1000회 까지 무료\n",
    ")\n",
    "res = model.invoke(\"gemini와 gemma 모델의 차이는 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1859933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, **목표, 규모, 기능, 그리고 접근 방식**에서 중요한 차이를 가집니다.\n",
      "\n",
      "핵심적인 차이점을 요약하자면 다음과 같습니다:\n",
      "\n",
      "| 카테고리       | Gemini                                          | Gemma                                                              |\n",
      "| :------------- | :---------------------------------------------- | :----------------------------------------------------------------- |\n",
      "| **목표**       | Google의 플래그십, 최첨단 AI, 범용성 및 최고 성능 | 개발자 커뮤니티를 위한 고품질, 경량화된 오픈 모델 (Open Models) 제공 |\n",
      "| **규모**       | 매우 거대함 (파라미터 수 미공개, 수천억 개 이상 추정), 여러 버전 존재 (Ultra, Pro, Nano) | 비교적 작음 (2B, 7B 파라미터), 효율성 및 로컬 실행에 최적화         |\n",
      "| **모달리티**   | **다중 모달리티(Multimodal)**: 텍스트, 코드, 이미지, 오디오, 비디오 이해 및 생성 | **주로 텍스트 기반**: 텍스트 생성 및 이해에 특화                  |\n",
      "| **접근 방식**  | **독점(Proprietary)**: Google 서비스(Bard/Gemini Advanced, Duet AI) 및 클라우드 API(Vertex AI)를 통해 접근 | **오픈 모델(Open Models)**: 모델 가중치(weights)를 공개하여 다운로드 및 로컬 실행, 파인튜닝 가능 |\n",
      "| **대상 사용자** | 기업, 대규모 개발자, Google 서비스 최종 사용자     | 개발자, 연구자, 스타트업, 개인 (로컬 환경 또는 자체 서버에 배포)    |\n",
      "| **성능**       | 최첨단(State-of-the-Art) 성능, 복잡한 추론 및 다중 작업 처리 | 해당 규모에서 매우 뛰어난 성능, 효율적인 자원 사용                 |\n",
      "| **용도**       | 복잡한 문제 해결, 창의적 콘텐츠 생성, 고도화된 대화, 멀티미디어 분석 | 챗봇, 요약, 코드 생성, 특정 도메인 파인튜닝, 연구 및 교육용         |\n",
      "| **기반 기술**  | 구글의 최신 AI 연구 및 개발 역량의 집약체        | Gemini의 연구 및 기술을 기반으로 경량화되고 최적화된 모델           |\n",
      "\n",
      "### 상세 설명:\n",
      "\n",
      "1.  **목표 및 철학:**\n",
      "    *   **Gemini:** Google의 가장 야심 찬 프로젝트이자 플래그십 AI 모델입니다. 사람처럼 복잡한 추론, 이해, 생성 능력을 갖추는 것을 목표로 하며, 가능한 한 많은 작업을 처리할 수 있는 범용적인 최첨단 인공지능을 지향합니다.\n",
      "    *   **Gemma:** Gemini의 기반 기술과 연구 역량을 활용하여 개발되었지만, 목표는 다릅니다. 개발자 커뮤니티와 연구자들이 쉽게 접근하고 활용할 수 있도록 **고품질의 경량화된 오픈 모델**을 제공하는 것이 주된 목표입니다. 이를 통해 AI 혁신을 가속화하고, AI의 민주화를 추구합니다.\n",
      "\n",
      "2.  **규모 및 성능:**\n",
      "    *   **Gemini:** 매우 거대한 모델이며, Ultra, Pro, Nano와 같은 다양한 크기와 능력의 버전이 존재합니다. 가장 큰 Gemini Ultra는 현존하는 모델 중 최고 수준의 성능을 자랑하며, 복잡한 추론, 수학, 코딩 능력에서 탁월합니다.\n",
      "    *   **Gemma:** 2B(20억), 7B(70억) 파라미터와 같은 비교적 작은 규모로 제공됩니다. 이 모델들은 크기가 작음에도 불구하고 동급 모델 중에서 뛰어난 성능을 보여주며, 효율성과 로컬 환경에서의 실행에 초점을 맞추고 있습니다. 당연히 가장 큰 Gemini 모델보다는 성능이 낮지만, 특정 작업이나 자원 제약이 있는 환경에서는 매우 유용합니다.\n",
      "\n",
      "3.  **모달리티 (Modality):**\n",
      "    *   **Gemini:** **진정한 다중 모달리티 모델**입니다. 텍스트, 이미지, 오디오, 비디오 등 다양한 유형의 정보를 동시에 이해하고 처리할 수 있으며, 이러한 다양한 데이터를 기반으로 새로운 콘텐츠를 생성할 수 있습니다. 예를 들어, 이미지와 텍스트를 함께 보여주며 질문하면 그에 맞는 답변을 할 수 있습니다.\n",
      "    *   **Gemma:** 현재 공개된 버전들은 **주로 텍스트 기반**입니다. 텍스트를 이해하고 생성하는 데 특화되어 있으며, 자연어 처리(NLP) 작업에 강력합니다. (향후 버전에서 모달리티가 확장될 가능성도 있습니다.)\n",
      "\n",
      "4.  **접근성 및 라이선스:**\n",
      "    *   **Gemini:** Google의 **독점(Proprietary)** 기술입니다. Google의 서비스(Bard/Gemini Advanced, Duet AI 등)나 Google Cloud의 Vertex AI 플랫폼을 통한 API 호출 방식으로만 접근할 수 있습니다. 모델의 내부 구조나 가중치는 공개되지 않습니다.\n",
      "    *   **Gemma:** **\"오픈 모델(Open Models)\"**로 분류됩니다. 모델의 가중치(weights)를 공개하여 개발자들이 직접 다운로드하여 로컬 서버나 클라우드 환경에서 실행하고, 자신들의 특정 목적에 맞게 파인튜닝(fine-tuning)할 수 있습니다. 상업적 사용을 포함한 특정 라이선스 정책을 따릅니다.\n",
      "\n",
      "5.  **대상 사용자 및 용도:**\n",
      "    *   **Gemini:** Google의 최고 수준 AI를 필요로 하는 대기업, 대규모 서비스 개발자, 그리고 Google 서비스를 이용하는 일반 최종 사용자들을 대상으로 합니다. 복잡한 문제 해결, 창의적인 콘텐츠 생성, 고급 분석, 그리고 강력한 AI 기반 서비스 구축에 적합합니다.\n",
      "    *   **Gemma:** AI 기술을 탐구하고, 로컬 환경에서 모델을 실행하거나, 특정 애플리케이션에 맞게 모델을 수정하려는 개발자, 연구자, 학생, 스타트업 등을 위한 것입니다. 챗봇, 요약 도구, 코드 생성 보조, 교육 및 연구 프로젝트 등 다양한 용도로 활용될 수 있습니다.\n",
      "\n",
      "요약하자면, **Gemini는 Google의 최첨단 기술을 집약한 \"완전체\"이자 \"블랙박스\" 형태의 AI 서비스 플랫폼**인 반면, **Gemma는 Gemini의 연구 기반 위에서 탄생한 \"경량화되고 개방된\" AI 도구 키트**라고 볼 수 있습니다. 둘 다 Google AI의 중요한 축을 담당하지만, 각각 다른 필요와 목표를 충족하기 위해 설계되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18500ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, **목표, 규모, 기능, 그리고 접근 방식**에서 중요한 차이를 가집니다.\n",
       "\n",
       "핵심적인 차이점을 요약하자면 다음과 같습니다:\n",
       "\n",
       "| 카테고리       | Gemini                                          | Gemma                                                              |\n",
       "| :------------- | :---------------------------------------------- | :----------------------------------------------------------------- |\n",
       "| **목표**       | Google의 플래그십, 최첨단 AI, 범용성 및 최고 성능 | 개발자 커뮤니티를 위한 고품질, 경량화된 오픈 모델 (Open Models) 제공 |\n",
       "| **규모**       | 매우 거대함 (파라미터 수 미공개, 수천억 개 이상 추정), 여러 버전 존재 (Ultra, Pro, Nano) | 비교적 작음 (2B, 7B 파라미터), 효율성 및 로컬 실행에 최적화         |\n",
       "| **모달리티**   | **다중 모달리티(Multimodal)**: 텍스트, 코드, 이미지, 오디오, 비디오 이해 및 생성 | **주로 텍스트 기반**: 텍스트 생성 및 이해에 특화                  |\n",
       "| **접근 방식**  | **독점(Proprietary)**: Google 서비스(Bard/Gemini Advanced, Duet AI) 및 클라우드 API(Vertex AI)를 통해 접근 | **오픈 모델(Open Models)**: 모델 가중치(weights)를 공개하여 다운로드 및 로컬 실행, 파인튜닝 가능 |\n",
       "| **대상 사용자** | 기업, 대규모 개발자, Google 서비스 최종 사용자     | 개발자, 연구자, 스타트업, 개인 (로컬 환경 또는 자체 서버에 배포)    |\n",
       "| **성능**       | 최첨단(State-of-the-Art) 성능, 복잡한 추론 및 다중 작업 처리 | 해당 규모에서 매우 뛰어난 성능, 효율적인 자원 사용                 |\n",
       "| **용도**       | 복잡한 문제 해결, 창의적 콘텐츠 생성, 고도화된 대화, 멀티미디어 분석 | 챗봇, 요약, 코드 생성, 특정 도메인 파인튜닝, 연구 및 교육용         |\n",
       "| **기반 기술**  | 구글의 최신 AI 연구 및 개발 역량의 집약체        | Gemini의 연구 및 기술을 기반으로 경량화되고 최적화된 모델           |\n",
       "\n",
       "### 상세 설명:\n",
       "\n",
       "1.  **목표 및 철학:**\n",
       "    *   **Gemini:** Google의 가장 야심 찬 프로젝트이자 플래그십 AI 모델입니다. 사람처럼 복잡한 추론, 이해, 생성 능력을 갖추는 것을 목표로 하며, 가능한 한 많은 작업을 처리할 수 있는 범용적인 최첨단 인공지능을 지향합니다.\n",
       "    *   **Gemma:** Gemini의 기반 기술과 연구 역량을 활용하여 개발되었지만, 목표는 다릅니다. 개발자 커뮤니티와 연구자들이 쉽게 접근하고 활용할 수 있도록 **고품질의 경량화된 오픈 모델**을 제공하는 것이 주된 목표입니다. 이를 통해 AI 혁신을 가속화하고, AI의 민주화를 추구합니다.\n",
       "\n",
       "2.  **규모 및 성능:**\n",
       "    *   **Gemini:** 매우 거대한 모델이며, Ultra, Pro, Nano와 같은 다양한 크기와 능력의 버전이 존재합니다. 가장 큰 Gemini Ultra는 현존하는 모델 중 최고 수준의 성능을 자랑하며, 복잡한 추론, 수학, 코딩 능력에서 탁월합니다.\n",
       "    *   **Gemma:** 2B(20억), 7B(70억) 파라미터와 같은 비교적 작은 규모로 제공됩니다. 이 모델들은 크기가 작음에도 불구하고 동급 모델 중에서 뛰어난 성능을 보여주며, 효율성과 로컬 환경에서의 실행에 초점을 맞추고 있습니다. 당연히 가장 큰 Gemini 모델보다는 성능이 낮지만, 특정 작업이나 자원 제약이 있는 환경에서는 매우 유용합니다.\n",
       "\n",
       "3.  **모달리티 (Modality):**\n",
       "    *   **Gemini:** **진정한 다중 모달리티 모델**입니다. 텍스트, 이미지, 오디오, 비디오 등 다양한 유형의 정보를 동시에 이해하고 처리할 수 있으며, 이러한 다양한 데이터를 기반으로 새로운 콘텐츠를 생성할 수 있습니다. 예를 들어, 이미지와 텍스트를 함께 보여주며 질문하면 그에 맞는 답변을 할 수 있습니다.\n",
       "    *   **Gemma:** 현재 공개된 버전들은 **주로 텍스트 기반**입니다. 텍스트를 이해하고 생성하는 데 특화되어 있으며, 자연어 처리(NLP) 작업에 강력합니다. (향후 버전에서 모달리티가 확장될 가능성도 있습니다.)\n",
       "\n",
       "4.  **접근성 및 라이선스:**\n",
       "    *   **Gemini:** Google의 **독점(Proprietary)** 기술입니다. Google의 서비스(Bard/Gemini Advanced, Duet AI 등)나 Google Cloud의 Vertex AI 플랫폼을 통한 API 호출 방식으로만 접근할 수 있습니다. 모델의 내부 구조나 가중치는 공개되지 않습니다.\n",
       "    *   **Gemma:** **\"오픈 모델(Open Models)\"**로 분류됩니다. 모델의 가중치(weights)를 공개하여 개발자들이 직접 다운로드하여 로컬 서버나 클라우드 환경에서 실행하고, 자신들의 특정 목적에 맞게 파인튜닝(fine-tuning)할 수 있습니다. 상업적 사용을 포함한 특정 라이선스 정책을 따릅니다.\n",
       "\n",
       "5.  **대상 사용자 및 용도:**\n",
       "    *   **Gemini:** Google의 최고 수준 AI를 필요로 하는 대기업, 대규모 서비스 개발자, 그리고 Google 서비스를 이용하는 일반 최종 사용자들을 대상으로 합니다. 복잡한 문제 해결, 창의적인 콘텐츠 생성, 고급 분석, 그리고 강력한 AI 기반 서비스 구축에 적합합니다.\n",
       "    *   **Gemma:** AI 기술을 탐구하고, 로컬 환경에서 모델을 실행하거나, 특정 애플리케이션에 맞게 모델을 수정하려는 개발자, 연구자, 학생, 스타트업 등을 위한 것입니다. 챗봇, 요약 도구, 코드 생성 보조, 교육 및 연구 프로젝트 등 다양한 용도로 활용될 수 있습니다.\n",
       "\n",
       "요약하자면, **Gemini는 Google의 최첨단 기술을 집약한 \"완전체\"이자 \"블랙박스\" 형태의 AI 서비스 플랫폼**인 반면, **Gemma는 Gemini의 연구 기반 위에서 탄생한 \"경량화되고 개방된\" AI 도구 키트**라고 볼 수 있습니다. 둘 다 Google AI의 중요한 축을 담당하지만, 각각 다른 필요와 목표를 충족하기 위해 설계되었습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
