{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 선형회귀 개요\n",
    "\n",
    "선형 회귀(線型回歸, Linear regression)는 종속 변수 y와 한 개 이상의 독립 변수X와의 선형 상관 관계를 모델링하는 회귀분석 기법. [위키백과](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 선형회귀 모델\n",
    "- 각 Feature들에 가중치(Weight)를 곱하고 편향(bias)를 더해 예측 결과를 출력한다.\n",
    "- Weight와 bias는 선형 회귀 모델 학습 과정에서 최적화해야 하는 파라미터다.\n",
    "  - 가중치는 각 feature(X) 가 target(y)에 미치는 영향도를 나타내는 값이다.\n",
    "  - 양수 가중치는 target값을 증가시키고, 음수 가중치는 감소시킨다. 0에서 멀 수록 target에 큰 영향을 미치는 feature이며, 0에 가까울수록 target과의 연관성이 적은 feature다.\n",
    "  - bias는 모든 feature가 0일 때의 target 값이다. \n",
    "- $\\hat{y_i} = w_1 x_{i1} + w_2 x_{i2}... + w_{p} x_{ip} + b$\n",
    "    - $\\hat{y_i}$: 예측값\n",
    "    - $x$: 특성(feature-컬럼)\n",
    "    - $w$: 가중치(weight), 회귀계수(regression coefficient). 특성이 $\\hat{y_i}$ 에 얼마나 영향을 주는지 정도\n",
    "    - $b$: 절편\n",
    "    - $p$: p 번째 특성(feature)/p번째 가중치\n",
    "    - $i$: i번째 관측치(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습\n",
    "#### Boston housing dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data, x,y 분리\n",
    "df = pd.read_csv(\"data/boston_dataset.csv\")\n",
    "X = df.drop(columns='MEDV')\n",
    "y = df['MEDV']\n",
    "\n",
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LinearRegression\n",
    "- 가장 기본적인 선형 회귀 모델\n",
    "- 각 Feauture에 가중합으로 Y값을 추론한다.\n",
    "- 학습 결과 속성(Instance 변수)\n",
    "    - **coef_**: 각 Feature에 곱하는 가중치\n",
    "    - **intercept_**: y절편. 모든 Feature가 0일때 예측값\n",
    "    \n",
    "### 데이터 전처리\n",
    "\n",
    "- **선형회귀 모델사용시 전처리**\n",
    "    - **범주형 Feature**\n",
    "      -  원핫 인코딩\n",
    "    - **연속형 Feature**\n",
    "        - Feature Scaling을 통해서 각 컬럼들의 값의 단위를 맞춰준다.\n",
    "        - StandardScaler를 사용할 때 성능이 더 잘나오는 경향이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-16T05:43:20.067827Z",
     "start_time": "2022-12-16T05:43:20.051167Z"
    }
   },
   "outputs": [],
   "source": [
    "# 전처리\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 모델 생성, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 학습을 통해 찾은 weights 와 bias 조회\n",
    "print(\"weights\")\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(lr.coef_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bias\")\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 회귀 - mse, rmse, (ma-절대값-e), r2\n",
    "from metrics import print_regression_metrcis\n",
    "\n",
    "print_regression_metrcis(y_train, lr.predict(X_train_scaled), title=\"Transet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_regression_metrcis(y_test, lr.predict(X_test_scaled), title=\"Testset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 이용\n",
    "- Feature Scaler -> LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pl = Pipeline([(\"scaler\", StandardScaler()),(\"model\", LinearRegression())], verbose=True)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_regression_metrcis(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### y_test 정답과 추론값 비교 - 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(range(y_test.size), y_test, marker=\"x\", label=\"정답\")\n",
    "plt.plot(range(y_test.size), pred, marker='o', label=\"예측\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 다항회귀 (Polynomial Regression)\n",
    "- 전처리방식 중 하나로 Feature가 너무 적어 y의 값들을 다 설명하지 못하여 underfitting이 된 경우 Feature를 늘려준다.\n",
    "- 각 Feature들을 거듭제곱한 것과 Feature들 끼리 곱한 새로운 특성들을 추가한다.\n",
    "    - 파라미터(Coef, weight)를 기준으로는 일차식이 되어 선형모델이다. 그렇지만 input 기준으로는 N차식이 되어 비선형 데이터를 추론할 수 있는 모델이 된다.\n",
    "- `PolynomialFeatures` Transformer를 사용해서 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T13:23:44.707240Z",
     "start_time": "2023-02-07T13:23:44.695238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 모델링을 통해 찾아야 하는 함수.\n",
    "def func(X):\n",
    "    return X**2 + X + 2 + np.random.normal(0,1, size=(X.size, 1))\n",
    "    \n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = func(X)\n",
    "y = y.flatten()\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X,  y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 모델생성, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X,  y, label=\"y\")\n",
    "plt.scatter(X, pred, label='y hat(예측)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import print_regression_metrcis\n",
    "print_regression_metrcis(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### PolynomialFeatures를 이용해 다항회귀구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pnf = PolynomialFeatures(\n",
    "    degree=2,            # 최고차항의 차수. ex) degree=4로 하면: x(원래 컬럼), x^2, x^3, x^4  한 feature추가.\n",
    "    include_bias=False,  #True(기본값) - 상수항 feature 생성여부. (모든 값이 1인 feature 추가여부)\n",
    ")\n",
    "# pnf.fit(X)\n",
    "# pnf.transform(X)\n",
    "X_poly = pnf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, X_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### LinearRegression 모델을 이용해 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2.coef_, lr2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-3, 3, 1000)[..., np.newaxis]  # (1000, ) -> (1000, 1)\n",
    "X_new_poly = pnf.transform(X_new)\n",
    "# X_new_poly.shape\n",
    "y_hat = lr2.predict(X_new_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['font.family'] = \"malgun gothic\"\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.scatter(X, y, label=\"정답\")\n",
    "plt.plot(X_new, y_hat, color='k', linewidth=2, label=\"Model추정\")\n",
    "plt.plot(X_new, lr.predict(X_new), color=\"r\", linewidth=2, label=\"Label 전처리전\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "from metrics import print_regression_metrcis\n",
    "print_regression_metrcis(y, lr2.predict(X_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## degree를 크게\n",
    "- Feature가 너무 많으면 Overfitting 문제가 생긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnf3 = PolynomialFeatures(degree=25, include_bias=False)\n",
    "X_poly3 = pnf3.fit_transform(X)\n",
    "print(X_poly3.shape)\n",
    "lr3 = LinearRegression()\n",
    "lr3.fit(X_poly3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = lr3.predict(X_poly3)\n",
    "print_regression_metrcis(y, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree=25 시각화\n",
    "y_hat = lr3.predict(pnf3.transform(X_new))\n",
    "plt.scatter(X, y, label=\"정답\")\n",
    "plt.plot(X_new, y_hat, color='k', linewidth=2, label=\"Model추정\")\n",
    "plt.legend()\n",
    "plt.title(\"degree=25로 전처리한 결과\")\n",
    "# plt.ylim(-5, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PolynomialFeatures 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "data = np.arange(12).reshape(6, 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly2 = pnf.fit_transform(data)\n",
    "poly2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 후 각 feature를 어떻게 계산했는지 조회\n",
    "pnf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poly2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(poly2, columns=pnf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnf2 = PolynomialFeatures(degree=5, include_bias=False)\n",
    "poly_n = pnf2.fit_transform(data)\n",
    "poly_n.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pnf2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PolynomialFeatures를 Boston Dataset에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from metrics import print_regression_metrcis\n",
    "\n",
    "df = pd.read_csv('data/boston_dataset.csv')\n",
    "X = df.drop(columns='MEDV')\n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 pipeline\n",
    "preprocessor = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)), \n",
    "    (\"scaler\", StandardScaler()), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessor.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor), \n",
    "    (\"model\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pipeline.predict(X_train)\n",
    "pred_test = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_regression_metrcis(y_train, pred_train, \"Train set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_regression_metrcis(y_test, pred_test, \"Test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 규제 (Regularization)\n",
    "- 선형 회귀 모델에서 과대적합(Overfitting) 문제를 해결하기 위해 가중치(회귀계수)에 페널티 값을 적용한다.\n",
    "- 입력데이터의 Feature들이 너무 많은 경우 Overfitting이 발생.\n",
    "    - Feature수에 비해 관측치 수가 적은 경우 모델이 복잡해 지면서 Overfitting이 발생한다.\n",
    "- 해결\n",
    "    - 데이터를 더 수집한다. \n",
    "    - Feature selection\n",
    "        - 불필요한 Features들을 제거한다.\n",
    "    - 규제 (Regularization) 을 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한한다.(0에 가까운 값으로 만들어 준다.)\n",
    "        - LinearRegression의 규제는 학습시 계산하는 오차를 키워서 모델이 오차를 줄이기 위해 가중치를 0에 가까운 값으로 만들도록 하는 방식을 사용한다.\n",
    "        - L1 규제 (Lasso)\n",
    "        - L2 규제 (Ridge)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression (L2 규제)\n",
    "- 손실함수(loss function)에 규제항으로 $\\alpha \\sum_{i=1}^{n}{w_{i}^{2}}$ (L2 Norm)을 더해준다.\n",
    "  - MSE(손실함수)는 예측값과 실제값 간의 오차를 계산한다.이때 L2 규제를 적용하면, 손실함수에 가중치의 제곱합(∑wᵢ²)을 더해 오차를 인위적으로 크게 만든다.\n",
    "      - 이로 인해 모델이 오차를 최소화하려면 가중치의 크기를 줄여야 한다.\n",
    "      - 결과적으로 가중치들이 0에 가까운 값으로 수렴하게 된다.\n",
    "      - 이는 각 feature의 영향력을 줄여서 모델이 과도하게 복잡해지는 것을 방지한다. 즉 모델의 복잡도를 낮추고 일반화 성능을 향상시킨다.\n",
    "- $\\alpha$는 하이퍼파라미터로 모델을 얼마나 많이 규제할지 조절한다. \n",
    "    - $\\alpha = 0$ 에 가까울수록 규제가 약해진다. (0일 경우 선형 회귀동일)\n",
    "    - $\\alpha$ 가 커질 수록 모든 가중치가 작아져 입력데이터의 Feature들 중 중요하지 않은 Feature의 예측에 대한 영향력이 작아지게 된다.\n",
    "\n",
    "$$\n",
    "\\text{손실함수}(w) = \\text{MSE}(w) + \\alpha \\cfrac{1}{2}\\sum_{i=1}^{n}{w_{i}^{2}}\n",
    "$$\n",
    "\n",
    "> **손실함수(Loss Function):** 모델의 예측한 값과 실제값 사이의 차이를 정의하는 함수로 모델이 학습할 때 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/boston_dataset.csv')\n",
    "X = df.drop(columns='MEDV')\n",
    "y = df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "## Ridge Regression은 LinearRegression 모델과 같은 공식의 모델임. 단지 최적화 방법이 다른 것 뿐이다.\n",
    "## 그래서 데이터 전처리는 연속형 Feature는 Feature Scaling을 범주형 Feature는 One Hot Encoding을 한다.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 규제 alpha 에 따른 weight 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\n",
    "\n",
    "# alpha에 따른 각 feature 곱해지는 weight들을 저장할 DataFrame\n",
    "coef_df = pd.DataFrame() \n",
    "bias_list = [] #bias 들 저장할 리스트\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    # 모델 생성 -> hyper parameter alpha 를 설정.\n",
    "    model = Ridge(alpha=alpha, random_state=0)\n",
    "    # 학습\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    # 학습 후 찾은 weight와 bias를 저장.\n",
    "    coef_df[f\"{alpha}\"] = model.coef_\n",
    "    bias_list.append(model.intercept_)\n",
    "    # 검증결과 출력\n",
    "    pred_train = model.predict(X_train_scaled)\n",
    "    pred_test = model.predict(X_test_scaled)\n",
    "    print(f\"Alpha {alpha} - Train: {r2_score(y_train, pred_train)}, Test: {r2_score(y_test, pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.index = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso(Least Absolut Shrinkage and Selection Operator) Regression (L1 규제)\n",
    "\n",
    "- 손실함수에 규제항으로 $\\alpha \\sum_{i=1}^{n}{\\left| w_i \\right|}$ (L1 Norm)더한다.\n",
    "  - MSE(손실함수)는 예측값과 실제값 간의 오차를 계산한다.이때 L1 규제를 적용하면, 손실함수에 가중치 절댓값의 합(∑|wᵢ|) 을 추가한다.\n",
    "    - 이로 인해 손실값이 커지고, 모델은 손실을 줄이기 위해 가중치 중 일부를 정확히 0으로 만든다.\n",
    "    - 결과적으로 불필요한 feature의 가중치가 0이 되어 모델에서 제외된다. 즉, feature selection이 자동으로 일어난다.\n",
    "    -  이는 모델 해석력을 높이고,  불필요한 특성이 개입되는 것을 막아 일반화 성능을 높이는 효과를 준다.\n",
    "\n",
    "$$\n",
    "\\text{손실함수}(w) = \\text{MSE}(w) + \\alpha \\sum_{i=1}^{n}{\\left| w_i \\right|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\n",
    "\n",
    "# alpha에 따른 각 feature 곱해지는 weight들을 저장할 DataFrame\n",
    "coef_df2 = pd.DataFrame() \n",
    "bias_list2 = [] #bias 들 저장할 리스트\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    model = Lasso(alpha=alpha, random_state=0)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    coef_df2[f\"{alpha}\"] = model.coef_\n",
    "    bias_list2.append(model.intercept_)\n",
    "    # 검증결과 출력\n",
    "    pred_train = model.predict(X_train_scaled)\n",
    "    pred_test = model.predict(X_test_scaled)\n",
    "    print(f\"Alpha {alpha} - Train: {r2_score(y_train, pred_train)}, Test: {r2_score(y_test, pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df2.index = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynormialFeatures로 전처리한 Boston Dataset 에 Ridge, Lasso 규제 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "preprocessor = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)), \n",
    "    (\"scaler\", StandardScaler()), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = preprocessor.fit_transform(X_train)\n",
    "X_test_poly = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### LinearRegression으로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from metrics import print_regression_metrcis\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_poly, y_train)\n",
    "\n",
    "print_regression_metrcis(y_train, lr.predict(X_train_poly))\n",
    "print('-------------------------------------')\n",
    "print_regression_metrcis(y_test, lr.predict(X_test_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Ridge 의 alpha값 변화에 따른 R square 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 20]\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "for alpha in alpha_list:\n",
    "    ridge = Ridge(alpha=alpha, random_state=0)\n",
    "    ridge.fit(X_train_poly, y_train)\n",
    "    train_r2.append(r2_score(y_train, ridge.predict(X_train_poly)))\n",
    "    test_r2.append(r2_score(y_test, ridge.predict(X_test_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_df = pd.DataFrame({\"alpha\":alpha_list, \"train\":train_r2, \"test\":test_r2})\n",
    "ridge_df.set_index(\"alpha\", inplace=True)\n",
    "ridge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### lasso 의 alpha값 변화에 따른 R square 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 20]\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "for alpha in alpha_list:\n",
    "    lasso = Lasso(alpha=alpha, random_state=0)\n",
    "    lasso.fit(X_train_poly, y_train)\n",
    "    train_r2.append(r2_score(y_train, lasso.predict(X_train_poly)))\n",
    "    test_r2.append(r2_score(y_test, lasso.predict(X_test_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_df = pd.DataFrame({\"alpha\":alpha_list, \"train\":train_r2, \"test\":test_r2})\n",
    "lasso_df.set_index(\"alpha\", inplace=True)\n",
    "lasso_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ElasticNet(엘라스틱넷)\n",
    "- 릿지와 라쏘를 절충한 모델.\n",
    "- 규제항에 릿지, 라쏘 규제항을 더해서 추가한다. \n",
    "- 혼합비율 $r$을 사용해 혼합정도를 조절\n",
    "- $r=0$이면 릿지와 같고 $r=1$이면 라쏘와 같다.\n",
    "\n",
    "$$\n",
    "\\text{손실함수}(w) = \\text{MSE}(w) + r\\alpha \\sum_{i=1}^{n}{\\left| w_i \\right|}  + \\cfrac{1-r}{2}\\alpha\\sum_{i=1}^{n}{w_{i}^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.5, l1_ratio=0.3)\n",
    "model.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_regression_metrcis(y_train, model.predict(X_train_poly), \"==========Trainset\")\n",
    "print_regression_metrcis(y_test, model.predict(X_test_poly), \"==========Testset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 정리\n",
    "- 일반적으로 선형회귀의 경우 어느정도 규제가 있는 경우가 성능이 좋다.\n",
    "- 기본적으로 **Ridge**를 사용한다.\n",
    "- Target에 영향을 주는 Feature가 몇 개뿐일 경우 특성의 가중치를 0으로 만들어 주는 **Lasso** 사용한다. \n",
    "- 특성 수가 학습 샘플 수 보다 많거나 feature간에 연관성이 높을 때는 **ElasticNet**을 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
